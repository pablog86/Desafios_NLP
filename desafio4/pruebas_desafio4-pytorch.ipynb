{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfa39F4lsLf3"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## LSTM Traductor\n",
    "Ejemplo basado en [LINK](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqO0PRcFsPTe"
   },
   "source": [
    "### Datos\n",
    "El objecto es utilizar datos disponibles de Anki de traducciones de texto en diferentes idiomas. Se construirá un modelo traductor seq2seq utilizando encoder-decoder.\\\n",
    "[LINK](https://www.manythings.org/anki/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cq3YXak9sGHd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random  \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica de accuracy con máscara de padding\n",
    "def sequence_acc(logits, tgt_idx, pad_idx=0):\n",
    "    \"\"\"\n",
    "    logits : [B, L, V]  salida del modelo\n",
    "    tgt_idx: [B, L]     índices ground-truth\n",
    "    \"\"\"\n",
    "    pred_idx = logits.argmax(dim=-1)\n",
    "    mask     = tgt_idx.ne(pad_idx)            # True donde hay palabra\n",
    "    correct  = (pred_idx.eq(tgt_idx) & mask).float()\n",
    "    total = mask.sum()\n",
    "    return correct.sum() / total if total > 0 else torch.tensor(0.0)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train(model,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          optimizer,\n",
    "          criterion,              \n",
    "          epochs     = 50,\n",
    "          patience   = 7,\n",
    "          min_delta  = 1e-4,\n",
    "          device     = \"cuda\",\n",
    "          pad_idx    = 0,\n",
    "          teacher_forcing_ratio = 0.5):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    wait          = 0\n",
    "    best_state    = None\n",
    "    history = {\"loss\": [], \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ----- TRAIN -----\n",
    "        model.train()\n",
    "        tot_loss, tot_acc = 0, 0\n",
    "        for enc_in, dec_in, tgt_idx in train_loader:\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            tgt_idx  = tgt_idx.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # logits = model(enc_in, dec_in)            # [B, L, V]\n",
    "            logits = model(enc_in, dec_in, teacher_forcing_ratio)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(-1, V), tgt_idx.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item()\n",
    "            tot_acc  += sequence_acc(logits, tgt_idx, pad_idx).item()\n",
    "        train_loss = tot_loss / len(train_loader)\n",
    "        train_acc  = tot_acc  / len(train_loader)\n",
    "\n",
    "        # ----- VALID -----\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for enc_in, dec_in, tgt_idx in valid_loader:\n",
    "                enc_in  = enc_in.to(device)\n",
    "                dec_in  = dec_in.to(device)\n",
    "                tgt_idx = tgt_idx.to(device)\n",
    "                logits = model(enc_in, dec_in)\n",
    "                B, L, V = logits.shape\n",
    "                loss = criterion(logits.view(-1, V), tgt_idx.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                val_acc  += sequence_acc(logits, tgt_idx, pad_idx).item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_acc  /= len(valid_loader)\n",
    "        history[\"loss\"].append(train_loss)\n",
    "        history[\"accuracy\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:3d}: \"\n",
    "              f\"Train L={train_loss:.4f} A={train_acc:.4f} | \"\n",
    "              f\"Val L={val_loss:.4f} A={val_acc:.4f}\")\n",
    "\n",
    "        # ----- EARLY STOPPING -----\n",
    "        if val_loss + min_delta < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state    = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BFiCH8nxoIY"
   },
   "source": [
    "### 1 - Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1655153154355,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "-9aNLZBDtA5J",
    "outputId": "68de3ded-aa96-40fe-fab5-8083525e8c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows disponibles: 118964\n",
      "Cantidad de rows utilizadas: 118964\n",
      "A deal is a deal. Un trato es un trato. <eos> <sos> Un trato es un trato.\n"
     ]
    }
   ],
   "source": [
    "# dataset_file\n",
    "text_file = \"./clase/spa-eng/spa.txt\"\n",
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# Por limitaciones de RAM no se leen todas las filas\n",
    "MAX_NUM_SENTENCES = 0\n",
    "\n",
    "# Mezclar el dataset, forzar semilla siempre igual\n",
    "np.random.seed([40])\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "count = 0\n",
    "\n",
    "if MAX_NUM_SENTENCES == 0:\n",
    "    MAX_NUM_SENTENCES = len(lines)\n",
    "for line in lines:\n",
    "    count += 1\n",
    "    if count > MAX_NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    # Input sentence --> eng\n",
    "    # output --> spa\n",
    "    input_sentence, output = line.rstrip().split('\\t')\n",
    "\n",
    "    # output sentence (decoder_output) tiene <eos>\n",
    "    output_sentence = output + ' <eos>'\n",
    "    # output sentence input (decoder_input) tiene <sos>\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"Cantidad de rows disponibles:\", len(lines))\n",
    "print(\"Cantidad de rows utilizadas:\", len(input_sentences))\n",
    "print(input_sentences[0], output_sentences[0], output_sentences_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P-ynUNP5xp6"
   },
   "source": [
    "### 2 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5WAZGOTfGyha"
   },
   "outputs": [],
   "source": [
    "# Definir el tamaño máximo del vocabulario\n",
    "MAX_VOCAB_SIZE = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1655153154356,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "eF1W6peoFGXA",
    "outputId": "e748ad10-0c8d-4bca-ab4c-76f1974e12cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario: 13524\n",
      "Sentencia de entrada más larga: 47\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar las palabras con el Tokenizer de Keras\n",
    "# Definir una máxima cantidad de palabras a utilizar:\n",
    "# - num_words --> the maximum number of words to keep, based on word frequency.\n",
    "# - Only the most common num_words-1 words will be kept.\n",
    "from clase.torch_helpers import Tokenizer\n",
    "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Sentencia de entrada más larga:\", max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1655153154936,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "zBzdKiTVIBYY",
    "outputId": "f313cf87-642e-4671-b88d-90ecd0e80c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario: 26341\n",
      "Sentencia de salida más larga: 50\n"
     ]
    }
   ],
   "source": [
    "# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n",
    "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
    "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
    "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
    "\n",
    "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Sentencia de salida más larga:\", max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xqb8ZJ4sJHgv"
   },
   "source": [
    "Como era de esperarse, las sentencias en castellano son más largas que en inglés, y lo mismo sucede con su vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGOn9N57IuYz"
   },
   "source": [
    "A la hora de realiza padding es importante teneer en cuenta que en el encoder los ceros se agregan al comienoz y en el decoder al final. Esto es porque la salida del encoder está basado en las últimas palabras de la sentencia (son las más importantes), mientras que en el decoder está basado en el comienzo de la secuencia de salida ya que es la realimentación del sistema y termina con fin de sentencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1655153154937,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "q0Ob4hAWJkcv",
    "outputId": "9152d151-b863-49c9-e527-940d37a85385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows del dataset: 118964\n",
      "encoder_input_sequences shape: (118964, 47)\n",
      "decoder_input_sequences shape: (118964, 50)\n",
      "decoder_output_sequences shape: (118964, 50)\n"
     ]
    }
   ],
   "source": [
    "from clase.torch_helpers import pad_sequences\n",
    "print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n",
    "\n",
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
    "\n",
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)\n",
    "\n",
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_output_sequences shape:\", decoder_output_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos un dataset sin one_hot encoding para ocupar menos memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_size: 47\n",
      "decoder_input_size: 50\n",
      "Output dim 50\n"
     ]
    }
   ],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Devuelve tensores int64 con PAD=0.\n",
    "    ─ encoder_inputs : [N, T_enc]\n",
    "    ─ decoder_inputs : [N, T_dec]   (<sos> + frase)\n",
    "    ─ decoder_outputs: [N, T_dec]   (frase + <eos>)\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_arr: np.ndarray,\n",
    "                       dec_in_arr: np.ndarray,\n",
    "                       dec_out_arr: np.ndarray):\n",
    "\n",
    "        # Convertir a tensores int64\n",
    "        self.encoder_inputs  = torch.from_numpy(enc_arr    .astype(np.int64))\n",
    "        self.decoder_inputs  = torch.from_numpy(dec_in_arr .astype(np.int64))\n",
    "        self.decoder_outputs = torch.from_numpy(dec_out_arr.astype(np.int64))\n",
    "\n",
    "        assert len(self.encoder_inputs) == len(self.decoder_inputs) == len(self.decoder_outputs), \\\n",
    "            \"Los tres arrays deben tener la misma longitud\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.encoder_inputs [idx],\n",
    "                self.decoder_inputs [idx],\n",
    "                self.decoder_outputs[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_inputs)\n",
    "    \n",
    "# Construcción\n",
    "data_set = Seq2SeqDataset(encoder_input_sequences,\n",
    "                          decoder_input_sequences,\n",
    "                          decoder_output_sequences)\n",
    "\n",
    "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
    "print(\"encoder_input_size:\", encoder_input_size)\n",
    "\n",
    "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
    "print(\"decoder_input_size:\", decoder_input_size)\n",
    "\n",
    "output_dim = data_set.decoder_outputs.shape[1]\n",
    "print(\"Output dim\", output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1655153159536,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "sUDPZeuAU1RI",
    "outputId": "f19e0632-7cfd-4671-dddc-edbcf715788f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 95172\n",
      "Tamaño del conjunto de validacion: 23792\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "valid_set_size = int(len(data_set) * 0.2)\n",
    "train_set_size = len(data_set) - valid_set_size\n",
    "\n",
    "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
    "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, len(data_set)))\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
    "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CJIsLBbj6rg"
   },
   "source": [
    "### 3 - Preparar los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizan dos embeddings pre entrenados de Fastext para el ingles y el español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Mosj2-x-kXBK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "model_embeddings = load_facebook_vectors(\"clase/cc.en.300.bin\")  \n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "model_embeddings_es = load_facebook_vectors(\"clase/cc.es.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13525, 300)\n",
      "(26342, 300)\n"
     ]
    }
   ],
   "source": [
    "vocab_size_in  = max(word2idx_inputs .values()) + 1   \n",
    "vocab_size_out = max(word2idx_outputs.values()) + 1   \n",
    "embed_dim = 300\n",
    "# Inglés\n",
    "embedding_matrix = np.zeros((vocab_size_in, embed_dim), dtype=np.float32)\n",
    "for w, idx in word2idx_inputs.items():\n",
    "    # idx incluye todos los valores hasta max_index\n",
    "    if w in model_embeddings:\n",
    "        embedding_matrix[idx] = model_embeddings[w]\n",
    "# Español\n",
    "embedding_matrix_es = np.zeros((vocab_size_out, embed_dim), dtype=np.float32)\n",
    "for w, idx in word2idx_outputs.items():\n",
    "    if w in model_embeddings_es:\n",
    "        embedding_matrix_es[idx] = model_embeddings_es[w]\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix_es.shape)\n",
    "nb_words = embedding_matrix.shape[0]\n",
    "num_words_output = embedding_matrix_es.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vKbhjtIwPgM"
   },
   "source": [
    "### 4 - Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        # combinaremos [hidden; encoder_outputs] → dec_hid_dim\n",
    "        self.attn = nn.Linear(enc_hid_dim*2 + dec_hid_dim, dec_hid_dim)\n",
    "        # vector para mapear a un escalar\n",
    "        self.v    = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden:      [B, dec_hid_dim]\n",
    "        # encoder_outputs: [B, src_len, enc_hid_dim*2]\n",
    "        src_len = encoder_outputs.size(1)\n",
    "\n",
    "        # repetir hidden src_len veces\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)            # [B, src_len, dec_hid_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  \n",
    "                                                                      # [B, src_len, dec_hid_dim]\n",
    "        attention = self.v(energy).squeeze(2)                          # [B, src_len]\n",
    "        return F.softmax(attention, dim=1)                             # [B, src_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14732,
     "status": "ok",
     "timestamp": 1655153181107,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "3fm3HCLMPSG-",
    "outputId": "39be3183-0a69-49a5-8aa1-9f24026edf32"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.num_layers = 4\n",
    "        self.embedding_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False  # marcar como layer no entrenable (freeze)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
    "                            num_layers=self.num_layers, dropout=0.4, bidirectional=True) # LSTM layer\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.layer_norm(out)\n",
    "        lstm_output, (ht, ct) = self.lstm(out)\n",
    "        # return (ht, ct)\n",
    "        #    shape → [num_layers, 2, B, hidden_size]\n",
    "        ht = ht.view(self.num_layers, 2, -1, self.lstm_size)\n",
    "        ct = ct.view(self.num_layers, 2, -1, self.lstm_size)\n",
    "        ht = ht.sum(dim=1)   # → [num_layers, B, hidden_size]\n",
    "        ct = ct.sum(dim=1)   # → [num_layers, B, hidden_size] \n",
    "        return lstm_output, (ht, ct) \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.num_layers = 4\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix_es))\n",
    "        self.embedding.weight.requires_grad = True  # Opcional: Freezar\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "\n",
    "        self.attn = Attention(self.lstm_size, self.lstm_size)\n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
    "        #                     num_layers=self.num_layers, dropout=0.2) # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim + self.lstm_size*2,   # ← aquí\n",
    "                            hidden_size=self.lstm_size,\n",
    "                            batch_first=True,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=0.4) # LSTM layer\n",
    "\n",
    "        # self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n",
    "        # self.fc1 = nn.Linear(in_features=self.lstm_size + self.lstm_size*2 + self.embedding_dim,\n",
    "        #                      out_features=self.output_dim)\n",
    "        self.fc1 = nn.Linear(in_features=self.lstm_size   # salida LSTM\n",
    "                            + self.lstm_size*2   # context vector\n",
    "                            + self.embedding_dim, \n",
    "                            out_features=self.output_dim)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x, prev_state, encoder_outputs):\n",
    "        # out = self.dropout(self.embedding(x))\n",
    "        # lstm_output, (ht, ct) = self.lstm(out, prev_state)\n",
    "        # out = self.fc1(lstm_output)  # conserva la dimensión de secuencia\n",
    "        embedded = self.embedding(x) # [B,1,emb_dim]\n",
    "        embedded = self.layer_norm(embedded)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # 1) calcular pesos\n",
    "        h = prev_state[0]                           # [num_layers, B, hid_dim]\n",
    "        hidden_last = h[-1]                         # [B, hid_dim]\n",
    "        attn_weights = self.attn(hidden_last, encoder_outputs)  # [B, src_len]\n",
    "        # 2) context vector\n",
    "        attn_weights = attn_weights.unsqueeze(1)    # [B,1,src_len]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # [B,1,2*hid]\n",
    "        # 3) concatenar embed + context → LSTM\n",
    "        lstm_in = torch.cat((embedded, context), dim=2)  # [B,1, emb+2*hid]\n",
    "        lstm_out, (ht, ct) = self.lstm(lstm_in, prev_state)\n",
    "        # 4) proyección\n",
    "        #    concatenar [lstm_out; context; embedded] antes de fc1\n",
    "        cat = torch.cat((\n",
    "            lstm_out,      # [B,1, hid]\n",
    "            context,       # [B,1, 2*hid]\n",
    "            embedded       # [B,1, emb]\n",
    "        ), dim=2)         # [B,1, hid+2*hid+emb]\n",
    "        out = self.fc1(cat)  # [B,1, output_dim]\n",
    "\n",
    "        return out, (ht, ct)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, encoder_input, decoder_input, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = decoder_input.shape[0]\n",
    "        decoder_input_len = decoder_input.shape[1]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, decoder_input_len, vocab_size, device=encoder_input.device)\n",
    "        # ultimo hidden state del encoder, primer estado oculto del decoder\n",
    "        # prev_state = self.encoder(encoder_input)\n",
    "        encoder_outputs, prev_state = self.encoder(encoder_input)\n",
    "        # En la primera iteracion se toma el primer token de target (<sos>)\n",
    "        input = decoder_input[:, 0:1]\n",
    "        for t in range(1, decoder_input_len):\n",
    "            # input = decoder_input[:, t:t+1]\n",
    "            output, prev_state = self.decoder(input, prev_state, encoder_outputs)\n",
    "            # top1 = output.argmax(1).view(-1, 1)\n",
    "            # top1 = output[:, -1, :].argmax(1).view(-1, 1)\n",
    "            top1 = output.squeeze(1).argmax(1).view(-1,1)\n",
    "            # guardar cada salida (softmax)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            # outputs[:, t, :] = output\n",
    "            use_teacher = random.random() < teacher_forcing_ratio\n",
    "            input = decoder_input[:, t:t+1] if use_teacher else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Seq2Seq                                  [1, 50, 26342]            --\n",
       "├─Encoder: 1-1                           [1, 47, 1024]             --\n",
       "│    └─Embedding: 2-1                    [1, 47, 300]              (4,057,500)\n",
       "│    └─LayerNorm: 2-2                    [1, 47, 300]              600\n",
       "│    └─LSTM: 2-3                         [1, 47, 1024]             22,233,088\n",
       "├─Decoder: 1-2                           [1, 1, 26342]             --\n",
       "│    └─Embedding: 2-4                    [1, 1, 300]               7,902,600\n",
       "│    └─LayerNorm: 2-5                    [1, 1, 300]               600\n",
       "│    └─Dropout: 2-6                      [1, 1, 300]               --\n",
       "│    └─Attention: 2-7                    [1, 47]                   --\n",
       "│    │    └─Linear: 3-1                  [1, 47, 512]              786,944\n",
       "│    │    └─Linear: 3-2                  [1, 47, 1]                512\n",
       "│    └─LSTM: 2-8                         [1, 1, 512]               10,067,968\n",
       "│    └─Linear: 2-9                       [1, 1, 26342]             48,390,254\n",
       "├─Decoder: 1-3                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-10                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-11                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-12                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-13                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-3                  [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-4                  [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-14                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-15                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-4                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-16                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-17                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-18                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-19                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-5                  [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-6                  [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-20                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-21                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-5                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-22                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-23                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-24                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-25                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-7                  [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-8                  [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-26                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-27                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-6                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-28                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-29                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-30                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-31                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-9                  [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-10                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-32                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-33                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-7                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-34                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-35                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-36                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-37                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-11                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-12                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-38                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-39                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-8                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-40                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-41                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-42                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-43                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-13                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-14                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-44                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-45                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-9                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-46                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-47                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-48                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-49                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-15                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-16                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-50                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-51                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-10                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-52                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-53                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-54                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-55                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-17                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-18                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-56                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-57                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-11                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-58                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-59                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-60                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-61                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-19                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-20                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-62                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-63                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-12                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-64                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-65                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-66                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-67                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-21                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-22                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-68                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-69                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-13                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-70                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-71                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-72                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-73                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-23                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-24                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-74                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-75                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-14                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-76                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-77                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-78                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-79                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-25                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-26                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-80                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-81                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-15                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-82                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-83                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-84                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-85                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-27                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-28                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-86                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-87                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-16                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-88                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-89                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-90                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-91                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-29                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-30                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-92                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-93                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-17                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-94                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-95                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-96                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-97                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-31                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-32                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-98                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-99                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-18                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-100                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-101                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-102                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-103                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-33                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-34                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-104                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-105                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-19                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-106                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-107                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-108                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-109                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-35                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-36                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-110                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-111                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-20                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-112                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-113                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-114                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-115                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-37                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-38                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-116                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-117                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-21                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-118                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-119                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-120                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-121                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-39                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-40                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-122                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-123                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-22                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-124                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-125                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-126                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-127                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-41                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-42                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-128                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-129                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-23                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-130                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-131                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-132                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-133                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-43                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-44                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-134                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-135                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-24                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-136                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-137                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-138                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-139                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-45                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-46                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-140                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-141                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-25                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-142                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-143                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-144                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-145                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-47                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-48                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-146                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-147                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-26                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-148                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-149                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-150                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-151                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-49                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-50                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-152                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-153                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-27                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-154                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-155                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-156                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-157                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-51                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-52                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-158                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-159                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-28                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-160                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-161                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-162                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-163                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-53                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-54                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-164                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-165                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-29                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-166                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-167                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-168                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-169                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-55                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-56                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-170                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-171                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-30                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-172                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-173                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-174                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-175                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-57                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-58                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-176                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-177                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-31                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-178                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-179                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-180                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-181                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-59                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-60                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-182                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-183                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-32                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-184                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-185                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-186                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-187                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-61                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-62                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-188                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-189                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-33                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-190                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-191                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-192                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-193                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-63                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-64                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-194                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-195                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-34                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-196                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-197                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-198                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-199                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-65                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-66                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-200                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-201                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-35                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-202                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-203                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-204                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-205                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-67                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-68                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-206                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-207                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-36                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-208                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-209                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-210                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-211                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-69                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-70                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-212                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-213                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-37                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-214                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-215                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-216                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-217                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-71                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-72                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-218                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-219                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-38                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-220                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-221                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-222                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-223                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-73                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-74                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-224                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-225                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-39                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-226                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-227                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-228                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-229                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-75                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-76                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-230                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-231                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-40                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-232                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-233                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-234                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-235                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-77                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-78                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-236                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-237                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-41                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-238                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-239                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-240                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-241                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-79                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-80                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-242                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-243                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-42                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-244                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-245                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-246                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-247                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-81                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-82                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-248                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-249                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-43                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-250                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-251                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-252                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-253                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-83                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-84                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-254                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-255                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-44                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-256                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-257                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-258                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-259                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-85                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-86                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-260                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-261                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-45                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-262                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-263                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-264                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-265                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-87                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-88                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-266                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-267                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-46                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-268                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-269                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-270                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-271                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-89                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-90                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-272                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-273                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-47                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-274                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-275                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-276                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-277                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-91                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-92                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-278                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-279                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-48                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-280                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-281                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-282                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-283                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-93                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-94                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-284                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-285                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-49                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-286                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-287                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-288                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-289                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-95                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-96                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-290                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-291                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-50                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-292                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-293                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-294                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-295                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-97                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-98                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-296                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-297                     [1, 1, 26342]             (recursive)\n",
       "==========================================================================================\n",
       "Total params: 93,440,066\n",
       "Trainable params: 89,382,566\n",
       "Non-trainable params: 4,057,500\n",
       "Total mult-adds (Units.GIGABYTES): 4.34\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 20.82\n",
       "Params size (MB): 373.76\n",
       "Estimated Total Size (MB): 394.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=nb_words)\n",
    "if cuda: encoder.cuda()\n",
    "decoder = Decoder(vocab_size=num_words_output, output_dim=num_words_output)\n",
    "if cuda: decoder.cuda()\n",
    "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
    "if cuda: seq2seq.cuda()\n",
    "\n",
    "# Diferentes learning rates para hacer fine tuning al embedding en español de Fasttext\n",
    "emb_params   = list(decoder.embedding.parameters())            # solo embedding ES\n",
    "other_params = [p for n, p in seq2seq.named_parameters()\n",
    "                if \"decoder.embedding\" not in n]               # resto de la red\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": emb_params,   \"lr\": lr*0.01},   # LR pequeño para embedding\n",
    "        {\"params\": other_params, \"lr\": lr},   # LR normal resto\n",
    "    ],\n",
    "    betas=(0.9, 0.98),\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # Omito el padding para que no falsee la metrica\n",
    "\n",
    "x_enc = data_set[0:1][0]\n",
    "x_dec = data_set[0:1][1]\n",
    "summary(seq2seq,\n",
    "        input_data=(x_enc.to(device).long(), x_dec.to(device).long()),\n",
    "        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597334,
     "status": "ok",
     "timestamp": 1655153778405,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "VDB0KWIegt8s",
    "outputId": "d1e002a8-fa8e-4dfc-f144-a95732026f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train L=6.6340 A=0.1598 | Val L=6.1579 A=0.1899\n",
      "Epoch   2: Train L=5.6769 A=0.2211 | Val L=5.0509 A=0.2772\n",
      "Epoch   3: Train L=4.5865 A=0.3149 | Val L=4.3364 A=0.3535\n",
      "Epoch   4: Train L=3.9229 A=0.3755 | Val L=4.0507 A=0.3867\n",
      "Epoch   5: Train L=3.5178 A=0.4146 | Val L=3.9030 A=0.4013\n",
      "Epoch   6: Train L=3.2387 A=0.4462 | Val L=3.8636 A=0.4082\n",
      "Epoch   7: Train L=3.0330 A=0.4743 | Val L=3.8860 A=0.4056\n",
      "Epoch   8: Train L=2.8760 A=0.4985 | Val L=3.8758 A=0.4152\n",
      "Epoch   9: Train L=2.7399 A=0.5228 | Val L=3.8919 A=0.4204\n",
      "Epoch  10: Train L=2.6391 A=0.5424 | Val L=3.9234 A=0.4251\n",
      "Epoch  11: Train L=2.5469 A=0.5604 | Val L=3.9703 A=0.4237\n",
      "Epoch  12: Train L=2.4703 A=0.5767 | Val L=4.0181 A=0.4220\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "history, model = train(seq2seq,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                epochs=100,\n",
    "                device=device,\n",
    "                patience=6\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1655154657801,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "pZzm3tx059Zv",
    "outputId": "b7a08ad6-392e-4e40-8491-fb707d23254c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_count = range(1, len(history['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=history['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=history['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbwn0ekDy_s2"
   },
   "source": [
    "### 5 - Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "71XeCtfYmOFx"
   },
   "outputs": [],
   "source": [
    "# Armar lo conversores de indice a palabra:\n",
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_text, \n",
    "                       model, \n",
    "                       input_tokenizer, \n",
    "                       word2idx_outputs, \n",
    "                       idx2word_target,\n",
    "                       max_input_len,\n",
    "                       max_output_len,\n",
    "                       device):\n",
    "    model.eval()\n",
    "    # 1) Tokenizar y paddear\n",
    "    seq = input_tokenizer.texts_to_sequences([input_text.lower()])[0]\n",
    "    seq = pad_sequences([seq], maxlen=max_input_len, padding='post')\n",
    "    encoder_input = torch.tensor(seq, dtype=torch.long).to(device)      # [1, max_input_len]\n",
    "    # 2) Pasar por el encoder\n",
    "    # prev_state = model.encoder(encoder_input)                           # (h, c)\n",
    "    encoder_outputs, prev_state = model.encoder(encoder_input)\n",
    "    # 3) Iniciar decoder con <sos>\n",
    "    sos = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    decoder_input = torch.tensor([[sos]], dtype=torch.long).to(device)  # [1, 1]\n",
    "    output_words = []\n",
    "    # 4) Loop hasta max_output_len o hasta <eos>\n",
    "    for _ in range(max_output_len):\n",
    "        # logits, prev_state = model.decoder(decoder_input, prev_state)\n",
    "        logits, prev_state = model.decoder(decoder_input, prev_state, encoder_outputs)\n",
    "        # logits: [1, 1, vocab_size]\n",
    "        logits = logits.squeeze(1)           # [1, vocab_size]\n",
    "        topi = logits.argmax(dim=1)          # [1]\n",
    "        idx = topi.item()                    # entero\n",
    "        if idx == eos:\n",
    "            break\n",
    "        output_words.append(idx2word_target[idx])\n",
    "        # re-alimentar al decoder\n",
    "        decoder_input = topi.unsqueeze(1)    # [1, 1]\n",
    "    return ' '.join(output_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:    You've got nothing.\n",
      "Output:   no tienes\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_test = input_sentences[i:i+1][0]\n",
    "translation = translate_sentence(\n",
    "    input_text=input_test,\n",
    "    model=model,\n",
    "    input_tokenizer=input_tokenizer,\n",
    "    word2idx_outputs=word2idx_outputs,\n",
    "    idx2word_target=idx2word_target,\n",
    "    max_input_len=max_input_len,\n",
    "    max_output_len=max_out_len,   \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Input:   \", input_test)\n",
    "print(\"Output:  \", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trabajó sobre el ejemplo de encoder-decoder para traducción portado de keras a pytorch con el fin de optimixar el uso de GPU para el setup. \\\n",
    "Se incrementó la red y se le agregaron componentes como normalización, dropout y atención con el propósito de mejorar el modelo. Se utilizó como refenrencia https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se omitió el uso de hot_one encoding para optimizar el consumo de memoria y asi poder usar todo el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizaron embeddings pre entrenados de FastText de 300 dims para el inglés y el español, con el fin de aumentar la capacidad de representación semántica. \\\n",
    "Para el caso del español se dejo un pequeño learning rate con el fin de alinear los espacios vectoriales haciendo fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lamentablemente no se logró mejoras respecto a lo implementado con keras, de esta forma podemos decir que el mejor camino es hacer un cambio de arquitectura a encoder-decoder usando Transformers."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMniDMuqoKT0lLVAJWxpRSt",
   "collapsed_sections": [],
   "name": "6c - traductor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
