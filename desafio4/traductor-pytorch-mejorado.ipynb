{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27ab205",
   "metadata": {},
   "source": [
    "## 🚀 Modificaciones aplicadas\n",
    "\n",
    "1. **Teacher Forcing Ratio**  \n",
    "   - Se añadió el parámetro `teacher_forcing_ratio` a la función `train` para alternar entre token real y predicho, reduciendo el _exposure bias_.  \n",
    "2. **LSTM Bidireccional en Encoder**  \n",
    "   - El `nn.LSTM` del encoder ahora usa `bidirectional=True` y concatena estados forward/backward.  \n",
    "3. **Gradient Clipping y Scheduler**  \n",
    "   - Se incorpora `torch.nn.utils.clip_grad_norm_` tras el _backward_ y un ejemplo de uso de `ReduceLROnPlateau`.  \n",
    "4. **Métrica BLEU y Ejemplos de Inferencia**  \n",
    "   - Añadido cálculo de BLEU tras cada epoch de validación y generación de ejemplos.  \n",
    "5. **TorchText para Batching**  \n",
    "   - Ejemplo de configuración de `Field` y `BucketIterator` para manejo automático de padding y longitudes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcf5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Mod: Entrenamiento con Teacher Forcing y Gradient Clipping ---\n",
    "# import random\n",
    "# from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# def train(model, iterator, optimizer, criterion, clip=1.0, teacher_forcing_ratio=0.5):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     for i, batch in enumerate(iterator):\n",
    "#         src = batch.src      # [src_len, batch_size]\n",
    "#         trg = batch.trg      # [trg_len, batch_size]\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass con teacher forcing ratio\n",
    "#         output = model(src, trg, teacher_forcing_ratio)\n",
    "#         # output: [trg_len, batch_size, output_dim]\n",
    "#         output_dim = output.shape[-1]\n",
    "#         output = output[1:].view(-1, output_dim)   # excluir <sos>\n",
    "#         trg = trg[1:].view(-1)\n",
    "\n",
    "#         loss = criterion(output, trg)\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Aplicar gradient clipping\n",
    "#         clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839641cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Mod: Encoder Bidireccional ---\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "#         super().__init__()\n",
    "#         self.hid_dim = hid_dim\n",
    "#         self.n_layers = n_layers\n",
    "\n",
    "#         self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "#         # Cambiado a bidireccional\n",
    "#         self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers,\n",
    "#                            dropout=dropout, bidirectional=True)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, src):\n",
    "#         # src: [src_len, batch_size]\n",
    "#         embedded = self.dropout(self.embedding(src))\n",
    "#         # outputs: [src_len, batch_size, hid_dim*2]\n",
    "#         outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "#         # Concatenar hidden forward y backward\n",
    "#         hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "#         cell   = torch.cat((cell[-2,:,:],   cell[-1,:,:]),   dim=1)\n",
    "\n",
    "#         # Ajustar dimensiones para el decoder\n",
    "#         return outputs, hidden.unsqueeze(0), cell.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523a1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Mod: Ejemplo de configuración con TorchText ---\n",
    "# from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "# # Definición de campos con SOS/EOS y manejo de longitudes\n",
    "# SRC = Field(tokenize=lambda x: x.split(), \n",
    "#             init_token='<sos>', eos_token='<eos>', \n",
    "#             lower=True, include_lengths=True)\n",
    "# TRG = Field(tokenize=lambda x: x.split(),\n",
    "#             init_token='<sos>', eos_token='<eos>', \n",
    "#             lower=True)\n",
    "\n",
    "# # Cargar dataset desde CSV/TSV (adaptar rutas y formato)\n",
    "# # train_data, valid_data = TabularDataset.splits(\n",
    "# #     path='.', train='train.csv', validation='valid.csv',\n",
    "# #     format='csv', fields=[('src', SRC), ('trg', TRG)]\n",
    "# # )\n",
    "\n",
    "# # Construir vocabularios limitados y mínimos\n",
    "# SRC.build_vocab(train_data, max_size=20000, min_freq=2)\n",
    "# TRG.build_vocab(train_data, max_size=30000, min_freq=2)\n",
    "\n",
    "# # Generar iteradores con sorting y padding automático\n",
    "# train_iter, valid_iter = BucketIterator.splits(\n",
    "#     (train_data, valid_data),\n",
    "#     batch_size=32,\n",
    "#     sort_within_batch=True,\n",
    "#     sort_key=lambda x: len(x.src),\n",
    "#     device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfa39F4lsLf3"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## LSTM Traductor\n",
    "Ejemplo basado en [LINK](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqO0PRcFsPTe"
   },
   "source": [
    "### Datos\n",
    "El objecto es utilizar datos disponibles de Anki de traducciones de texto en diferentes idiomas. Se construirá un modelo traductor seq2seq utilizando encoder-decoder.\\\n",
    "[LINK](https://www.manythings.org/anki/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cq3YXak9sGHd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random  \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica de accuracy con máscara de padding\n",
    "def sequence_acc(logits, tgt_idx, pad_idx=0):\n",
    "    \"\"\"\n",
    "    logits : [B, L, V]  salida del modelo\n",
    "    tgt_idx: [B, L]     índices ground-truth\n",
    "    \"\"\"\n",
    "    pred_idx = logits.argmax(dim=-1)\n",
    "    mask     = tgt_idx.ne(pad_idx)            # True donde hay palabra\n",
    "    correct  = (pred_idx.eq(tgt_idx) & mask).float()\n",
    "    total = mask.sum()\n",
    "    return correct.sum() / total if total > 0 else torch.tensor(0.0)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train(model,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          optimizer,\n",
    "          criterion,              \n",
    "          epochs     = 50,\n",
    "          patience   = 7,\n",
    "          min_delta  = 1e-4,\n",
    "          device     = \"cuda\",\n",
    "          pad_idx    = 0,\n",
    "          teacher_forcing_ratio = 0.5):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    wait          = 0\n",
    "    best_state    = None\n",
    "    history = {\"loss\": [], \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ----- TRAIN -----\n",
    "        model.train()\n",
    "        tot_loss, tot_acc = 0, 0\n",
    "        for enc_in, dec_in, tgt_idx in train_loader:\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            tgt_idx  = tgt_idx.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # logits = model(enc_in, dec_in)            # [B, L, V]\n",
    "            logits = model(enc_in, dec_in, teacher_forcing_ratio)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(-1, V), tgt_idx.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item()\n",
    "            tot_acc  += sequence_acc(logits, tgt_idx, pad_idx).item()\n",
    "        train_loss = tot_loss / len(train_loader)\n",
    "        train_acc  = tot_acc  / len(train_loader)\n",
    "\n",
    "        # ----- VALID -----\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for enc_in, dec_in, tgt_idx in valid_loader:\n",
    "                enc_in  = enc_in.to(device)\n",
    "                dec_in  = dec_in.to(device)\n",
    "                tgt_idx = tgt_idx.to(device)\n",
    "                logits = model(enc_in, dec_in)\n",
    "                B, L, V = logits.shape\n",
    "                loss = criterion(logits.view(-1, V), tgt_idx.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                val_acc  += sequence_acc(logits, tgt_idx, pad_idx).item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_acc  /= len(valid_loader)\n",
    "        history[\"loss\"].append(train_loss)\n",
    "        history[\"accuracy\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:3d}: \"\n",
    "              f\"Train L={train_loss:.4f} A={train_acc:.4f} | \"\n",
    "              f\"Val L={val_loss:.4f} A={val_acc:.4f}\")\n",
    "\n",
    "        # ----- EARLY STOPPING -----\n",
    "        if val_loss + min_delta < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state    = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BFiCH8nxoIY"
   },
   "source": [
    "### 1 - Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1655153154355,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "-9aNLZBDtA5J",
    "outputId": "68de3ded-aa96-40fe-fab5-8083525e8c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows disponibles: 118964\n",
      "Cantidad de rows utilizadas: 118964\n",
      "A deal is a deal. Un trato es un trato. <eos> <sos> Un trato es un trato.\n"
     ]
    }
   ],
   "source": [
    "# dataset_file\n",
    "text_file = \"./clase/spa-eng/spa.txt\"\n",
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# Por limitaciones de RAM no se leen todas las filas\n",
    "MAX_NUM_SENTENCES = 0\n",
    "\n",
    "# Mezclar el dataset, forzar semilla siempre igual\n",
    "np.random.seed([40])\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "count = 0\n",
    "\n",
    "if MAX_NUM_SENTENCES == 0:\n",
    "    MAX_NUM_SENTENCES = len(lines)\n",
    "for line in lines:\n",
    "    count += 1\n",
    "    if count > MAX_NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    # Input sentence --> eng\n",
    "    # output --> spa\n",
    "    input_sentence, output = line.rstrip().split('\\t')\n",
    "\n",
    "    # output sentence (decoder_output) tiene <eos>\n",
    "    output_sentence = output + ' <eos>'\n",
    "    # output sentence input (decoder_input) tiene <sos>\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"Cantidad de rows disponibles:\", len(lines))\n",
    "print(\"Cantidad de rows utilizadas:\", len(input_sentences))\n",
    "print(input_sentences[0], output_sentences[0], output_sentences_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P-ynUNP5xp6"
   },
   "source": [
    "### 2 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5WAZGOTfGyha"
   },
   "outputs": [],
   "source": [
    "# Definir el tamaño máximo del vocabulario\n",
    "MAX_VOCAB_SIZE = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1655153154356,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "eF1W6peoFGXA",
    "outputId": "e748ad10-0c8d-4bca-ab4c-76f1974e12cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario: 13524\n",
      "Sentencia de entrada más larga: 47\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar las palabras con el Tokenizer de Keras\n",
    "# Definir una máxima cantidad de palabras a utilizar:\n",
    "# - num_words --> the maximum number of words to keep, based on word frequency.\n",
    "# - Only the most common num_words-1 words will be kept.\n",
    "from clase.torch_helpers import Tokenizer\n",
    "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Sentencia de entrada más larga:\", max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1655153154936,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "zBzdKiTVIBYY",
    "outputId": "f313cf87-642e-4671-b88d-90ecd0e80c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario: 26341\n",
      "Sentencia de salida más larga: 50\n"
     ]
    }
   ],
   "source": [
    "# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n",
    "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
    "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
    "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
    "\n",
    "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Sentencia de salida más larga:\", max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xqb8ZJ4sJHgv"
   },
   "source": [
    "Como era de esperarse, las sentencias en castellano son más largas que en inglés, y lo mismo sucede con su vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGOn9N57IuYz"
   },
   "source": [
    "A la hora de realiza padding es importante teneer en cuenta que en el encoder los ceros se agregan al comienoz y en el decoder al final. Esto es porque la salida del encoder está basado en las últimas palabras de la sentencia (son las más importantes), mientras que en el decoder está basado en el comienzo de la secuencia de salida ya que es la realimentación del sistema y termina con fin de sentencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1655153154937,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "q0Ob4hAWJkcv",
    "outputId": "9152d151-b863-49c9-e527-940d37a85385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows del dataset: 118964\n",
      "encoder_input_sequences shape: (118964, 47)\n",
      "decoder_input_sequences shape: (118964, 50)\n",
      "decoder_output_sequences shape: (118964, 50)\n"
     ]
    }
   ],
   "source": [
    "from clase.torch_helpers import pad_sequences\n",
    "print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n",
    "\n",
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
    "\n",
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)\n",
    "\n",
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_output_sequences shape:\", decoder_output_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos un dataset sin one_hot encoding para ocupar menos memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_size: 47\n",
      "decoder_input_size: 50\n",
      "Output dim 50\n"
     ]
    }
   ],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Devuelve tensores int64 con PAD=0.\n",
    "    ─ encoder_inputs : [N, T_enc]\n",
    "    ─ decoder_inputs : [N, T_dec]   (<sos> + frase)\n",
    "    ─ decoder_outputs: [N, T_dec]   (frase + <eos>)\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_arr: np.ndarray,\n",
    "                       dec_in_arr: np.ndarray,\n",
    "                       dec_out_arr: np.ndarray):\n",
    "\n",
    "        # Convertir a tensores int64\n",
    "        self.encoder_inputs  = torch.from_numpy(enc_arr    .astype(np.int64))\n",
    "        self.decoder_inputs  = torch.from_numpy(dec_in_arr .astype(np.int64))\n",
    "        self.decoder_outputs = torch.from_numpy(dec_out_arr.astype(np.int64))\n",
    "\n",
    "        assert len(self.encoder_inputs) == len(self.decoder_inputs) == len(self.decoder_outputs), \\\n",
    "            \"Los tres arrays deben tener la misma longitud\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.encoder_inputs [idx],\n",
    "                self.decoder_inputs [idx],\n",
    "                self.decoder_outputs[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_inputs)\n",
    "    \n",
    "# Construcción\n",
    "data_set = Seq2SeqDataset(encoder_input_sequences,\n",
    "                          decoder_input_sequences,\n",
    "                          decoder_output_sequences)\n",
    "\n",
    "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
    "print(\"encoder_input_size:\", encoder_input_size)\n",
    "\n",
    "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
    "print(\"decoder_input_size:\", decoder_input_size)\n",
    "\n",
    "output_dim = data_set.decoder_outputs.shape[1]\n",
    "print(\"Output dim\", output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1655153159536,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "sUDPZeuAU1RI",
    "outputId": "f19e0632-7cfd-4671-dddc-edbcf715788f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 95172\n",
      "Tamaño del conjunto de validacion: 23792\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "valid_set_size = int(len(data_set) * 0.2)\n",
    "train_set_size = len(data_set) - valid_set_size\n",
    "\n",
    "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
    "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, len(data_set)))\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
    "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CJIsLBbj6rg"
   },
   "source": [
    "### 3 - Preparar los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizan dos embeddings pre entrenados de Fastext para el ingles y el español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Mosj2-x-kXBK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "model_embeddings = load_facebook_vectors(\"clase/cc.en.300.bin\")  \n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "model_embeddings_es = load_facebook_vectors(\"clase/cc.es.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13525, 300)\n",
      "(26342, 300)\n"
     ]
    }
   ],
   "source": [
    "vocab_size_in  = max(word2idx_inputs .values()) + 1   \n",
    "vocab_size_out = max(word2idx_outputs.values()) + 1   \n",
    "embed_dim = 300\n",
    "# Inglés\n",
    "embedding_matrix = np.zeros((vocab_size_in, embed_dim), dtype=np.float32)\n",
    "for w, idx in word2idx_inputs.items():\n",
    "    # idx incluye todos los valores hasta max_index\n",
    "    if w in model_embeddings:\n",
    "        embedding_matrix[idx] = model_embeddings[w]\n",
    "# Español\n",
    "embedding_matrix_es = np.zeros((vocab_size_out, embed_dim), dtype=np.float32)\n",
    "for w, idx in word2idx_outputs.items():\n",
    "    if w in model_embeddings_es:\n",
    "        embedding_matrix_es[idx] = model_embeddings_es[w]\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix_es.shape)\n",
    "nb_words = embedding_matrix.shape[0]\n",
    "num_words_output = embedding_matrix_es.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vKbhjtIwPgM"
   },
   "source": [
    "### 4 - Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        # combinaremos [hidden; encoder_outputs] → dec_hid_dim\n",
    "        self.attn = nn.Linear(enc_hid_dim*2 + dec_hid_dim, dec_hid_dim)\n",
    "        # vector para mapear a un escalar\n",
    "        self.v    = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden:      [B, dec_hid_dim]\n",
    "        # encoder_outputs: [B, src_len, enc_hid_dim*2]\n",
    "        src_len = encoder_outputs.size(1)\n",
    "\n",
    "        # repetir hidden src_len veces\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)            # [B, src_len, dec_hid_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  \n",
    "                                                                      # [B, src_len, dec_hid_dim]\n",
    "        attention = self.v(energy).squeeze(2)                          # [B, src_len]\n",
    "        return F.softmax(attention, dim=1)                             # [B, src_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14732,
     "status": "ok",
     "timestamp": 1655153181107,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "3fm3HCLMPSG-",
    "outputId": "39be3183-0a69-49a5-8aa1-9f24026edf32"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.num_layers = 4\n",
    "        self.embedding_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False  # marcar como layer no entrenable (freeze)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
    "                            num_layers=self.num_layers, dropout=0.4, bidirectional=True) # LSTM layer\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.layer_norm(out)\n",
    "        lstm_output, (ht, ct) = self.lstm(out)\n",
    "        # return (ht, ct)\n",
    "        #    shape → [num_layers, 2, B, hidden_size]\n",
    "        ht = ht.view(self.num_layers, 2, -1, self.lstm_size)\n",
    "        ct = ct.view(self.num_layers, 2, -1, self.lstm_size)\n",
    "        ht = ht.sum(dim=1)   # → [num_layers, B, hidden_size]\n",
    "        ct = ct.sum(dim=1)   # → [num_layers, B, hidden_size] \n",
    "        return lstm_output, (ht, ct) \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.num_layers = 4\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix_es))\n",
    "        self.embedding.weight.requires_grad = True  # Opcional: Freezar\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "\n",
    "        self.attn = Attention(self.lstm_size, self.lstm_size)\n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
    "        #                     num_layers=self.num_layers, dropout=0.2) # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim + self.lstm_size*2,   # ← aquí\n",
    "                            hidden_size=self.lstm_size,\n",
    "                            batch_first=True,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=0.4) # LSTM layer\n",
    "\n",
    "        # self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n",
    "        # self.fc1 = nn.Linear(in_features=self.lstm_size + self.lstm_size*2 + self.embedding_dim,\n",
    "        #                      out_features=self.output_dim)\n",
    "        self.fc1 = nn.Linear(in_features=self.lstm_size   # salida LSTM\n",
    "                            + self.lstm_size*2   # context vector\n",
    "                            + self.embedding_dim, \n",
    "                            out_features=self.output_dim)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x, prev_state, encoder_outputs):\n",
    "        # out = self.dropout(self.embedding(x))\n",
    "        # lstm_output, (ht, ct) = self.lstm(out, prev_state)\n",
    "        # out = self.fc1(lstm_output)  # conserva la dimensión de secuencia\n",
    "        embedded = self.embedding(x) # [B,1,emb_dim]\n",
    "        embedded = self.layer_norm(embedded)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # 1) calcular pesos\n",
    "        h = prev_state[0]                           # [num_layers, B, hid_dim]\n",
    "        hidden_last = h[-1]                         # [B, hid_dim]\n",
    "        attn_weights = self.attn(hidden_last, encoder_outputs)  # [B, src_len]\n",
    "        # 2) context vector\n",
    "        attn_weights = attn_weights.unsqueeze(1)    # [B,1,src_len]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # [B,1,2*hid]\n",
    "        # 3) concatenar embed + context → LSTM\n",
    "        lstm_in = torch.cat((embedded, context), dim=2)  # [B,1, emb+2*hid]\n",
    "        lstm_out, (ht, ct) = self.lstm(lstm_in, prev_state)\n",
    "        # 4) proyección\n",
    "        #    concatenar [lstm_out; context; embedded] antes de fc1\n",
    "        cat = torch.cat((\n",
    "            lstm_out,      # [B,1, hid]\n",
    "            context,       # [B,1, 2*hid]\n",
    "            embedded       # [B,1, emb]\n",
    "        ), dim=2)         # [B,1, hid+2*hid+emb]\n",
    "        out = self.fc1(cat)  # [B,1, output_dim]\n",
    "\n",
    "        return out, (ht, ct)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, encoder_input, decoder_input, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = decoder_input.shape[0]\n",
    "        decoder_input_len = decoder_input.shape[1]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, decoder_input_len, vocab_size, device=encoder_input.device)\n",
    "        # ultimo hidden state del encoder, primer estado oculto del decoder\n",
    "        # prev_state = self.encoder(encoder_input)\n",
    "        encoder_outputs, prev_state = self.encoder(encoder_input)\n",
    "        # En la primera iteracion se toma el primer token de target (<sos>)\n",
    "        input = decoder_input[:, 0:1]\n",
    "        for t in range(1, decoder_input_len):\n",
    "            # input = decoder_input[:, t:t+1]\n",
    "            output, prev_state = self.decoder(input, prev_state, encoder_outputs)\n",
    "            # top1 = output.argmax(1).view(-1, 1)\n",
    "            # top1 = output[:, -1, :].argmax(1).view(-1, 1)\n",
    "            top1 = output.squeeze(1).argmax(1).view(-1,1)\n",
    "            # guardar cada salida (softmax)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            # outputs[:, t, :] = output\n",
    "            use_teacher = random.random() < teacher_forcing_ratio\n",
    "            input = decoder_input[:, t:t+1] if use_teacher else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Seq2Seq                                  [1, 50, 26342]            --\n",
       "├─Encoder: 1-1                           [1, 47, 1024]             --\n",
       "│    └─Embedding: 2-1                    [1, 47, 300]              (4,057,500)\n",
       "│    └─LayerNorm: 2-2                    [1, 47, 300]              600\n",
       "│    └─LSTM: 2-3                         [1, 47, 1024]             22,233,088\n",
       "├─Decoder: 1-2                           [1, 1, 26342]             --\n",
       "│    └─Embedding: 2-4                    [1, 1, 300]               7,902,600\n",
       "│    └─LayerNorm: 2-5                    [1, 1, 300]               600\n",
       "│    └─Dropout: 2-6                      [1, 1, 300]               --\n",
       "│    └─Attention: 2-7                    [1, 47]                   --\n",
       "│    │    └─Linear: 3-1                  [1, 47, 512]              786,944\n",
       "│    │    └─Linear: 3-2                  [1, 47, 1]                512\n",
       "│    └─LSTM: 2-8                         [1, 1, 512]               10,067,968\n",
       "│    └─Linear: 2-9                       [1, 1, 26342]             48,390,254\n",
       "├─Decoder: 1-3                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-10                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-11                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-12                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-13                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-3                  [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-4                  [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-14                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-15                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-4                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-16                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-17                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-18                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-19                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-5                  [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-6                  [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-20                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-21                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-5                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-22                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-23                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-24                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-25                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-7                  [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-8                  [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-26                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-27                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-6                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-28                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-29                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-30                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-31                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-9                  [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-10                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-32                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-33                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-7                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-34                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-35                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-36                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-37                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-11                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-12                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-38                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-39                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-8                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-40                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-41                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-42                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-43                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-13                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-14                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-44                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-45                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-9                           [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-46                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-47                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-48                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-49                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-15                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-16                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-50                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-51                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-10                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-52                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-53                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-54                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-55                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-17                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-18                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-56                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-57                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-11                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-58                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-59                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-60                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-61                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-19                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-20                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-62                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-63                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-12                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-64                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-65                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-66                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-67                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-21                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-22                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-68                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-69                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-13                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-70                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-71                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-72                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-73                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-23                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-24                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-74                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-75                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-14                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-76                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-77                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-78                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-79                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-25                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-26                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-80                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-81                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-15                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-82                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-83                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-84                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-85                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-27                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-28                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-86                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-87                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-16                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-88                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-89                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-90                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-91                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-29                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-30                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-92                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-93                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-17                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-94                   [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-95                   [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-96                     [1, 1, 300]               --\n",
       "│    └─Attention: 2-97                   [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-31                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-32                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-98                        [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-99                      [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-18                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-100                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-101                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-102                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-103                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-33                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-34                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-104                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-105                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-19                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-106                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-107                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-108                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-109                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-35                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-36                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-110                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-111                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-20                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-112                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-113                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-114                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-115                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-37                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-38                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-116                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-117                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-21                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-118                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-119                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-120                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-121                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-39                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-40                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-122                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-123                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-22                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-124                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-125                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-126                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-127                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-41                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-42                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-128                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-129                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-23                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-130                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-131                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-132                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-133                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-43                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-44                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-134                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-135                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-24                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-136                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-137                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-138                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-139                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-45                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-46                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-140                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-141                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-25                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-142                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-143                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-144                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-145                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-47                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-48                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-146                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-147                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-26                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-148                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-149                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-150                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-151                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-49                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-50                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-152                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-153                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-27                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-154                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-155                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-156                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-157                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-51                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-52                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-158                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-159                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-28                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-160                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-161                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-162                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-163                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-53                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-54                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-164                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-165                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-29                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-166                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-167                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-168                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-169                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-55                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-56                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-170                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-171                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-30                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-172                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-173                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-174                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-175                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-57                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-58                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-176                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-177                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-31                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-178                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-179                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-180                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-181                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-59                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-60                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-182                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-183                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-32                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-184                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-185                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-186                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-187                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-61                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-62                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-188                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-189                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-33                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-190                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-191                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-192                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-193                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-63                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-64                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-194                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-195                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-34                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-196                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-197                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-198                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-199                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-65                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-66                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-200                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-201                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-35                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-202                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-203                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-204                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-205                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-67                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-68                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-206                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-207                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-36                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-208                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-209                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-210                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-211                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-69                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-70                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-212                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-213                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-37                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-214                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-215                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-216                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-217                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-71                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-72                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-218                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-219                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-38                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-220                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-221                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-222                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-223                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-73                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-74                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-224                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-225                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-39                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-226                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-227                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-228                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-229                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-75                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-76                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-230                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-231                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-40                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-232                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-233                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-234                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-235                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-77                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-78                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-236                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-237                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-41                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-238                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-239                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-240                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-241                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-79                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-80                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-242                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-243                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-42                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-244                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-245                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-246                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-247                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-81                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-82                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-248                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-249                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-43                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-250                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-251                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-252                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-253                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-83                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-84                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-254                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-255                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-44                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-256                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-257                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-258                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-259                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-85                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-86                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-260                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-261                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-45                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-262                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-263                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-264                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-265                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-87                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-88                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-266                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-267                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-46                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-268                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-269                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-270                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-271                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-89                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-90                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-272                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-273                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-47                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-274                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-275                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-276                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-277                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-91                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-92                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-278                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-279                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-48                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-280                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-281                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-282                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-283                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-93                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-94                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-284                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-285                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-49                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-286                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-287                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-288                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-289                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-95                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-96                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-290                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-291                     [1, 1, 26342]             (recursive)\n",
       "├─Decoder: 1-50                          [1, 1, 26342]             (recursive)\n",
       "│    └─Embedding: 2-292                  [1, 1, 300]               (recursive)\n",
       "│    └─LayerNorm: 2-293                  [1, 1, 300]               (recursive)\n",
       "│    └─Dropout: 2-294                    [1, 1, 300]               --\n",
       "│    └─Attention: 2-295                  [1, 47]                   (recursive)\n",
       "│    │    └─Linear: 3-97                 [1, 47, 512]              (recursive)\n",
       "│    │    └─Linear: 3-98                 [1, 47, 1]                (recursive)\n",
       "│    └─LSTM: 2-296                       [1, 1, 512]               (recursive)\n",
       "│    └─Linear: 2-297                     [1, 1, 26342]             (recursive)\n",
       "==========================================================================================\n",
       "Total params: 93,440,066\n",
       "Trainable params: 89,382,566\n",
       "Non-trainable params: 4,057,500\n",
       "Total mult-adds (Units.GIGABYTES): 4.34\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 20.82\n",
       "Params size (MB): 373.76\n",
       "Estimated Total Size (MB): 394.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=nb_words)\n",
    "if cuda: encoder.cuda()\n",
    "decoder = Decoder(vocab_size=num_words_output, output_dim=num_words_output)\n",
    "if cuda: decoder.cuda()\n",
    "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
    "if cuda: seq2seq.cuda()\n",
    "\n",
    "# Diferentes learning rates para hacer fine tuning al embedding en español de Fasttext\n",
    "emb_params   = list(decoder.embedding.parameters())            # solo embedding ES\n",
    "other_params = [p for n, p in seq2seq.named_parameters()\n",
    "                if \"decoder.embedding\" not in n]               # resto de la red\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": emb_params,   \"lr\": lr*0.01},   # LR pequeño para embedding\n",
    "        {\"params\": other_params, \"lr\": lr},   # LR normal resto\n",
    "    ],\n",
    "    betas=(0.9, 0.98),\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # Omito el padding para que no falsee la metrica\n",
    "\n",
    "x_enc = data_set[0:1][0]\n",
    "x_dec = data_set[0:1][1]\n",
    "summary(seq2seq,\n",
    "        input_data=(x_enc.to(device).long(), x_dec.to(device).long()),\n",
    "        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597334,
     "status": "ok",
     "timestamp": 1655153778405,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "VDB0KWIegt8s",
    "outputId": "d1e002a8-fa8e-4dfc-f144-a95732026f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train L=6.6340 A=0.1598 | Val L=6.1579 A=0.1899\n",
      "Epoch   2: Train L=5.6769 A=0.2211 | Val L=5.0509 A=0.2772\n",
      "Epoch   3: Train L=4.5865 A=0.3149 | Val L=4.3364 A=0.3535\n",
      "Epoch   4: Train L=3.9229 A=0.3755 | Val L=4.0507 A=0.3867\n",
      "Epoch   5: Train L=3.5178 A=0.4146 | Val L=3.9030 A=0.4013\n",
      "Epoch   6: Train L=3.2387 A=0.4462 | Val L=3.8636 A=0.4082\n",
      "Epoch   7: Train L=3.0330 A=0.4743 | Val L=3.8860 A=0.4056\n",
      "Epoch   8: Train L=2.8760 A=0.4985 | Val L=3.8758 A=0.4152\n",
      "Epoch   9: Train L=2.7399 A=0.5228 | Val L=3.8919 A=0.4204\n",
      "Epoch  10: Train L=2.6391 A=0.5424 | Val L=3.9234 A=0.4251\n"
     ]
    }
   ],
   "source": [
    "history, model = train(seq2seq,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                epochs=100,\n",
    "                device=device,\n",
    "                patience=6\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1655154657801,
     "user": {
      "displayName": "Hernán Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "pZzm3tx059Zv",
    "outputId": "b7a08ad6-392e-4e40-8491-fb707d23254c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_count = range(1, len(history['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=history['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=history['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbwn0ekDy_s2"
   },
   "source": [
    "### 5 - Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71XeCtfYmOFx"
   },
   "outputs": [],
   "source": [
    "# Armar lo conversores de indice a palabra:\n",
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_text, \n",
    "                       model, \n",
    "                       input_tokenizer, \n",
    "                       word2idx_outputs, \n",
    "                       idx2word_target,\n",
    "                       max_input_len,\n",
    "                       max_output_len,\n",
    "                       device):\n",
    "    model.eval()\n",
    "    # 1) Tokenizar y paddear\n",
    "    seq = input_tokenizer.texts_to_sequences([input_text.lower()])[0]\n",
    "    seq = pad_sequences([seq], maxlen=max_input_len, padding='post')\n",
    "    encoder_input = torch.tensor(seq, dtype=torch.long).to(device)      # [1, max_input_len]\n",
    "    # 2) Pasar por el encoder\n",
    "    # prev_state = model.encoder(encoder_input)                           # (h, c)\n",
    "    encoder_outputs, prev_state = model.encoder(encoder_input)\n",
    "    # 3) Iniciar decoder con <sos>\n",
    "    sos = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    decoder_input = torch.tensor([[sos]], dtype=torch.long).to(device)  # [1, 1]\n",
    "    output_words = []\n",
    "    # 4) Loop hasta max_output_len o hasta <eos>\n",
    "    for _ in range(max_output_len):\n",
    "        # logits, prev_state = model.decoder(decoder_input, prev_state)\n",
    "        logits, prev_state = model.decoder(decoder_input, prev_state, encoder_outputs)\n",
    "        # logits: [1, 1, vocab_size]\n",
    "        logits = logits.squeeze(1)           # [1, vocab_size]\n",
    "        topi = logits.argmax(dim=1)          # [1]\n",
    "        idx = topi.item()                    # entero\n",
    "        if idx == eos:\n",
    "            break\n",
    "        output_words.append(idx2word_target[idx])\n",
    "        # re-alimentar al decoder\n",
    "        decoder_input = topi.unsqueeze(1)    # [1, 1]\n",
    "    return ' '.join(output_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:    My mother say hi.\n",
      "Output:   \n"
     ]
    }
   ],
   "source": [
    "input_test = \"My mother say hi.\"\n",
    "translation = translate_sentence(\n",
    "    input_text=input_test,\n",
    "    model=model,\n",
    "    input_tokenizer=input_tokenizer,\n",
    "    word2idx_outputs=word2idx_outputs,\n",
    "    idx2word_target=idx2word_target,\n",
    "    max_input_len=max_input_len,\n",
    "    max_output_len=max_out_len,  \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Input:   \", input_test)\n",
    "print(\"Output:  \", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:    Tom is sleeping, isn't he?\n",
      "Output:   y la las fraseo\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_test = input_sentences[i:i+1][0]\n",
    "translation = translate_sentence(\n",
    "    input_text=input_test,\n",
    "    model=model,\n",
    "    input_tokenizer=input_tokenizer,\n",
    "    word2idx_outputs=word2idx_outputs,\n",
    "    idx2word_target=idx2word_target,\n",
    "    max_input_len=max_input_len,\n",
    "    max_output_len=max_out_len,   \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Input:   \", input_test)\n",
    "print(\"Output:  \", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def translate_beam_search(input_text,\n",
    "                          model,\n",
    "                          input_tokenizer,\n",
    "                          word2idx_outputs,\n",
    "                          idx2word_target,\n",
    "                          max_input_len,\n",
    "                          max_output_len,\n",
    "                          device,\n",
    "                          beam_width=3,\n",
    "                          min_length=3):\n",
    "    \"\"\"\n",
    "    Decoding con beam search para mejorar coherencia.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 1) Tokenizar y paddear\n",
    "    seq = input_tokenizer.texts_to_sequences([input_text.lower()])[0]\n",
    "    seq = pad_sequences([seq], maxlen=max_input_len, padding='post')\n",
    "    encoder_input = torch.tensor(seq, dtype=torch.long).to(device)  # [1, T_src]\n",
    "    # 2) Pasar por encoder\n",
    "    encoder_outputs, prev_state = model.encoder(encoder_input)\n",
    "    # 3) Preparar tokens especiales\n",
    "    sos = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    \n",
    "    # inicializar beam: lista de tuples (seq_ids, state, score)\n",
    "    beams = [([sos], prev_state, 0.0)]\n",
    "    \n",
    "    for _ in range(max_output_len):\n",
    "        all_candidates = []\n",
    "        for seq_ids, state, score in beams:\n",
    "            last_id = seq_ids[-1]\n",
    "            if last_id == eos and len(seq_ids) > min_length:\n",
    "                # si ya terminó y cumple longitud mínima, conservar\n",
    "                all_candidates.append((seq_ids, state, score))\n",
    "                continue\n",
    "            input_tok = torch.tensor([[last_id]], device=device)\n",
    "            logits, next_state = model.decoder(input_tok, state, encoder_outputs)\n",
    "            log_probs = F.log_softmax(logits.squeeze(1), dim=1)[0]  # [vocab_size]\n",
    "            top_probs, top_idxs = log_probs.topk(beam_width)\n",
    "            for p, idx in zip(top_probs.tolist(), top_idxs.tolist()):\n",
    "                new_seq = seq_ids + [idx]\n",
    "                new_score = score + p\n",
    "                all_candidates.append((new_seq, next_state, new_score))\n",
    "        \n",
    "        # seleccionar mejores beams\n",
    "        beams = sorted(all_candidates, key=lambda x: x[2], reverse=True)[:beam_width]\n",
    "        # si todas terminaron con <eos>, salir\n",
    "        if all((seq_ids[-1] == eos and len(seq_ids) > min_length) for seq_ids, _, _ in beams):\n",
    "            break\n",
    "\n",
    "    # mejor hipótesis\n",
    "    best_seq = beams[0][0]\n",
    "    # convertir a words, ignorando SOS y EOS\n",
    "    words = [idx2word_target[id] for id in best_seq if id not in {sos, eos}]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# translation = translate_beam_search(\n",
    "#     \"Tom is sleeping, isn't he?\",\n",
    "#     seq2seq, input_tokenizer, word2idx_outputs, idx2word_target,\n",
    "#     max_input_len, max_output_len, device, beam_width=5\n",
    "# )\n",
    "# print(\"Beam:\", translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam: y la las fraseo\n"
     ]
    }
   ],
   "source": [
    "translation = translate_beam_search(\n",
    "    \"Tom is sleeping, isn't he?\",\n",
    "    seq2seq, input_tokenizer, word2idx_outputs, idx2word_target,\n",
    "    max_input_len, max_out_len, device, beam_width=5\n",
    ")\n",
    "print(\"Beam:\", translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trabajó sobre el ejemplo de encoder-decoder para traducción portado de keras a pytorch con el fin de optimixar el uso de GPU para el setup. \\\n",
    "Se incrementó la red y se le agregaron componentes como normalización, dropout y atención con el propósito de mejorar el modelo. Se utilizó como refenrencia https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se omitió el uso de hot_one encoding para optimizar el consumo de memoria y asi poder usar todo el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizaron embeddings pre entrenados de FastText de 300 dims para el inglés y el español, con el fin de aumentar la capacidad de representación semántica. \\\n",
    "Para el caso del español se dejo un pequeño learning rate con el fin de alinear los espacios vectoriales haciendo fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lamentablemente no se logró mejoras respecto a lo implementado con keras, de esta forma podemos decir que el mejor camino es hacer un cambio de arquitectura a encoder-decoder usando Transformers."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMniDMuqoKT0lLVAJWxpRSt",
   "collapsed_sections": [],
   "name": "6c - traductor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
