{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27ab205",
   "metadata": {},
   "source": [
    "## üöÄ Modificaciones aplicadas\n",
    "\n",
    "1. **Teacher Forcing Ratio**  \n",
    "   - Se a√±adi√≥ el par√°metro `teacher_forcing_ratio` a la funci√≥n `train` para alternar entre token real y predicho, reduciendo el _exposure bias_.  \n",
    "2. **LSTM Bidireccional en Encoder**  \n",
    "   - El `nn.LSTM` del encoder ahora usa `bidirectional=True` y concatena estados forward/backward.  \n",
    "3. **Gradient Clipping y Scheduler**  \n",
    "   - Se incorpora `torch.nn.utils.clip_grad_norm_` tras el _backward_ y un ejemplo de uso de `ReduceLROnPlateau`.  \n",
    "4. **M√©trica BLEU y Ejemplos de Inferencia**  \n",
    "   - A√±adido c√°lculo de BLEU tras cada epoch de validaci√≥n y generaci√≥n de ejemplos.  \n",
    "5. **TorchText para Batching**  \n",
    "   - Ejemplo de configuraci√≥n de `Field` y `BucketIterator` para manejo autom√°tico de padding y longitudes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcf5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Mod: Entrenamiento con Teacher Forcing y Gradient Clipping ---\n",
    "# import random\n",
    "# from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# def train(model, iterator, optimizer, criterion, clip=1.0, teacher_forcing_ratio=0.5):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     for i, batch in enumerate(iterator):\n",
    "#         src = batch.src      # [src_len, batch_size]\n",
    "#         trg = batch.trg      # [trg_len, batch_size]\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass con teacher forcing ratio\n",
    "#         output = model(src, trg, teacher_forcing_ratio)\n",
    "#         # output: [trg_len, batch_size, output_dim]\n",
    "#         output_dim = output.shape[-1]\n",
    "#         output = output[1:].view(-1, output_dim)   # excluir <sos>\n",
    "#         trg = trg[1:].view(-1)\n",
    "\n",
    "#         loss = criterion(output, trg)\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Aplicar gradient clipping\n",
    "#         clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839641cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Mod: Encoder Bidireccional ---\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "#         super().__init__()\n",
    "#         self.hid_dim = hid_dim\n",
    "#         self.n_layers = n_layers\n",
    "\n",
    "#         self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "#         # Cambiado a bidireccional\n",
    "#         self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers,\n",
    "#                            dropout=dropout, bidirectional=True)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, src):\n",
    "#         # src: [src_len, batch_size]\n",
    "#         embedded = self.dropout(self.embedding(src))\n",
    "#         # outputs: [src_len, batch_size, hid_dim*2]\n",
    "#         outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "#         # Concatenar hidden forward y backward\n",
    "#         hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "#         cell   = torch.cat((cell[-2,:,:],   cell[-1,:,:]),   dim=1)\n",
    "\n",
    "#         # Ajustar dimensiones para el decoder\n",
    "#         return outputs, hidden.unsqueeze(0), cell.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523a1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Mod: Ejemplo de configuraci√≥n con TorchText ---\n",
    "# from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "# # Definici√≥n de campos con SOS/EOS y manejo de longitudes\n",
    "# SRC = Field(tokenize=lambda x: x.split(), \n",
    "#             init_token='<sos>', eos_token='<eos>', \n",
    "#             lower=True, include_lengths=True)\n",
    "# TRG = Field(tokenize=lambda x: x.split(),\n",
    "#             init_token='<sos>', eos_token='<eos>', \n",
    "#             lower=True)\n",
    "\n",
    "# # Cargar dataset desde CSV/TSV (adaptar rutas y formato)\n",
    "# # train_data, valid_data = TabularDataset.splits(\n",
    "# #     path='.', train='train.csv', validation='valid.csv',\n",
    "# #     format='csv', fields=[('src', SRC), ('trg', TRG)]\n",
    "# # )\n",
    "\n",
    "# # Construir vocabularios limitados y m√≠nimos\n",
    "# SRC.build_vocab(train_data, max_size=20000, min_freq=2)\n",
    "# TRG.build_vocab(train_data, max_size=30000, min_freq=2)\n",
    "\n",
    "# # Generar iteradores con sorting y padding autom√°tico\n",
    "# train_iter, valid_iter = BucketIterator.splits(\n",
    "#     (train_data, valid_data),\n",
    "#     batch_size=32,\n",
    "#     sort_within_batch=True,\n",
    "#     sort_key=lambda x: len(x.src),\n",
    "#     device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfa39F4lsLf3"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## LSTM Traductor\n",
    "Ejemplo basado en [LINK](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqO0PRcFsPTe"
   },
   "source": [
    "### Datos\n",
    "El objecto es utilizar datos disponibles de Anki de traducciones de texto en diferentes idiomas. Se construir√° un modelo traductor seq2seq utilizando encoder-decoder.\\\n",
    "[LINK](https://www.manythings.org/anki/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cq3YXak9sGHd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random  \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©trica de accuracy con m√°scara de padding\n",
    "def sequence_acc(logits, tgt_idx, pad_idx=0):\n",
    "    \"\"\"\n",
    "    logits : [B, L, V]  salida del modelo\n",
    "    tgt_idx: [B, L]     √≠ndices ground-truth\n",
    "    \"\"\"\n",
    "    pred_idx = logits.argmax(dim=-1)\n",
    "    mask     = tgt_idx.ne(pad_idx)            # True donde hay palabra\n",
    "    correct  = (pred_idx.eq(tgt_idx) & mask).float()\n",
    "    total = mask.sum()\n",
    "    return correct.sum() / total if total > 0 else torch.tensor(0.0)\n",
    "\n",
    "# Funci√≥n de entrenamiento\n",
    "def train(model,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          optimizer,\n",
    "          criterion,              \n",
    "          epochs     = 50,\n",
    "          patience   = 7,\n",
    "          min_delta  = 1e-4,\n",
    "          device     = \"cuda\",\n",
    "          pad_idx    = 0,\n",
    "          teacher_forcing_ratio = 0.5):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    wait          = 0\n",
    "    best_state    = None\n",
    "    history = {\"loss\": [], \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ----- TRAIN -----\n",
    "        model.train()\n",
    "        tot_loss, tot_acc = 0, 0\n",
    "        for enc_in, dec_in, tgt_idx in train_loader:\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            tgt_idx  = tgt_idx.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # logits = model(enc_in, dec_in)            # [B, L, V]\n",
    "            logits = model(enc_in, dec_in, teacher_forcing_ratio)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(-1, V), tgt_idx.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item()\n",
    "            tot_acc  += sequence_acc(logits, tgt_idx, pad_idx).item()\n",
    "        train_loss = tot_loss / len(train_loader)\n",
    "        train_acc  = tot_acc  / len(train_loader)\n",
    "\n",
    "        # ----- VALID -----\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for enc_in, dec_in, tgt_idx in valid_loader:\n",
    "                enc_in  = enc_in.to(device)\n",
    "                dec_in  = dec_in.to(device)\n",
    "                tgt_idx = tgt_idx.to(device)\n",
    "                logits = model(enc_in, dec_in)\n",
    "                B, L, V = logits.shape\n",
    "                loss = criterion(logits.view(-1, V), tgt_idx.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                val_acc  += sequence_acc(logits, tgt_idx, pad_idx).item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_acc  /= len(valid_loader)\n",
    "        history[\"loss\"].append(train_loss)\n",
    "        history[\"accuracy\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:3d}: \"\n",
    "              f\"Train L={train_loss:.4f} A={train_acc:.4f} | \"\n",
    "              f\"Val L={val_loss:.4f} A={val_acc:.4f}\")\n",
    "\n",
    "        # ----- EARLY STOPPING -----\n",
    "        if val_loss + min_delta < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state    = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BFiCH8nxoIY"
   },
   "source": [
    "### 1 - Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1655153154355,
     "user": {
      "displayName": "Hern√°n Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "-9aNLZBDtA5J",
    "outputId": "68de3ded-aa96-40fe-fab5-8083525e8c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows disponibles: 118964\n",
      "Cantidad de rows utilizadas: 118964\n",
      "A deal is a deal. Un trato es un trato. <eos> <sos> Un trato es un trato.\n"
     ]
    }
   ],
   "source": [
    "# dataset_file\n",
    "text_file = \"./clase/spa-eng/spa.txt\"\n",
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# Por limitaciones de RAM no se leen todas las filas\n",
    "MAX_NUM_SENTENCES = 0\n",
    "\n",
    "# Mezclar el dataset, forzar semilla siempre igual\n",
    "np.random.seed([40])\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "count = 0\n",
    "\n",
    "if MAX_NUM_SENTENCES == 0:\n",
    "    MAX_NUM_SENTENCES = len(lines)\n",
    "for line in lines:\n",
    "    count += 1\n",
    "    if count > MAX_NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    # Input sentence --> eng\n",
    "    # output --> spa\n",
    "    input_sentence, output = line.rstrip().split('\\t')\n",
    "\n",
    "    # output sentence (decoder_output) tiene <eos>\n",
    "    output_sentence = output + ' <eos>'\n",
    "    # output sentence input (decoder_input) tiene <sos>\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"Cantidad de rows disponibles:\", len(lines))\n",
    "print(\"Cantidad de rows utilizadas:\", len(input_sentences))\n",
    "print(input_sentences[0], output_sentences[0], output_sentences_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P-ynUNP5xp6"
   },
   "source": [
    "### 2 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5WAZGOTfGyha"
   },
   "outputs": [],
   "source": [
    "# Definir el tama√±o m√°ximo del vocabulario\n",
    "MAX_VOCAB_SIZE = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1655153154356,
     "user": {
      "displayName": "Hern√°n Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "eF1W6peoFGXA",
    "outputId": "e748ad10-0c8d-4bca-ab4c-76f1974e12cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario: 13524\n",
      "Sentencia de entrada m√°s larga: 47\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar las palabras con el Tokenizer de Keras\n",
    "# Definir una m√°xima cantidad de palabras a utilizar:\n",
    "# - num_words --> the maximum number of words to keep, based on word frequency.\n",
    "# - Only the most common num_words-1 words will be kept.\n",
    "from clase.torch_helpers import Tokenizer\n",
    "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Sentencia de entrada m√°s larga:\", max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1655153154936,
     "user": {
      "displayName": "Hern√°n Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "zBzdKiTVIBYY",
    "outputId": "f313cf87-642e-4671-b88d-90ecd0e80c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario: 26341\n",
      "Sentencia de salida m√°s larga: 50\n"
     ]
    }
   ],
   "source": [
    "# A los filtros de s√≠mbolos del Tokenizer agregamos el \"¬ø\",\n",
    "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
    "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¬ø?@[\\\\]^_`{|}~\\t\\n')\n",
    "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
    "\n",
    "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Sentencia de salida m√°s larga:\", max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xqb8ZJ4sJHgv"
   },
   "source": [
    "Como era de esperarse, las sentencias en castellano son m√°s largas que en ingl√©s, y lo mismo sucede con su vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGOn9N57IuYz"
   },
   "source": [
    "A la hora de realiza padding es importante teneer en cuenta que en el encoder los ceros se agregan al comienoz y en el decoder al final. Esto es porque la salida del encoder est√° basado en las √∫ltimas palabras de la sentencia (son las m√°s importantes), mientras que en el decoder est√° basado en el comienzo de la secuencia de salida ya que es la realimentaci√≥n del sistema y termina con fin de sentencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1655153154937,
     "user": {
      "displayName": "Hern√°n Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "q0Ob4hAWJkcv",
    "outputId": "9152d151-b863-49c9-e527-940d37a85385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows del dataset: 118964\n",
      "encoder_input_sequences shape: (118964, 47)\n",
      "decoder_input_sequences shape: (118964, 50)\n",
      "decoder_output_sequences shape: (118964, 50)\n"
     ]
    }
   ],
   "source": [
    "from clase.torch_helpers import pad_sequences\n",
    "print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n",
    "\n",
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
    "\n",
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)\n",
    "\n",
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_output_sequences shape:\", decoder_output_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos un dataset sin one_hot encoding para ocupar menos memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_size: 47\n",
      "decoder_input_size: 50\n",
      "Output dim 50\n"
     ]
    }
   ],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Devuelve tensores int64 con PAD=0.\n",
    "    ‚îÄ encoder_inputs : [N, T_enc]\n",
    "    ‚îÄ decoder_inputs : [N, T_dec]   (<sos> + frase)\n",
    "    ‚îÄ decoder_outputs: [N, T_dec]   (frase + <eos>)\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_arr: np.ndarray,\n",
    "                       dec_in_arr: np.ndarray,\n",
    "                       dec_out_arr: np.ndarray):\n",
    "\n",
    "        # Convertir a tensores int64\n",
    "        self.encoder_inputs  = torch.from_numpy(enc_arr    .astype(np.int64))\n",
    "        self.decoder_inputs  = torch.from_numpy(dec_in_arr .astype(np.int64))\n",
    "        self.decoder_outputs = torch.from_numpy(dec_out_arr.astype(np.int64))\n",
    "\n",
    "        assert len(self.encoder_inputs) == len(self.decoder_inputs) == len(self.decoder_outputs), \\\n",
    "            \"Los tres arrays deben tener la misma longitud\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.encoder_inputs [idx],\n",
    "                self.decoder_inputs [idx],\n",
    "                self.decoder_outputs[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_inputs)\n",
    "    \n",
    "# Construcci√≥n\n",
    "data_set = Seq2SeqDataset(encoder_input_sequences,\n",
    "                          decoder_input_sequences,\n",
    "                          decoder_output_sequences)\n",
    "\n",
    "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
    "print(\"encoder_input_size:\", encoder_input_size)\n",
    "\n",
    "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
    "print(\"decoder_input_size:\", decoder_input_size)\n",
    "\n",
    "output_dim = data_set.decoder_outputs.shape[1]\n",
    "print(\"Output dim\", output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1655153159536,
     "user": {
      "displayName": "Hern√°n Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "sUDPZeuAU1RI",
    "outputId": "f19e0632-7cfd-4671-dddc-edbcf715788f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del conjunto de entrenamiento: 95172\n",
      "Tama√±o del conjunto de validacion: 23792\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "valid_set_size = int(len(data_set) * 0.2)\n",
    "train_set_size = len(data_set) - valid_set_size\n",
    "\n",
    "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
    "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, len(data_set)))\n",
    "\n",
    "print(\"Tama√±o del conjunto de entrenamiento:\", len(train_set))\n",
    "print(\"Tama√±o del conjunto de validacion:\", len(valid_set))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CJIsLBbj6rg"
   },
   "source": [
    "### 3 - Preparar los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizan dos embeddings pre entrenados de Fastext para el ingles y el espa√±ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Mosj2-x-kXBK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "model_embeddings = load_facebook_vectors(\"clase/cc.en.300.bin\")  \n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "model_embeddings_es = load_facebook_vectors(\"clase/cc.es.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13525, 300)\n",
      "(26342, 300)\n"
     ]
    }
   ],
   "source": [
    "vocab_size_in  = max(word2idx_inputs .values()) + 1   \n",
    "vocab_size_out = max(word2idx_outputs.values()) + 1   \n",
    "embed_dim = 300\n",
    "# Ingl√©s\n",
    "embedding_matrix = np.zeros((vocab_size_in, embed_dim), dtype=np.float32)\n",
    "for w, idx in word2idx_inputs.items():\n",
    "    # idx incluye todos los valores hasta max_index\n",
    "    if w in model_embeddings:\n",
    "        embedding_matrix[idx] = model_embeddings[w]\n",
    "# Espa√±ol\n",
    "embedding_matrix_es = np.zeros((vocab_size_out, embed_dim), dtype=np.float32)\n",
    "for w, idx in word2idx_outputs.items():\n",
    "    if w in model_embeddings_es:\n",
    "        embedding_matrix_es[idx] = model_embeddings_es[w]\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix_es.shape)\n",
    "nb_words = embedding_matrix.shape[0]\n",
    "num_words_output = embedding_matrix_es.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vKbhjtIwPgM"
   },
   "source": [
    "### 4 - Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hid_dim*2 + dec_hid_dim, dec_hid_dim)\n",
    "        self.v    = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [B, dec_hid_dim]\n",
    "        # encoder_outputs: [T_src, B, enc_hid_dim*2]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        # repetir hidden T_src veces\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)      # [B, T_src, dec_hid_dim]\n",
    "        enc_out = encoder_outputs.permute(1,0,2)               # [B, T_src, enc_hid_dim*2]\n",
    "        energy  = torch.tanh(self.attn(torch.cat((hidden, enc_out), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)                  # [B, T_src]\n",
    "        return torch.softmax(attention, dim=1)                 # [B, T_src]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14732,
     "status": "ok",
     "timestamp": 1655153181107,
     "user": {
      "displayName": "Hern√°n Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "3fm3HCLMPSG-",
    "outputId": "39be3183-0a69-49a5-8aa1-9f24026edf32"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False  # marcar como layer no entrenable (freeze)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
    "                            num_layers=self.num_layers, dropout=0.2, bidirectional=True) # LSTM layer\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        lstm_output, (ht, ct) = self.lstm(out)\n",
    "        # return (ht, ct)\n",
    "        #    shape ‚Üí [num_layers, 2, B, hidden_size]\n",
    "        ht = ht.view(self.num_layers, 2, -1, self.lstm_size)\n",
    "        ct = ct.view(self.num_layers, 2, -1, self.lstm_size)\n",
    "        ht = ht.sum(dim=1)   # ‚Üí [num_layers, B, hidden_size]\n",
    "        ct = ct.sum(dim=1)   # ‚Üí [num_layers, B, hidden_size] \n",
    "        return lstm_output, (ht, ct) \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix_es))\n",
    "        self.embedding.weight.requires_grad = True  # Opcional: Freezar\n",
    "\n",
    "        self.attn = Attention(self.lstm_size, self.lstm_size)\n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
    "        #                     num_layers=self.num_layers, dropout=0.2) # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim + self.lstm_size*2,   # ‚Üê aqu√≠\n",
    "                            hidden_size=self.lstm_size,\n",
    "                            batch_first=True,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=0.2) # LSTM layer\n",
    "\n",
    "        # self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n",
    "        # self.fc1 = nn.Linear(in_features=self.lstm_size + self.lstm_size*2 + self.embedding_dim,\n",
    "        #                      out_features=self.output_dim)\n",
    "        self.fc1 = nn.Linear(in_features=self.lstm_size   # salida LSTM\n",
    "                            + self.lstm_size*2   # context vector\n",
    "                            + self.embedding_dim, \n",
    "                            out_features=self.output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, prev_state, encoder_outputs):\n",
    "        # out = self.dropout(self.embedding(x))\n",
    "        # lstm_output, (ht, ct) = self.lstm(out, prev_state)\n",
    "        # out = self.fc1(lstm_output)  # conserva la dimensi√≥n de secuencia\n",
    "\n",
    "        embedded = self.dropout(self.embedding(x))  # [B,1,emb_dim]\n",
    "        # 1) calcular pesos\n",
    "        h = prev_state[0]                           # [num_layers, B, hid_dim]\n",
    "        hidden_last = h[-1]                         # [B, hid_dim]\n",
    "        attn_weights = self.attn(hidden_last, encoder_outputs)  # [B, src_len]\n",
    "        # 2) context vector\n",
    "        attn_weights = attn_weights.unsqueeze(1)    # [B,1,src_len]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # [B,1,2*hid]\n",
    "        # 3) concatenar embed + context ‚Üí LSTM\n",
    "        lstm_in = torch.cat((embedded, context), dim=2)  # [B,1, emb+2*hid]\n",
    "        lstm_out, (ht, ct) = self.lstm(lstm_in, prev_state)\n",
    "        # 4) proyecci√≥n\n",
    "        #    concatenar [lstm_out; context; embedded] antes de fc1\n",
    "        cat = torch.cat((\n",
    "            lstm_out,      # [B,1, hid]\n",
    "            context,       # [B,1, 2*hid]\n",
    "            embedded       # [B,1, emb]\n",
    "        ), dim=2)         # [B,1, hid+2*hid+emb]\n",
    "        out = self.fc1(cat)  # [B,1, output_dim]\n",
    "\n",
    "        return out, (ht, ct)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, encoder_input, decoder_input, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = decoder_input.shape[0]\n",
    "        decoder_input_len = decoder_input.shape[1]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, decoder_input_len, vocab_size, device=encoder_input.device)\n",
    "        # ultimo hidden state del encoder, primer estado oculto del decoder\n",
    "        # prev_state = self.encoder(encoder_input)\n",
    "        encoder_outputs, prev_state = self.encoder(encoder_input)\n",
    "        # En la primera iteracion se toma el primer token de target (<sos>)\n",
    "        input = decoder_input[:, 0:1]\n",
    "        for t in range(1, decoder_input_len):\n",
    "            # input = decoder_input[:, t:t+1]\n",
    "            output, prev_state = self.decoder(input, prev_state, encoder_outputs)\n",
    "            # top1 = output.argmax(1).view(-1, 1)\n",
    "            # top1 = output[:, -1, :].argmax(1).view(-1, 1)\n",
    "            top1 = output.squeeze(1).argmax(1).view(-1,1)\n",
    "            # guardar cada salida (softmax)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            # outputs[:, t, :] = output\n",
    "            use_teacher = random.random() < teacher_forcing_ratio\n",
    "            input = decoder_input[:, t:t+1] if use_teacher else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Encoder: 1, Embedding: 2, LSTM: 2, Embedding: 2, Dropout: 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:295\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mSeq2Seq.forward\u001b[39m\u001b[34m(self, encoder_input, decoder_input, teacher_forcing_ratio)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, decoder_input_len):\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# input = decoder_input[:, t:t+1]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     output, prev_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# top1 = output.argmax(1).view(-1, 1)\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# top1 = output[:, -1, :].argmax(1).view(-1, 1)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, x, prev_state, encoder_outputs)\u001b[39m\n\u001b[32m     60\u001b[39m hidden_last = h[-\u001b[32m1\u001b[39m]                         \u001b[38;5;66;03m# [B, hid_dim]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, src_len]\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# 2) context vector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, hidden, encoder_outputs)\u001b[39m\n\u001b[32m     13\u001b[39m enc_out = encoder_outputs.permute(\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m2\u001b[39m)               \u001b[38;5;66;03m# [B, T_src, enc_hid_dim*2]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m energy  = torch.tanh(\u001b[38;5;28mself\u001b[39m.attn(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[32m     15\u001b[39m attention = \u001b[38;5;28mself\u001b[39m.v(energy).squeeze(\u001b[32m2\u001b[39m)                  \u001b[38;5;66;03m# [B, T_src]\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 47 for tensor number 1 in the list.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m x_enc = data_set[\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     25\u001b[39m x_dec = data_set[\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq2seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[39m, in \u001b[36msummary\u001b[39m\u001b[34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m validate_user_params(\n\u001b[32m    217\u001b[39m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[32m    218\u001b[39m )\n\u001b[32m    220\u001b[39m x, correct_input_size = process_input(\n\u001b[32m    221\u001b[39m     input_data, input_size, batch_dim, device, dtypes\n\u001b[32m    222\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m summary_list = \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m formatting = FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[32m    227\u001b[39m results = ModelStatistics(\n\u001b[32m    228\u001b[39m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[32m    229\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pablo\\IA-repos\\Desafios_NLP\\venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    303\u001b[39m     executed_layers = [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer.executed]\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Encoder: 1, Embedding: 2, LSTM: 2, Embedding: 2, Dropout: 2]"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=nb_words)\n",
    "if cuda: encoder.cuda()\n",
    "decoder = Decoder(vocab_size=num_words_output, output_dim=num_words_output)\n",
    "if cuda: decoder.cuda()\n",
    "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
    "if cuda: seq2seq.cuda()\n",
    "\n",
    "# Diferentes learning rates para hacer fine tuning al embedding en espa√±ol de Fasttext\n",
    "emb_params   = list(decoder.embedding.parameters())            # solo embedding ES\n",
    "other_params = [p for n, p in seq2seq.named_parameters()\n",
    "                if \"decoder.embedding\" not in n]               # resto de la red\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": emb_params,   \"lr\": lr*0.01},   # LR peque√±o para embedding\n",
    "        {\"params\": other_params, \"lr\": lr},   # LR normal resto\n",
    "    ],\n",
    "    betas=(0.9, 0.98),\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # Omito el padding para que no falsee la metrica\n",
    "\n",
    "x_enc = data_set[0:1][0]\n",
    "x_dec = data_set[0:1][1]\n",
    "summary(seq2seq,\n",
    "        input_data=(x_enc.to(device).long(), x_dec.to(device).long()),\n",
    "        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597334,
     "status": "ok",
     "timestamp": 1655153778405,
     "user": {
      "displayName": "Hern√°n Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "VDB0KWIegt8s",
    "outputId": "d1e002a8-fa8e-4dfc-f144-a95732026f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train L=6.6731 A=0.1510 | Val L=6.4847 A=0.1604\n",
      "Epoch   2: Train L=6.1999 A=0.1798 | Val L=6.0029 A=0.1979\n",
      "Epoch   3: Train L=5.7359 A=0.2099 | Val L=5.6482 A=0.2269\n",
      "Epoch   4: Train L=5.3427 A=0.2397 | Val L=5.3431 A=0.2574\n",
      "Epoch   5: Train L=4.9926 A=0.2683 | Val L=5.1410 A=0.2776\n",
      "Epoch   6: Train L=4.6897 A=0.2923 | Val L=4.9580 A=0.2957\n",
      "Epoch   7: Train L=4.4376 A=0.3121 | Val L=4.8522 A=0.3098\n",
      "Epoch   8: Train L=4.2216 A=0.3308 | Val L=4.7509 A=0.3214\n",
      "Epoch   9: Train L=4.0448 A=0.3468 | Val L=4.6753 A=0.3324\n",
      "Epoch  10: Train L=3.8869 A=0.3631 | Val L=4.6361 A=0.3400\n",
      "Epoch  11: Train L=3.7644 A=0.3762 | Val L=4.6137 A=0.3445\n",
      "Epoch  12: Train L=3.6390 A=0.3914 | Val L=4.5740 A=0.3500\n",
      "Epoch  13: Train L=3.5455 A=0.4024 | Val L=4.5873 A=0.3512\n",
      "Epoch  14: Train L=3.4553 A=0.4142 | Val L=4.5538 A=0.3604\n",
      "Epoch  15: Train L=3.3801 A=0.4235 | Val L=4.5653 A=0.3621\n",
      "Epoch  16: Train L=3.3140 A=0.4324 | Val L=4.5823 A=0.3591\n",
      "Epoch  17: Train L=3.2509 A=0.4412 | Val L=4.6300 A=0.3606\n",
      "Epoch  18: Train L=3.2001 A=0.4494 | Val L=4.5845 A=0.3617\n",
      "Epoch  19: Train L=3.1536 A=0.4549 | Val L=4.6072 A=0.3629\n",
      "Epoch  20: Train L=3.1091 A=0.4629 | Val L=4.6175 A=0.3679\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "history, model = train(seq2seq,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                epochs=100,\n",
    "                device=device,\n",
    "                patience=6\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1655154657801,
     "user": {
      "displayName": "Hern√°n Contigiani",
      "userId": "01142101934719343059"
     },
     "user_tz": 180
    },
    "id": "pZzm3tx059Zv",
    "outputId": "b7a08ad6-392e-4e40-8491-fb707d23254c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_count = range(1, len(history['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=history['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=history['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbwn0ekDy_s2"
   },
   "source": [
    "### 5 - Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "71XeCtfYmOFx"
   },
   "outputs": [],
   "source": [
    "# Armar lo conversores de indice a palabra:\n",
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_text, \n",
    "                       model, \n",
    "                       input_tokenizer, \n",
    "                       word2idx_outputs, \n",
    "                       idx2word_target,\n",
    "                       max_input_len,\n",
    "                       max_output_len,\n",
    "                       device):\n",
    "    model.eval()\n",
    "    # 1) Tokenizar y paddear\n",
    "    seq = input_tokenizer.texts_to_sequences([input_text.lower()])[0]\n",
    "    seq = pad_sequences([seq], maxlen=max_input_len, padding='post')\n",
    "    encoder_input = torch.tensor(seq, dtype=torch.long).to(device)      # [1, max_input_len]\n",
    "    # 2) Pasar por el encoder\n",
    "    prev_state = model.encoder(encoder_input)                           # (h, c)\n",
    "    # 3) Iniciar decoder con <sos>\n",
    "    sos = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    decoder_input = torch.tensor([[sos]], dtype=torch.long).to(device)  # [1, 1]\n",
    "    output_words = []\n",
    "    # 4) Loop hasta max_output_len o hasta <eos>\n",
    "    for _ in range(max_output_len):\n",
    "        logits, prev_state = model.decoder(decoder_input, prev_state)\n",
    "        # logits: [1, 1, vocab_size]\n",
    "        logits = logits.squeeze(1)           # [1, vocab_size]\n",
    "        topi = logits.argmax(dim=1)          # [1]\n",
    "        idx = topi.item()                    # entero\n",
    "        if idx == eos:\n",
    "            break\n",
    "        output_words.append(idx2word_target[idx])\n",
    "        # re-alimentar al decoder\n",
    "        decoder_input = topi.unsqueeze(1)    # [1, 1]\n",
    "    return ' '.join(output_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:    My mother say hi.\n",
      "Output:   lo\n"
     ]
    }
   ],
   "source": [
    "input_test = \"My mother say hi.\"\n",
    "translation = translate_sentence(\n",
    "    input_text=input_test,\n",
    "    model=model,\n",
    "    input_tokenizer=input_tokenizer,\n",
    "    word2idx_outputs=word2idx_outputs,\n",
    "    idx2word_target=idx2word_target,\n",
    "    max_input_len=max_input_len,\n",
    "    max_output_len=max_out_len,  \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Input:   \", input_test)\n",
    "print(\"Output:  \", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:    I went to see a movie with Tom after work.\n",
      "Output:   la las de de\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_test = input_sentences[i:i+1][0]\n",
    "translation = translate_sentence(\n",
    "    input_text=input_test,\n",
    "    model=model,\n",
    "    input_tokenizer=input_tokenizer,\n",
    "    word2idx_outputs=word2idx_outputs,\n",
    "    idx2word_target=idx2word_target,\n",
    "    max_input_len=max_input_len,\n",
    "    max_output_len=max_out_len,   \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Input:   \", input_test)\n",
    "print(\"Output:  \", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se prob√≥ con redes m√°s chicas de una capa, dos y tres. \n",
    "- Ampliando el estado oculto, con y sin dropout. \n",
    "- Sin filtrar le token de padding en el crossEntropy, el modelo daba mucho mejor accuracy, pero traducia muy mal. \n",
    "- Filtrando el padding, el accuracy da extremadamente bajo y sigue traduciendo igual de mal."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMniDMuqoKT0lLVAJWxpRSt",
   "collapsed_sections": [],
   "name": "6c - traductor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
