{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4b - one-to-many.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP5Jfak4WbM+yEndU6ANyPU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NEnBiuLcukJc"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## RNN one-to-many"]},{"cell_type":"markdown","metadata":{"id":"i96B2RF8uqEb"},"source":["#### Datos\n","El objecto es utilizar una serie de sucuencias númericas (datos sintéticos) para poner a prueba el uso de las redes RNN. Este ejemplo se inspiró en otro artículo, lo tienen como referencia en el siguiente link:\\\n","[LINK](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/)"]},{"cell_type":"code","metadata":{"id":"Lx0HQ-1RvJw9","executionInfo":{"status":"ok","timestamp":1656680068284,"user_tz":180,"elapsed":4629,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}}},"source":["import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# torchsummar actualmente tiene un problema con las LSTM, por eso\n","# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n","!pip3 install torchinfo\n","from torchinfo import summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7yONnycbZ9kZ","executionInfo":{"status":"ok","timestamp":1656680078222,"user_tz":180,"elapsed":9954,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"390eecd7-f5de-45cd-eec9-b5dd57a73952"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.0\n"]}]},{"cell_type":"code","source":["import os\n","import platform\n","\n","if os.access('torch_helpers.py', os.F_OK) is False:\n","    if platform.system() == 'Windows':\n","        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n","    else:\n","        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKC5SmeuTFPv","executionInfo":{"status":"ok","timestamp":1656680078783,"user_tz":180,"elapsed":578,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"984a5fbc-368a-4198-d777-81572ad9b0ad"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-01 12:54:37--  http://torch_helpers.py/\n","Resolving torch_helpers.py (torch_helpers.py)... failed: Name or service not known.\n","wget: unable to resolve host address ‘torch_helpers.py’\n","--2022-07-01 12:54:37--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 23883 (23K) [text/plain]\n","Saving to: ‘torch_helpers.py’\n","\n","\rtorch_helpers.py      0%[                    ]       0  --.-KB/s               \rtorch_helpers.py    100%[===================>]  23.32K  --.-KB/s    in 0s      \n","\n","2022-07-01 12:54:37 (108 MB/s) - ‘torch_helpers.py’ saved [23883/23883]\n","\n","FINISHED --2022-07-01 12:54:37--\n","Total wall clock time: 0.1s\n","Downloaded: 1 files, 23K in 0s (108 MB/s)\n"]}]},{"cell_type":"code","source":["def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n","    # Defino listas para realizar graficas de los resultados\n","    train_loss = []\n","    valid_loss = []\n","\n","    # Defino mi loop de entrenamiento\n","\n","    for epoch in range(epochs):\n","\n","        epoch_train_loss = 0.0\n","        epoch_train_accuracy = 0.0\n","\n","        for train_data, train_target in train_loader:\n","\n","            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n","            # los va acumulando\n","            optimizer.zero_grad()\n","\n","            output = model(train_data)\n","\n","            # Computo el error de la salida comparando contra las etiquetas\n","            loss = criterion(output, train_target)\n","\n","            # Almaceno el error del batch para luego tener el error promedio de la epoca\n","            epoch_train_loss += loss.item()\n","\n","            # Computo el nuevo set de gradientes a lo largo de toda la red\n","            loss.backward()\n","\n","            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n","            optimizer.step()\n","\n","        # Calculo la media de error para la epoca de entrenamiento.\n","        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n","        epoch_train_loss = epoch_train_loss / len(train_loader)\n","        train_loss.append(epoch_train_loss)\n","\n","        # Realizo el paso de validación computando error y accuracy, y\n","        # almacenando los valores para imprimirlos y graficarlos\n","        valid_data, valid_target = iter(valid_loader).next()\n","        output = model(valid_data)\n","        \n","        epoch_valid_loss = criterion(output, valid_target).item()\n","        valid_loss.append(epoch_valid_loss)\n","\n","        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\")\n","\n","    history = {\n","        \"loss\": train_loss,\n","        \"val_loss\": valid_loss,\n","    }\n","    return history"],"metadata":{"id":"YMFLzZx7cJET","executionInfo":{"status":"ok","timestamp":1656680078784,"user_tz":180,"elapsed":20,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"10bFkG1YuaD9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656680078784,"user_tz":180,"elapsed":18,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"130608fc-0138-4d4d-fa7d-0757d94e27c6"},"source":["# Generar datos sintéticos\n","X = list()\n","y = list()\n","\n","# X es una lista de números de 1 al 43 que avanzan de 3 en 3\n","X = [x for x in range(1, 44, 3)]\n","\n","# \"y\" (target) se obtiene como por cada dato de entrada se\n","# se obtienen dos datos de salida como x+1 y x+2\n","y = [ [x+1, x+2] for x in X]\n","\n","print(\"datos X:\", X)\n","print(\"datos y:\", y)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["datos X: [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43]\n","datos y: [[2, 3], [5, 6], [8, 9], [11, 12], [14, 15], [17, 18], [20, 21], [23, 24], [26, 27], [29, 30], [32, 33], [35, 36], [38, 39], [41, 42], [44, 45]]\n"]}]},{"cell_type":"code","metadata":{"id":"Oqabd-kYvza9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656680078785,"user_tz":180,"elapsed":15,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"6382c72b-0068-4d3d-ac3a-69a5466ab6fd"},"source":["# Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1)\n","X = np.array(X).reshape(len(X), 1, 1)\n","print(\"datos X:\", X)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["datos X: [[[ 1]]\n","\n"," [[ 4]]\n","\n"," [[ 7]]\n","\n"," [[10]]\n","\n"," [[13]]\n","\n"," [[16]]\n","\n"," [[19]]\n","\n"," [[22]]\n","\n"," [[25]]\n","\n"," [[28]]\n","\n"," [[31]]\n","\n"," [[34]]\n","\n"," [[37]]\n","\n"," [[40]]\n","\n"," [[43]]]\n"]}]},{"cell_type":"code","source":["# (batch size, seq_len, input_size)\n","X.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PytzYIMsUS1T","executionInfo":{"status":"ok","timestamp":1656680078785,"user_tz":180,"elapsed":11,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"72dc2cd3-072f-4109-c84d-9ad9848a0c24"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15, 1, 1)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"gYz6XpuyxBbQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656680078786,"user_tz":180,"elapsed":9,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"0fe154a3-981e-4b58-941d-4dd07c0e7957"},"source":["y = np.asanyarray(y)\n","y.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15, 2)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["class Data(Dataset):\n","    def __init__(self, x, y):\n","        # Convertir los arrays de numpy a tensores. \n","        # pytorch espera en general entradas 32bits\n","        self.x = torch.from_numpy(x.astype(np.float32))\n","        # las loss unfction esperan la salida float\n","        self.y = torch.from_numpy(y.astype(np.float32))\n","\n","        self.len = self.y.shape[0]\n","\n","    def __getitem__(self,index):\n","        return self.x[index], self.y[index]\n","\n","    def __len__(self):\n","        return self.len\n","\n","data_set = Data(X, y)\n","\n","input_dim = data_set.x.shape[1:]\n","seq_length = input_dim[0]\n","input_size = input_dim[1]\n","print(\"Input dim\", input_dim)\n","print(\"seq_length:\", seq_length)\n","print(\"input_size:\", input_size)\n","\n","output_dim = data_set.y.shape[1]\n","print(\"Output dim\", output_dim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnrjCgx9TtOU","executionInfo":{"status":"ok","timestamp":1656680115352,"user_tz":180,"elapsed":521,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"d192bdac-5c90-4ee3-fff1-4c8ab265952c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Input dim torch.Size([1, 1])\n","seq_length: 1\n","input_size: 1\n","Output dim 2\n"]}]},{"cell_type":"code","source":["data_set.x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vggjoIXUIve","executionInfo":{"status":"ok","timestamp":1656680123339,"user_tz":180,"elapsed":468,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"1a1d2503-7c60-4905-92d5-cfc79d3c214d"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([15, 1, 1])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["data_set.y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjl1f2gXUVHU","executionInfo":{"status":"ok","timestamp":1656680126000,"user_tz":180,"elapsed":8,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"c4a54234-4cfc-473b-cadc-114c3be60221"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([15, 2])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["torch.manual_seed(42)\n","valid_set_size = int(data_set.len * 0.2)\n","train_set_size = data_set.len - valid_set_size\n","\n","# Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos\n","train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n","valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n","\n","print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n","print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n","valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4j2mXBQhitKg","executionInfo":{"status":"ok","timestamp":1656680507143,"user_tz":180,"elapsed":7,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"2f6c87be-274d-4084-bf04-9aa071b6ef95"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del conjunto de entrenamiento: 12\n","Tamaño del conjunto de validacion: 3\n"]}]},{"cell_type":"markdown","metadata":{"id":"VG3-d_NXwDGD"},"source":["### 2 - Entrenar el modelo"]},{"cell_type":"code","source":["from torch_helpers import CustomLSTM\n","\n","class Model1(nn.Module):\n","    def __init__(self, input_size, output_dim):\n","        super().__init__()\n","\n","        #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer\n","        # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor\n","        # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la\n","        # layer de Pytorch RNN no permite modificar la funcion de activacion\n","        #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer\n","        self.rnn1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer\n","        self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer\n","        \n","    def forward(self, x):\n","        lstm_output, _ = self.rnn1(x)\n","        out = self.fc(lstm_output[:,-1,:]) # take last output (last seq)\n","        return out\n","\n","model1 = Model1(input_size=input_size, output_dim=output_dim)\n","\n","# Crear el optimizador la una función de error\n","model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)\n","model1_criterion = nn.MSELoss()  # mean squared error\n","\n","summary(model1, input_size=(1, seq_length, input_size))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwEo7Sb7fY_l","executionInfo":{"status":"ok","timestamp":1656680456036,"user_tz":180,"elapsed":7,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"640669e0-8097-45c2-8f45-c92f8241920a"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Model1                                   [1, 2]                    --\n","├─CustomLSTM: 1-1                        [1, 1, 64]                16,896\n","│    └─Sigmoid: 2-1                      [1, 64]                   --\n","│    └─Sigmoid: 2-2                      [1, 64]                   --\n","│    └─ReLU: 2-3                         [1, 64]                   --\n","│    └─Sigmoid: 2-4                      [1, 64]                   --\n","│    └─ReLU: 2-5                         [1, 64]                   --\n","├─Linear: 1-2                            [1, 2]                    130\n","==========================================================================================\n","Total params: 17,026\n","Trainable params: 17,026\n","Non-trainable params: 0\n","Total mult-adds (M): 0.00\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.00\n","=========================================================================================="]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["history1 = train(model1,\n","                train_loader,\n","                valid_loader,\n","                model1_optimizer,\n","                model1_criterion,\n","                epochs=500\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJRYAPO6hlKO","executionInfo":{"status":"ok","timestamp":1656680514213,"user_tz":180,"elapsed":1595,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"a652ed9a-a78d-4f69-e6af-613a101b05bb"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/500 - Train loss 462.355 - Valid Loss 1645.288\n","Epoch: 2/500 - Train loss 449.647 - Valid Loss 1595.563\n","Epoch: 3/500 - Train loss 436.923 - Valid Loss 1542.512\n","Epoch: 4/500 - Train loss 423.058 - Valid Loss 1482.594\n","Epoch: 5/500 - Train loss 407.156 - Valid Loss 1413.558\n","Epoch: 6/500 - Train loss 388.677 - Valid Loss 1333.730\n","Epoch: 7/500 - Train loss 367.355 - Valid Loss 1241.423\n","Epoch: 8/500 - Train loss 342.932 - Valid Loss 1135.327\n","Epoch: 9/500 - Train loss 315.275 - Valid Loss 1015.040\n","Epoch: 10/500 - Train loss 284.417 - Valid Loss 881.632\n","Epoch: 11/500 - Train loss 250.619 - Valid Loss 738.088\n","Epoch: 12/500 - Train loss 214.433 - Valid Loss 589.436\n","Epoch: 13/500 - Train loss 176.758 - Valid Loss 442.449\n","Epoch: 14/500 - Train loss 138.841 - Valid Loss 305.013\n","Epoch: 15/500 - Train loss 102.255 - Valid Loss 185.223\n","Epoch: 16/500 - Train loss 68.777 - Valid Loss 90.527\n","Epoch: 17/500 - Train loss 40.382 - Valid Loss 27.567\n","Epoch: 18/500 - Train loss 19.018 - Valid Loss 1.170\n","Epoch: 19/500 - Train loss 6.365 - Valid Loss 13.141\n","Epoch: 20/500 - Train loss 3.393 - Valid Loss 58.027\n","Epoch: 21/500 - Train loss 9.377 - Valid Loss 117.426\n","Epoch: 22/500 - Train loss 20.543 - Valid Loss 168.502\n","Epoch: 23/500 - Train loss 31.511 - Valid Loss 197.384\n","Epoch: 24/500 - Train loss 38.496 - Valid Loss 201.206\n","Epoch: 25/500 - Train loss 40.247 - Valid Loss 184.472\n","Epoch: 26/500 - Train loss 37.399 - Valid Loss 154.714\n","Epoch: 27/500 - Train loss 31.484 - Valid Loss 119.418\n","Epoch: 28/500 - Train loss 24.185 - Valid Loss 84.508\n","Epoch: 29/500 - Train loss 16.922 - Valid Loss 53.998\n","Epoch: 30/500 - Train loss 10.691 - Valid Loss 30.097\n","Epoch: 31/500 - Train loss 6.051 - Valid Loss 13.554\n","Epoch: 32/500 - Train loss 3.162 - Valid Loss 4.009\n","Epoch: 33/500 - Train loss 1.906 - Valid Loss 0.295\n","Epoch: 34/500 - Train loss 1.963 - Valid Loss 0.805\n","Epoch: 35/500 - Train loss 2.916 - Valid Loss 3.837\n","Epoch: 36/500 - Train loss 4.329 - Valid Loss 7.855\n","Epoch: 37/500 - Train loss 5.818 - Valid Loss 11.643\n","Epoch: 38/500 - Train loss 7.085 - Valid Loss 14.383\n","Epoch: 39/500 - Train loss 7.936 - Valid Loss 15.644\n","Epoch: 40/500 - Train loss 8.277 - Valid Loss 15.345\n","Epoch: 41/500 - Train loss 8.103 - Valid Loss 13.679\n","Epoch: 42/500 - Train loss 7.479 - Valid Loss 11.036\n","Epoch: 43/500 - Train loss 6.520 - Valid Loss 7.915\n","Epoch: 44/500 - Train loss 5.371 - Valid Loss 4.844\n","Epoch: 45/500 - Train loss 4.182 - Valid Loss 2.304\n","Epoch: 46/500 - Train loss 3.095 - Valid Loss 0.662\n","Epoch: 47/500 - Train loss 2.222 - Valid Loss 0.124\n","Epoch: 48/500 - Train loss 1.632 - Valid Loss 0.705\n","Epoch: 49/500 - Train loss 1.348 - Valid Loss 2.239\n","Epoch: 50/500 - Train loss 1.343 - Valid Loss 4.407\n","Epoch: 51/500 - Train loss 1.551 - Valid Loss 6.803\n","Epoch: 52/500 - Train loss 1.878 - Valid Loss 9.005\n","Epoch: 53/500 - Train loss 2.225 - Valid Loss 10.657\n","Epoch: 54/500 - Train loss 2.505 - Valid Loss 11.527\n","Epoch: 55/500 - Train loss 2.657 - Valid Loss 11.533\n","Epoch: 56/500 - Train loss 2.657 - Valid Loss 10.745\n","Epoch: 57/500 - Train loss 2.514 - Valid Loss 9.345\n","Epoch: 58/500 - Train loss 2.264 - Valid Loss 7.583\n","Epoch: 59/500 - Train loss 1.961 - Valid Loss 5.719\n","Epoch: 60/500 - Train loss 1.658 - Valid Loss 3.978\n","Epoch: 61/500 - Train loss 1.401 - Valid Loss 2.518\n","Epoch: 62/500 - Train loss 1.217 - Valid Loss 1.418\n","Epoch: 63/500 - Train loss 1.118 - Valid Loss 0.687\n","Epoch: 64/500 - Train loss 1.096 - Valid Loss 0.276\n","Epoch: 65/500 - Train loss 1.132 - Valid Loss 0.102\n","Epoch: 66/500 - Train loss 1.200 - Valid Loss 0.072\n","Epoch: 67/500 - Train loss 1.274 - Valid Loss 0.104\n","Epoch: 68/500 - Train loss 1.330 - Valid Loss 0.139\n","Epoch: 69/500 - Train loss 1.355 - Valid Loss 0.144\n","Epoch: 70/500 - Train loss 1.340 - Valid Loss 0.116\n","Epoch: 71/500 - Train loss 1.289 - Valid Loss 0.072\n","Epoch: 72/500 - Train loss 1.209 - Valid Loss 0.045\n","Epoch: 73/500 - Train loss 1.112 - Valid Loss 0.070\n","Epoch: 74/500 - Train loss 1.014 - Valid Loss 0.188\n","Epoch: 75/500 - Train loss 0.932 - Valid Loss 0.443\n","Epoch: 76/500 - Train loss 0.886 - Valid Loss 0.850\n","Epoch: 77/500 - Train loss 0.889 - Valid Loss 1.198\n","Epoch: 78/500 - Train loss 0.911 - Valid Loss 1.331\n","Epoch: 79/500 - Train loss 0.911 - Valid Loss 1.302\n","Epoch: 80/500 - Train loss 0.886 - Valid Loss 1.193\n","Epoch: 81/500 - Train loss 0.849 - Valid Loss 1.055\n","Epoch: 82/500 - Train loss 0.810 - Valid Loss 0.909\n","Epoch: 83/500 - Train loss 0.775 - Valid Loss 0.765\n","Epoch: 84/500 - Train loss 0.746 - Valid Loss 0.628\n","Epoch: 85/500 - Train loss 0.721 - Valid Loss 0.506\n","Epoch: 86/500 - Train loss 0.700 - Valid Loss 0.401\n","Epoch: 87/500 - Train loss 0.681 - Valid Loss 0.317\n","Epoch: 88/500 - Train loss 0.663 - Valid Loss 0.253\n","Epoch: 89/500 - Train loss 0.644 - Valid Loss 0.208\n","Epoch: 90/500 - Train loss 0.623 - Valid Loss 0.179\n","Epoch: 91/500 - Train loss 0.598 - Valid Loss 0.163\n","Epoch: 92/500 - Train loss 0.570 - Valid Loss 0.158\n","Epoch: 93/500 - Train loss 0.537 - Valid Loss 0.160\n","Epoch: 94/500 - Train loss 0.502 - Valid Loss 0.168\n","Epoch: 95/500 - Train loss 0.464 - Valid Loss 0.180\n","Epoch: 96/500 - Train loss 0.427 - Valid Loss 0.193\n","Epoch: 97/500 - Train loss 0.393 - Valid Loss 0.203\n","Epoch: 98/500 - Train loss 0.366 - Valid Loss 0.206\n","Epoch: 99/500 - Train loss 0.346 - Valid Loss 0.194\n","Epoch: 100/500 - Train loss 0.329 - Valid Loss 0.162\n","Epoch: 101/500 - Train loss 0.309 - Valid Loss 0.115\n","Epoch: 102/500 - Train loss 0.280 - Valid Loss 0.069\n","Epoch: 103/500 - Train loss 0.248 - Valid Loss 0.036\n","Epoch: 104/500 - Train loss 0.220 - Valid Loss 0.024\n","Epoch: 105/500 - Train loss 0.200 - Valid Loss 0.029\n","Epoch: 106/500 - Train loss 0.187 - Valid Loss 0.044\n","Epoch: 107/500 - Train loss 0.176 - Valid Loss 0.060\n","Epoch: 108/500 - Train loss 0.165 - Valid Loss 0.071\n","Epoch: 109/500 - Train loss 0.151 - Valid Loss 0.076\n","Epoch: 110/500 - Train loss 0.137 - Valid Loss 0.074\n","Epoch: 111/500 - Train loss 0.126 - Valid Loss 0.069\n","Epoch: 112/500 - Train loss 0.118 - Valid Loss 0.064\n","Epoch: 113/500 - Train loss 0.114 - Valid Loss 0.061\n","Epoch: 114/500 - Train loss 0.111 - Valid Loss 0.061\n","Epoch: 115/500 - Train loss 0.105 - Valid Loss 0.063\n","Epoch: 116/500 - Train loss 0.097 - Valid Loss 0.065\n","Epoch: 117/500 - Train loss 0.089 - Valid Loss 0.065\n","Epoch: 118/500 - Train loss 0.082 - Valid Loss 0.060\n","Epoch: 119/500 - Train loss 0.076 - Valid Loss 0.053\n","Epoch: 120/500 - Train loss 0.070 - Valid Loss 0.042\n","Epoch: 121/500 - Train loss 0.065 - Valid Loss 0.030\n","Epoch: 122/500 - Train loss 0.059 - Valid Loss 0.021\n","Epoch: 123/500 - Train loss 0.053 - Valid Loss 0.015\n","Epoch: 124/500 - Train loss 0.047 - Valid Loss 0.014\n","Epoch: 125/500 - Train loss 0.043 - Valid Loss 0.018\n","Epoch: 126/500 - Train loss 0.040 - Valid Loss 0.027\n","Epoch: 127/500 - Train loss 0.037 - Valid Loss 0.037\n","Epoch: 128/500 - Train loss 0.035 - Valid Loss 0.048\n","Epoch: 129/500 - Train loss 0.034 - Valid Loss 0.058\n","Epoch: 130/500 - Train loss 0.032 - Valid Loss 0.066\n","Epoch: 131/500 - Train loss 0.031 - Valid Loss 0.070\n","Epoch: 132/500 - Train loss 0.029 - Valid Loss 0.072\n","Epoch: 133/500 - Train loss 0.027 - Valid Loss 0.071\n","Epoch: 134/500 - Train loss 0.026 - Valid Loss 0.069\n","Epoch: 135/500 - Train loss 0.025 - Valid Loss 0.066\n","Epoch: 136/500 - Train loss 0.023 - Valid Loss 0.062\n","Epoch: 137/500 - Train loss 0.022 - Valid Loss 0.059\n","Epoch: 138/500 - Train loss 0.021 - Valid Loss 0.055\n","Epoch: 139/500 - Train loss 0.020 - Valid Loss 0.052\n","Epoch: 140/500 - Train loss 0.019 - Valid Loss 0.050\n","Epoch: 141/500 - Train loss 0.019 - Valid Loss 0.048\n","Epoch: 142/500 - Train loss 0.018 - Valid Loss 0.046\n","Epoch: 143/500 - Train loss 0.017 - Valid Loss 0.045\n","Epoch: 144/500 - Train loss 0.017 - Valid Loss 0.044\n","Epoch: 145/500 - Train loss 0.016 - Valid Loss 0.043\n","Epoch: 146/500 - Train loss 0.016 - Valid Loss 0.042\n","Epoch: 147/500 - Train loss 0.016 - Valid Loss 0.041\n","Epoch: 148/500 - Train loss 0.016 - Valid Loss 0.040\n","Epoch: 149/500 - Train loss 0.016 - Valid Loss 0.040\n","Epoch: 150/500 - Train loss 0.015 - Valid Loss 0.039\n","Epoch: 151/500 - Train loss 0.015 - Valid Loss 0.039\n","Epoch: 152/500 - Train loss 0.015 - Valid Loss 0.039\n","Epoch: 153/500 - Train loss 0.015 - Valid Loss 0.039\n","Epoch: 154/500 - Train loss 0.014 - Valid Loss 0.040\n","Epoch: 155/500 - Train loss 0.014 - Valid Loss 0.041\n","Epoch: 156/500 - Train loss 0.014 - Valid Loss 0.043\n","Epoch: 157/500 - Train loss 0.014 - Valid Loss 0.045\n","Epoch: 158/500 - Train loss 0.014 - Valid Loss 0.048\n","Epoch: 159/500 - Train loss 0.014 - Valid Loss 0.050\n","Epoch: 160/500 - Train loss 0.014 - Valid Loss 0.053\n","Epoch: 161/500 - Train loss 0.014 - Valid Loss 0.055\n","Epoch: 162/500 - Train loss 0.013 - Valid Loss 0.057\n","Epoch: 163/500 - Train loss 0.013 - Valid Loss 0.059\n","Epoch: 164/500 - Train loss 0.013 - Valid Loss 0.060\n","Epoch: 165/500 - Train loss 0.013 - Valid Loss 0.061\n","Epoch: 166/500 - Train loss 0.013 - Valid Loss 0.061\n","Epoch: 167/500 - Train loss 0.013 - Valid Loss 0.060\n","Epoch: 168/500 - Train loss 0.013 - Valid Loss 0.059\n","Epoch: 169/500 - Train loss 0.013 - Valid Loss 0.058\n","Epoch: 170/500 - Train loss 0.013 - Valid Loss 0.057\n","Epoch: 171/500 - Train loss 0.013 - Valid Loss 0.055\n","Epoch: 172/500 - Train loss 0.013 - Valid Loss 0.053\n","Epoch: 173/500 - Train loss 0.013 - Valid Loss 0.052\n","Epoch: 174/500 - Train loss 0.013 - Valid Loss 0.051\n","Epoch: 175/500 - Train loss 0.013 - Valid Loss 0.049\n","Epoch: 176/500 - Train loss 0.012 - Valid Loss 0.049\n","Epoch: 177/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 178/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 179/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 180/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 181/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 182/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 183/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 184/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 185/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 186/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 187/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 188/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 189/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 190/500 - Train loss 0.012 - Valid Loss 0.048\n","Epoch: 191/500 - Train loss 0.011 - Valid Loss 0.048\n","Epoch: 192/500 - Train loss 0.011 - Valid Loss 0.048\n","Epoch: 193/500 - Train loss 0.011 - Valid Loss 0.048\n","Epoch: 194/500 - Train loss 0.011 - Valid Loss 0.048\n","Epoch: 195/500 - Train loss 0.011 - Valid Loss 0.048\n","Epoch: 196/500 - Train loss 0.011 - Valid Loss 0.048\n","Epoch: 197/500 - Train loss 0.011 - Valid Loss 0.048\n","Epoch: 198/500 - Train loss 0.011 - Valid Loss 0.047\n","Epoch: 199/500 - Train loss 0.011 - Valid Loss 0.047\n","Epoch: 200/500 - Train loss 0.011 - Valid Loss 0.047\n","Epoch: 201/500 - Train loss 0.011 - Valid Loss 0.047\n","Epoch: 202/500 - Train loss 0.011 - Valid Loss 0.046\n","Epoch: 203/500 - Train loss 0.011 - Valid Loss 0.046\n","Epoch: 204/500 - Train loss 0.011 - Valid Loss 0.045\n","Epoch: 205/500 - Train loss 0.011 - Valid Loss 0.045\n","Epoch: 206/500 - Train loss 0.011 - Valid Loss 0.044\n","Epoch: 207/500 - Train loss 0.010 - Valid Loss 0.044\n","Epoch: 208/500 - Train loss 0.010 - Valid Loss 0.044\n","Epoch: 209/500 - Train loss 0.010 - Valid Loss 0.043\n","Epoch: 210/500 - Train loss 0.010 - Valid Loss 0.043\n","Epoch: 211/500 - Train loss 0.010 - Valid Loss 0.043\n","Epoch: 212/500 - Train loss 0.010 - Valid Loss 0.043\n","Epoch: 213/500 - Train loss 0.010 - Valid Loss 0.043\n","Epoch: 214/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 215/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 216/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 217/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 218/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 219/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 220/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 221/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 222/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 223/500 - Train loss 0.010 - Valid Loss 0.042\n","Epoch: 224/500 - Train loss 0.010 - Valid Loss 0.041\n","Epoch: 225/500 - Train loss 0.010 - Valid Loss 0.041\n","Epoch: 226/500 - Train loss 0.009 - Valid Loss 0.041\n","Epoch: 227/500 - Train loss 0.009 - Valid Loss 0.041\n","Epoch: 228/500 - Train loss 0.009 - Valid Loss 0.041\n","Epoch: 229/500 - Train loss 0.009 - Valid Loss 0.040\n","Epoch: 230/500 - Train loss 0.009 - Valid Loss 0.040\n","Epoch: 231/500 - Train loss 0.009 - Valid Loss 0.040\n","Epoch: 232/500 - Train loss 0.009 - Valid Loss 0.040\n","Epoch: 233/500 - Train loss 0.009 - Valid Loss 0.040\n","Epoch: 234/500 - Train loss 0.009 - Valid Loss 0.039\n","Epoch: 235/500 - Train loss 0.009 - Valid Loss 0.039\n","Epoch: 236/500 - Train loss 0.009 - Valid Loss 0.039\n","Epoch: 237/500 - Train loss 0.009 - Valid Loss 0.039\n","Epoch: 238/500 - Train loss 0.009 - Valid Loss 0.039\n","Epoch: 239/500 - Train loss 0.009 - Valid Loss 0.039\n","Epoch: 240/500 - Train loss 0.009 - Valid Loss 0.038\n","Epoch: 241/500 - Train loss 0.009 - Valid Loss 0.038\n","Epoch: 242/500 - Train loss 0.009 - Valid Loss 0.038\n","Epoch: 243/500 - Train loss 0.009 - Valid Loss 0.038\n","Epoch: 244/500 - Train loss 0.009 - Valid Loss 0.038\n","Epoch: 245/500 - Train loss 0.009 - Valid Loss 0.038\n","Epoch: 246/500 - Train loss 0.009 - Valid Loss 0.038\n","Epoch: 247/500 - Train loss 0.008 - Valid Loss 0.037\n","Epoch: 248/500 - Train loss 0.008 - Valid Loss 0.037\n","Epoch: 249/500 - Train loss 0.008 - Valid Loss 0.037\n","Epoch: 250/500 - Train loss 0.008 - Valid Loss 0.037\n","Epoch: 251/500 - Train loss 0.008 - Valid Loss 0.037\n","Epoch: 252/500 - Train loss 0.008 - Valid Loss 0.037\n","Epoch: 253/500 - Train loss 0.008 - Valid Loss 0.037\n","Epoch: 254/500 - Train loss 0.008 - Valid Loss 0.036\n","Epoch: 255/500 - Train loss 0.008 - Valid Loss 0.036\n","Epoch: 256/500 - Train loss 0.008 - Valid Loss 0.036\n","Epoch: 257/500 - Train loss 0.008 - Valid Loss 0.036\n","Epoch: 258/500 - Train loss 0.008 - Valid Loss 0.036\n","Epoch: 259/500 - Train loss 0.008 - Valid Loss 0.036\n","Epoch: 260/500 - Train loss 0.008 - Valid Loss 0.036\n","Epoch: 261/500 - Train loss 0.008 - Valid Loss 0.035\n","Epoch: 262/500 - Train loss 0.008 - Valid Loss 0.035\n","Epoch: 263/500 - Train loss 0.008 - Valid Loss 0.035\n","Epoch: 264/500 - Train loss 0.008 - Valid Loss 0.035\n","Epoch: 265/500 - Train loss 0.008 - Valid Loss 0.035\n","Epoch: 266/500 - Train loss 0.008 - Valid Loss 0.035\n","Epoch: 267/500 - Train loss 0.008 - Valid Loss 0.035\n","Epoch: 268/500 - Train loss 0.008 - Valid Loss 0.034\n","Epoch: 269/500 - Train loss 0.008 - Valid Loss 0.034\n","Epoch: 270/500 - Train loss 0.008 - Valid Loss 0.034\n","Epoch: 271/500 - Train loss 0.008 - Valid Loss 0.034\n","Epoch: 272/500 - Train loss 0.008 - Valid Loss 0.034\n","Epoch: 273/500 - Train loss 0.007 - Valid Loss 0.034\n","Epoch: 274/500 - Train loss 0.007 - Valid Loss 0.034\n","Epoch: 275/500 - Train loss 0.007 - Valid Loss 0.033\n","Epoch: 276/500 - Train loss 0.007 - Valid Loss 0.033\n","Epoch: 277/500 - Train loss 0.007 - Valid Loss 0.033\n","Epoch: 278/500 - Train loss 0.007 - Valid Loss 0.033\n","Epoch: 279/500 - Train loss 0.007 - Valid Loss 0.033\n","Epoch: 280/500 - Train loss 0.007 - Valid Loss 0.033\n","Epoch: 281/500 - Train loss 0.007 - Valid Loss 0.033\n","Epoch: 282/500 - Train loss 0.007 - Valid Loss 0.033\n","Epoch: 283/500 - Train loss 0.007 - Valid Loss 0.032\n","Epoch: 284/500 - Train loss 0.007 - Valid Loss 0.032\n","Epoch: 285/500 - Train loss 0.007 - Valid Loss 0.032\n","Epoch: 286/500 - Train loss 0.007 - Valid Loss 0.032\n","Epoch: 287/500 - Train loss 0.007 - Valid Loss 0.032\n","Epoch: 288/500 - Train loss 0.007 - Valid Loss 0.032\n","Epoch: 289/500 - Train loss 0.007 - Valid Loss 0.032\n","Epoch: 290/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 291/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 292/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 293/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 294/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 295/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 296/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 297/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 298/500 - Train loss 0.007 - Valid Loss 0.031\n","Epoch: 299/500 - Train loss 0.007 - Valid Loss 0.030\n","Epoch: 300/500 - Train loss 0.007 - Valid Loss 0.030\n","Epoch: 301/500 - Train loss 0.007 - Valid Loss 0.030\n","Epoch: 302/500 - Train loss 0.007 - Valid Loss 0.030\n","Epoch: 303/500 - Train loss 0.007 - Valid Loss 0.030\n","Epoch: 304/500 - Train loss 0.006 - Valid Loss 0.030\n","Epoch: 305/500 - Train loss 0.006 - Valid Loss 0.030\n","Epoch: 306/500 - Train loss 0.006 - Valid Loss 0.030\n","Epoch: 307/500 - Train loss 0.006 - Valid Loss 0.029\n","Epoch: 308/500 - Train loss 0.006 - Valid Loss 0.029\n","Epoch: 309/500 - Train loss 0.006 - Valid Loss 0.029\n","Epoch: 310/500 - Train loss 0.006 - Valid Loss 0.029\n","Epoch: 311/500 - Train loss 0.006 - Valid Loss 0.029\n","Epoch: 312/500 - Train loss 0.006 - Valid Loss 0.029\n","Epoch: 313/500 - Train loss 0.006 - Valid Loss 0.029\n","Epoch: 314/500 - Train loss 0.006 - Valid Loss 0.029\n","Epoch: 315/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 316/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 317/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 318/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 319/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 320/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 321/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 322/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 323/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 324/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 325/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 326/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 327/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 328/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 329/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 330/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 331/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 332/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 333/500 - Train loss 0.006 - Valid Loss 0.027\n","Epoch: 334/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 335/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 336/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 337/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 338/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 339/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 340/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 341/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 342/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 343/500 - Train loss 0.006 - Valid Loss 0.025\n","Epoch: 344/500 - Train loss 0.006 - Valid Loss 0.025\n","Epoch: 345/500 - Train loss 0.006 - Valid Loss 0.025\n","Epoch: 346/500 - Train loss 0.005 - Valid Loss 0.025\n","Epoch: 347/500 - Train loss 0.005 - Valid Loss 0.025\n","Epoch: 348/500 - Train loss 0.005 - Valid Loss 0.025\n","Epoch: 349/500 - Train loss 0.005 - Valid Loss 0.025\n","Epoch: 350/500 - Train loss 0.005 - Valid Loss 0.025\n","Epoch: 351/500 - Train loss 0.005 - Valid Loss 0.025\n","Epoch: 352/500 - Train loss 0.005 - Valid Loss 0.025\n","Epoch: 353/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 354/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 355/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 356/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 357/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 358/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 359/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 360/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 361/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 362/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 363/500 - Train loss 0.005 - Valid Loss 0.024\n","Epoch: 364/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 365/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 366/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 367/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 368/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 369/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 370/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 371/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 372/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 373/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 374/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 375/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 376/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 377/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 378/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 379/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 380/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 381/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 382/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 383/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 384/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 385/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 386/500 - Train loss 0.005 - Valid Loss 0.022\n","Epoch: 387/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 388/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 389/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 390/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 391/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 392/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 393/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 394/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 395/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 396/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 397/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 398/500 - Train loss 0.005 - Valid Loss 0.021\n","Epoch: 399/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 400/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 401/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 402/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 403/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 404/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 405/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 406/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 407/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 408/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 409/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 410/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 411/500 - Train loss 0.005 - Valid Loss 0.020\n","Epoch: 412/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 413/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 414/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 415/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 416/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 417/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 418/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 419/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 420/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 421/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 422/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 423/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 424/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 425/500 - Train loss 0.004 - Valid Loss 0.019\n","Epoch: 426/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 427/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 428/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 429/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 430/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 431/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 432/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 433/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 434/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 435/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 436/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 437/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 438/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 439/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 440/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 441/500 - Train loss 0.004 - Valid Loss 0.018\n","Epoch: 442/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 443/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 444/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 445/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 446/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 447/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 448/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 449/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 450/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 451/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 452/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 453/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 454/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 455/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 456/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 457/500 - Train loss 0.004 - Valid Loss 0.017\n","Epoch: 458/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 459/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 460/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 461/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 462/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 463/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 464/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 465/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 466/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 467/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 468/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 469/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 470/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 471/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 472/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 473/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 474/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 475/500 - Train loss 0.004 - Valid Loss 0.016\n","Epoch: 476/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 477/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 478/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 479/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 480/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 481/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 482/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 483/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 484/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 485/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 486/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 487/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 488/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 489/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 490/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 491/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 492/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 493/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 494/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 495/500 - Train loss 0.004 - Valid Loss 0.015\n","Epoch: 496/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 497/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 498/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 499/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 500/500 - Train loss 0.004 - Valid Loss 0.014\n"]}]},{"cell_type":"code","source":["epoch_count = range(1, len(history1['loss']) + 1)\n","sns.lineplot(x=epoch_count,  y=history1['loss'], label='train')\n","sns.lineplot(x=epoch_count,  y=history1['val_loss'], label='valid')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"DY39Ruoahrsk","executionInfo":{"status":"ok","timestamp":1656680519251,"user_tz":180,"elapsed":393,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"454aaf18-fc6c-46fb-b960-a6357a5325bf"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"88tdVCOyxcuy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656680564721,"user_tz":180,"elapsed":291,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"5475340d-3496-4955-a466-6d49170e2aea"},"source":["# Ensayo\n","x_test = 10\n","y_test = [x_test + 1, x_test + 2]\n","test_input = np.array([x_test])\n","test_input = test_input.reshape((1, seq_length, input_size))\n","test_input = torch.from_numpy(test_input.astype(np.float32))\n","\n","test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1)\n","\n","y_hat = model1(test_input)\n","\n","print(\"y_test:\", y_test)\n","print(\"y_hat:\", y_hat)\n","\n","loss = model1_criterion(y_hat, test_target).item()\n","print(\"loss:\", loss)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["y_test: [11, 12]\n","y_hat: tensor([[11.0004, 12.0687]], grad_fn=<AddmmBackward0>)\n","loss: 0.5364619493484497\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]}]},{"cell_type":"markdown","metadata":{"id":"AT8b9EfGyshD"},"source":["### 3 - Multi-layer LSTM"]},{"cell_type":"code","source":["from torch_helpers import CustomLSTM\n","\n","# En esta oportunidad se utilizarán dos layer LSTM\n","class Model2(nn.Module):\n","    def __init__(self, input_size, output_dim):\n","        super().__init__()\n","\n","        self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer\n","        self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer\n","        self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer\n","        \n","    def forward(self, x):\n","        lstm_output, _ = self.lstm1(x)\n","        lstm_output, _ = self.lstm2(lstm_output)\n","        out = self.fc(lstm_output[:,-1,:]) # take last output (last seq)\n","        return out\n","\n","model2 = Model2(input_size=input_size, output_dim=output_dim)\n","\n","# Crear el optimizador la una función de error\n","model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n","model2_criterion = nn.MSELoss()  # mean squared error\n","\n","summary(model2, input_size=(1, seq_length, input_size))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xVfTMScdFqR","executionInfo":{"status":"ok","timestamp":1656681111689,"user_tz":180,"elapsed":7,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"b1126586-0c0e-42c4-d77b-119c677fbe81"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Model2                                   [1, 2]                    --\n","├─CustomLSTM: 1-1                        [1, 1, 64]                16,896\n","├─CustomLSTM: 1                          --                        --\n","│    └─Sigmoid: 2-1                      [1, 64]                   --\n","│    └─Sigmoid: 2-2                      [1, 64]                   --\n","├─CustomLSTM: 1                          --                        --\n","│    └─ReLU: 2-3                         [1, 64]                   --\n","├─CustomLSTM: 1                          --                        --\n","│    └─Sigmoid: 2-4                      [1, 64]                   --\n","├─CustomLSTM: 1                          --                        --\n","│    └─ReLU: 2-5                         [1, 64]                   --\n","├─CustomLSTM: 1-2                        [1, 1, 64]                33,024\n","│    └─Sigmoid: 2-6                      [1, 64]                   --\n","│    └─Sigmoid: 2-7                      [1, 64]                   --\n","│    └─ReLU: 2-8                         [1, 64]                   --\n","│    └─Sigmoid: 2-9                      [1, 64]                   --\n","│    └─ReLU: 2-10                        [1, 64]                   --\n","├─Linear: 1-3                            [1, 2]                    130\n","==========================================================================================\n","Total params: 50,050\n","Trainable params: 50,050\n","Non-trainable params: 0\n","Total mult-adds (M): 0.00\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.00\n","=========================================================================================="]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["history2 = train(model2,\n","                train_loader,\n","                valid_loader,\n","                model2_optimizer,\n","                model2_criterion,\n","                epochs=500\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtoKCzXrei60","executionInfo":{"status":"ok","timestamp":1656681133235,"user_tz":180,"elapsed":2666,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"d0ed5b25-7774-4f0e-8097-4f333dabf6a9"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/500 - Train loss 470.314 - Valid Loss 1724.781\n","Epoch: 2/500 - Train loss 467.929 - Valid Loss 1713.987\n","Epoch: 3/500 - Train loss 465.140 - Valid Loss 1694.561\n","Epoch: 4/500 - Train loss 460.645 - Valid Loss 1656.527\n","Epoch: 5/500 - Train loss 452.558 - Valid Loss 1585.815\n","Epoch: 6/500 - Train loss 438.152 - Valid Loss 1467.096\n","Epoch: 7/500 - Train loss 413.713 - Valid Loss 1289.118\n","Epoch: 8/500 - Train loss 374.876 - Valid Loss 1050.281\n","Epoch: 9/500 - Train loss 318.913 - Valid Loss 767.279\n","Epoch: 10/500 - Train loss 246.870 - Valid Loss 463.499\n","Epoch: 11/500 - Train loss 163.997 - Valid Loss 181.160\n","Epoch: 12/500 - Train loss 81.845 - Valid Loss 10.403\n","Epoch: 13/500 - Train loss 21.010 - Valid Loss 106.299\n","Epoch: 14/500 - Train loss 16.392 - Valid Loss 452.939\n","Epoch: 15/500 - Train loss 74.102 - Valid Loss 588.090\n","Epoch: 16/500 - Train loss 102.103 - Valid Loss 462.242\n","Epoch: 17/500 - Train loss 80.693 - Valid Loss 255.158\n","Epoch: 18/500 - Train loss 43.432 - Valid Loss 93.115\n","Epoch: 19/500 - Train loss 15.513 - Valid Loss 13.986\n","Epoch: 20/500 - Train loss 4.879 - Valid Loss 1.410\n","Epoch: 21/500 - Train loss 7.822 - Valid Loss 23.500\n","Epoch: 22/500 - Train loss 17.069 - Valid Loss 53.273\n","Epoch: 23/500 - Train loss 26.545 - Valid Loss 74.527\n","Epoch: 24/500 - Train loss 32.717 - Valid Loss 81.027\n","Epoch: 25/500 - Train loss 34.276 - Valid Loss 72.708\n","Epoch: 26/500 - Train loss 31.395 - Valid Loss 53.527\n","Epoch: 27/500 - Train loss 25.159 - Valid Loss 29.956\n","Epoch: 28/500 - Train loss 17.251 - Valid Loss 9.758\n","Epoch: 29/500 - Train loss 9.695 - Valid Loss 0.391\n","Epoch: 30/500 - Train loss 4.484 - Valid Loss 6.926\n","Epoch: 31/500 - Train loss 3.053 - Valid Loss 27.043\n","Epoch: 32/500 - Train loss 5.298 - Valid Loss 51.100\n","Epoch: 33/500 - Train loss 9.285 - Valid Loss 68.455\n","Epoch: 34/500 - Train loss 12.536 - Valid Loss 71.998\n","Epoch: 35/500 - Train loss 13.264 - Valid Loss 61.038\n","Epoch: 36/500 - Train loss 11.222 - Valid Loss 41.410\n","Epoch: 37/500 - Train loss 7.629 - Valid Loss 21.474\n","Epoch: 38/500 - Train loss 4.245 - Valid Loss 7.405\n","Epoch: 39/500 - Train loss 2.319 - Valid Loss 1.178\n","Epoch: 40/500 - Train loss 2.137 - Valid Loss 1.225\n","Epoch: 41/500 - Train loss 3.194 - Valid Loss 4.327\n","Epoch: 42/500 - Train loss 4.640 - Valid Loss 7.383\n","Epoch: 43/500 - Train loss 5.701 - Valid Loss 8.474\n","Epoch: 44/500 - Train loss 5.928 - Valid Loss 7.168\n","Epoch: 45/500 - Train loss 5.256 - Valid Loss 4.306\n","Epoch: 46/500 - Train loss 3.959 - Valid Loss 1.462\n","Epoch: 47/500 - Train loss 2.514 - Valid Loss 0.248\n","Epoch: 48/500 - Train loss 1.447 - Valid Loss 1.690\n","Epoch: 49/500 - Train loss 1.165 - Valid Loss 5.718\n","Epoch: 50/500 - Train loss 1.776 - Valid Loss 9.549\n","Epoch: 51/500 - Train loss 2.558 - Valid Loss 10.579\n","Epoch: 52/500 - Train loss 2.746 - Valid Loss 9.019\n","Epoch: 53/500 - Train loss 2.345 - Valid Loss 6.144\n","Epoch: 54/500 - Train loss 1.707 - Valid Loss 3.227\n","Epoch: 55/500 - Train loss 1.165 - Valid Loss 1.128\n","Epoch: 56/500 - Train loss 0.896 - Valid Loss 0.141\n","Epoch: 57/500 - Train loss 0.908 - Valid Loss 0.041\n","Epoch: 58/500 - Train loss 1.091 - Valid Loss 0.341\n","Epoch: 59/500 - Train loss 1.290 - Valid Loss 0.583\n","Epoch: 60/500 - Train loss 1.382 - Valid Loss 0.547\n","Epoch: 61/500 - Train loss 1.315 - Valid Loss 0.293\n","Epoch: 62/500 - Train loss 1.121 - Valid Loss 0.067\n","Epoch: 63/500 - Train loss 0.887 - Valid Loss 0.129\n","Epoch: 64/500 - Train loss 0.706 - Valid Loss 0.590\n","Epoch: 65/500 - Train loss 0.632 - Valid Loss 1.334\n","Epoch: 66/500 - Train loss 0.660 - Valid Loss 2.074\n","Epoch: 67/500 - Train loss 0.733 - Valid Loss 2.502\n","Epoch: 68/500 - Train loss 0.780 - Valid Loss 2.450\n","Epoch: 69/500 - Train loss 0.754 - Valid Loss 1.969\n","Epoch: 70/500 - Train loss 0.657 - Valid Loss 1.282\n","Epoch: 71/500 - Train loss 0.531 - Valid Loss 0.650\n","Epoch: 72/500 - Train loss 0.425 - Valid Loss 0.248\n","Epoch: 73/500 - Train loss 0.370 - Valid Loss 0.106\n","Epoch: 74/500 - Train loss 0.362 - Valid Loss 0.137\n","Epoch: 75/500 - Train loss 0.376 - Valid Loss 0.213\n","Epoch: 76/500 - Train loss 0.379 - Valid Loss 0.239\n","Epoch: 77/500 - Train loss 0.351 - Valid Loss 0.191\n","Epoch: 78/500 - Train loss 0.295 - Valid Loss 0.106\n","Epoch: 79/500 - Train loss 0.229 - Valid Loss 0.044\n","Epoch: 80/500 - Train loss 0.179 - Valid Loss 0.039\n","Epoch: 81/500 - Train loss 0.157 - Valid Loss 0.077\n","Epoch: 82/500 - Train loss 0.162 - Valid Loss 0.116\n","Epoch: 83/500 - Train loss 0.176 - Valid Loss 0.117\n","Epoch: 84/500 - Train loss 0.181 - Valid Loss 0.078\n","Epoch: 85/500 - Train loss 0.167 - Valid Loss 0.036\n","Epoch: 86/500 - Train loss 0.139 - Valid Loss 0.042\n","Epoch: 87/500 - Train loss 0.112 - Valid Loss 0.124\n","Epoch: 88/500 - Train loss 0.100 - Valid Loss 0.264\n","Epoch: 89/500 - Train loss 0.104 - Valid Loss 0.409\n","Epoch: 90/500 - Train loss 0.116 - Valid Loss 0.503\n","Epoch: 91/500 - Train loss 0.125 - Valid Loss 0.513\n","Epoch: 92/500 - Train loss 0.122 - Valid Loss 0.445\n","Epoch: 93/500 - Train loss 0.110 - Valid Loss 0.335\n","Epoch: 94/500 - Train loss 0.096 - Valid Loss 0.223\n","Epoch: 95/500 - Train loss 0.088 - Valid Loss 0.139\n","Epoch: 96/500 - Train loss 0.086 - Valid Loss 0.089\n","Epoch: 97/500 - Train loss 0.089 - Valid Loss 0.066\n","Epoch: 98/500 - Train loss 0.089 - Valid Loss 0.058\n","Epoch: 99/500 - Train loss 0.084 - Valid Loss 0.058\n","Epoch: 100/500 - Train loss 0.075 - Valid Loss 0.067\n","Epoch: 101/500 - Train loss 0.065 - Valid Loss 0.084\n","Epoch: 102/500 - Train loss 0.059 - Valid Loss 0.103\n","Epoch: 103/500 - Train loss 0.056 - Valid Loss 0.118\n","Epoch: 104/500 - Train loss 0.055 - Valid Loss 0.120\n","Epoch: 105/500 - Train loss 0.054 - Valid Loss 0.107\n","Epoch: 106/500 - Train loss 0.052 - Valid Loss 0.084\n","Epoch: 107/500 - Train loss 0.048 - Valid Loss 0.060\n","Epoch: 108/500 - Train loss 0.044 - Valid Loss 0.043\n","Epoch: 109/500 - Train loss 0.042 - Valid Loss 0.035\n","Epoch: 110/500 - Train loss 0.041 - Valid Loss 0.035\n","Epoch: 111/500 - Train loss 0.041 - Valid Loss 0.038\n","Epoch: 112/500 - Train loss 0.040 - Valid Loss 0.040\n","Epoch: 113/500 - Train loss 0.039 - Valid Loss 0.040\n","Epoch: 114/500 - Train loss 0.037 - Valid Loss 0.040\n","Epoch: 115/500 - Train loss 0.035 - Valid Loss 0.042\n","Epoch: 116/500 - Train loss 0.033 - Valid Loss 0.045\n","Epoch: 117/500 - Train loss 0.032 - Valid Loss 0.049\n","Epoch: 118/500 - Train loss 0.031 - Valid Loss 0.052\n","Epoch: 119/500 - Train loss 0.031 - Valid Loss 0.051\n","Epoch: 120/500 - Train loss 0.029 - Valid Loss 0.047\n","Epoch: 121/500 - Train loss 0.028 - Valid Loss 0.042\n","Epoch: 122/500 - Train loss 0.026 - Valid Loss 0.038\n","Epoch: 123/500 - Train loss 0.025 - Valid Loss 0.034\n","Epoch: 124/500 - Train loss 0.024 - Valid Loss 0.033\n","Epoch: 125/500 - Train loss 0.023 - Valid Loss 0.031\n","Epoch: 126/500 - Train loss 0.023 - Valid Loss 0.031\n","Epoch: 127/500 - Train loss 0.022 - Valid Loss 0.030\n","Epoch: 128/500 - Train loss 0.021 - Valid Loss 0.030\n","Epoch: 129/500 - Train loss 0.020 - Valid Loss 0.032\n","Epoch: 130/500 - Train loss 0.019 - Valid Loss 0.034\n","Epoch: 131/500 - Train loss 0.019 - Valid Loss 0.037\n","Epoch: 132/500 - Train loss 0.018 - Valid Loss 0.038\n","Epoch: 133/500 - Train loss 0.018 - Valid Loss 0.037\n","Epoch: 134/500 - Train loss 0.017 - Valid Loss 0.036\n","Epoch: 135/500 - Train loss 0.017 - Valid Loss 0.034\n","Epoch: 136/500 - Train loss 0.016 - Valid Loss 0.032\n","Epoch: 137/500 - Train loss 0.016 - Valid Loss 0.031\n","Epoch: 138/500 - Train loss 0.015 - Valid Loss 0.030\n","Epoch: 139/500 - Train loss 0.015 - Valid Loss 0.030\n","Epoch: 140/500 - Train loss 0.015 - Valid Loss 0.029\n","Epoch: 141/500 - Train loss 0.014 - Valid Loss 0.029\n","Epoch: 142/500 - Train loss 0.014 - Valid Loss 0.029\n","Epoch: 143/500 - Train loss 0.013 - Valid Loss 0.029\n","Epoch: 144/500 - Train loss 0.013 - Valid Loss 0.029\n","Epoch: 145/500 - Train loss 0.013 - Valid Loss 0.029\n","Epoch: 146/500 - Train loss 0.013 - Valid Loss 0.028\n","Epoch: 147/500 - Train loss 0.012 - Valid Loss 0.027\n","Epoch: 148/500 - Train loss 0.012 - Valid Loss 0.026\n","Epoch: 149/500 - Train loss 0.012 - Valid Loss 0.026\n","Epoch: 150/500 - Train loss 0.011 - Valid Loss 0.025\n","Epoch: 151/500 - Train loss 0.011 - Valid Loss 0.025\n","Epoch: 152/500 - Train loss 0.011 - Valid Loss 0.025\n","Epoch: 153/500 - Train loss 0.011 - Valid Loss 0.024\n","Epoch: 154/500 - Train loss 0.010 - Valid Loss 0.024\n","Epoch: 155/500 - Train loss 0.010 - Valid Loss 0.024\n","Epoch: 156/500 - Train loss 0.010 - Valid Loss 0.024\n","Epoch: 157/500 - Train loss 0.010 - Valid Loss 0.024\n","Epoch: 158/500 - Train loss 0.010 - Valid Loss 0.024\n","Epoch: 159/500 - Train loss 0.009 - Valid Loss 0.024\n","Epoch: 160/500 - Train loss 0.009 - Valid Loss 0.024\n","Epoch: 161/500 - Train loss 0.009 - Valid Loss 0.024\n","Epoch: 162/500 - Train loss 0.009 - Valid Loss 0.023\n","Epoch: 163/500 - Train loss 0.009 - Valid Loss 0.023\n","Epoch: 164/500 - Train loss 0.009 - Valid Loss 0.023\n","Epoch: 165/500 - Train loss 0.008 - Valid Loss 0.022\n","Epoch: 166/500 - Train loss 0.008 - Valid Loss 0.022\n","Epoch: 167/500 - Train loss 0.008 - Valid Loss 0.022\n","Epoch: 168/500 - Train loss 0.008 - Valid Loss 0.021\n","Epoch: 169/500 - Train loss 0.008 - Valid Loss 0.021\n","Epoch: 170/500 - Train loss 0.008 - Valid Loss 0.021\n","Epoch: 171/500 - Train loss 0.008 - Valid Loss 0.021\n","Epoch: 172/500 - Train loss 0.007 - Valid Loss 0.021\n","Epoch: 173/500 - Train loss 0.007 - Valid Loss 0.020\n","Epoch: 174/500 - Train loss 0.007 - Valid Loss 0.020\n","Epoch: 175/500 - Train loss 0.007 - Valid Loss 0.020\n","Epoch: 176/500 - Train loss 0.007 - Valid Loss 0.020\n","Epoch: 177/500 - Train loss 0.007 - Valid Loss 0.020\n","Epoch: 178/500 - Train loss 0.007 - Valid Loss 0.019\n","Epoch: 179/500 - Train loss 0.007 - Valid Loss 0.019\n","Epoch: 180/500 - Train loss 0.007 - Valid Loss 0.019\n","Epoch: 181/500 - Train loss 0.006 - Valid Loss 0.019\n","Epoch: 182/500 - Train loss 0.006 - Valid Loss 0.019\n","Epoch: 183/500 - Train loss 0.006 - Valid Loss 0.019\n","Epoch: 184/500 - Train loss 0.006 - Valid Loss 0.018\n","Epoch: 185/500 - Train loss 0.006 - Valid Loss 0.018\n","Epoch: 186/500 - Train loss 0.006 - Valid Loss 0.018\n","Epoch: 187/500 - Train loss 0.006 - Valid Loss 0.018\n","Epoch: 188/500 - Train loss 0.006 - Valid Loss 0.018\n","Epoch: 189/500 - Train loss 0.006 - Valid Loss 0.017\n","Epoch: 190/500 - Train loss 0.006 - Valid Loss 0.017\n","Epoch: 191/500 - Train loss 0.006 - Valid Loss 0.017\n","Epoch: 192/500 - Train loss 0.006 - Valid Loss 0.017\n","Epoch: 193/500 - Train loss 0.006 - Valid Loss 0.017\n","Epoch: 194/500 - Train loss 0.005 - Valid Loss 0.017\n","Epoch: 195/500 - Train loss 0.005 - Valid Loss 0.017\n","Epoch: 196/500 - Train loss 0.005 - Valid Loss 0.016\n","Epoch: 197/500 - Train loss 0.005 - Valid Loss 0.016\n","Epoch: 198/500 - Train loss 0.005 - Valid Loss 0.016\n","Epoch: 199/500 - Train loss 0.005 - Valid Loss 0.016\n","Epoch: 200/500 - Train loss 0.005 - Valid Loss 0.016\n","Epoch: 201/500 - Train loss 0.005 - Valid Loss 0.016\n","Epoch: 202/500 - Train loss 0.005 - Valid Loss 0.016\n","Epoch: 203/500 - Train loss 0.005 - Valid Loss 0.016\n","Epoch: 204/500 - Train loss 0.005 - Valid Loss 0.015\n","Epoch: 205/500 - Train loss 0.005 - Valid Loss 0.015\n","Epoch: 206/500 - Train loss 0.005 - Valid Loss 0.015\n","Epoch: 207/500 - Train loss 0.005 - Valid Loss 0.015\n","Epoch: 208/500 - Train loss 0.005 - Valid Loss 0.015\n","Epoch: 209/500 - Train loss 0.005 - Valid Loss 0.015\n","Epoch: 210/500 - Train loss 0.005 - Valid Loss 0.015\n","Epoch: 211/500 - Train loss 0.005 - Valid Loss 0.014\n","Epoch: 212/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 213/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 214/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 215/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 216/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 217/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 218/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 219/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 220/500 - Train loss 0.004 - Valid Loss 0.014\n","Epoch: 221/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 222/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 223/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 224/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 225/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 226/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 227/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 228/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 229/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 230/500 - Train loss 0.004 - Valid Loss 0.013\n","Epoch: 231/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 232/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 233/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 234/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 235/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 236/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 237/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 238/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 239/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 240/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 241/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 242/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 243/500 - Train loss 0.004 - Valid Loss 0.012\n","Epoch: 244/500 - Train loss 0.003 - Valid Loss 0.012\n","Epoch: 245/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 246/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 247/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 248/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 249/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 250/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 251/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 252/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 253/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 254/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 255/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 256/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 257/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 258/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 259/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 260/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 261/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 262/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 263/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 264/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 265/500 - Train loss 0.003 - Valid Loss 0.011\n","Epoch: 266/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 267/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 268/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 269/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 270/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 271/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 272/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 273/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 274/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 275/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 276/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 277/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 278/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 279/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 280/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 281/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 282/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 283/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 284/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 285/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 286/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 287/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 288/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 289/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 290/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 291/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 292/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 293/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 294/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 295/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 296/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 297/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 298/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 299/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 300/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 301/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 302/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 303/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 304/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 305/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 306/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 307/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 308/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 309/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 310/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 311/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 312/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 313/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 314/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 315/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 316/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 317/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 318/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 319/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 320/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 321/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 322/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 323/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 324/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 325/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 326/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 327/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 328/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 329/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 330/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 331/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 332/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 333/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 334/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 335/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 336/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 337/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 338/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 339/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 340/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 341/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 342/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 343/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 344/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 345/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 346/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 347/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 348/500 - Train loss 0.003 - Valid Loss 0.010\n","Epoch: 349/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 350/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 351/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 352/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 353/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 354/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 355/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 356/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 357/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 358/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 359/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 360/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 361/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 362/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 363/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 364/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 365/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 366/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 367/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 368/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 369/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 370/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 371/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 372/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 373/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 374/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 375/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 376/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 377/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 378/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 379/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 380/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 381/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 382/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 383/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 384/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 385/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 386/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 387/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 388/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 389/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 390/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 391/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 392/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 393/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 394/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 395/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 396/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 397/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 398/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 399/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 400/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 401/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 402/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 403/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 404/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 405/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 406/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 407/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 408/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 409/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 410/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 411/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 412/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 413/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 414/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 415/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 416/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 417/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 418/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 419/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 420/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 421/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 422/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 423/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 424/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 425/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 426/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 427/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 428/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 429/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 430/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 431/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 432/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 433/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 434/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 435/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 436/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 437/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 438/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 439/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 440/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 441/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 442/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 443/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 444/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 445/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 446/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 447/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 448/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 449/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 450/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 451/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 452/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 453/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 454/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 455/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 456/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 457/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 458/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 459/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 460/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 461/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 462/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 463/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 464/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 465/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 466/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 467/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 468/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 469/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 470/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 471/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 472/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 473/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 474/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 475/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 476/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 477/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 478/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 479/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 480/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 481/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 482/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 483/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 484/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 485/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 486/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 487/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 488/500 - Train loss 0.002 - Valid Loss 0.011\n","Epoch: 489/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 490/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 491/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 492/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 493/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 494/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 495/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 496/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 497/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 498/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 499/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 500/500 - Train loss 0.002 - Valid Loss 0.012\n"]}]},{"cell_type":"code","source":["epoch_count = range(1, len(history2['loss']) + 1)\n","sns.lineplot(x=epoch_count,  y=history2['loss'], label='train')\n","sns.lineplot(x=epoch_count,  y=history2['val_loss'], label='valid')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"0KOSVDSBeo1A","executionInfo":{"status":"ok","timestamp":1656681148996,"user_tz":180,"elapsed":1422,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"88a89fd2-fde5-4860-993a-0419c4dfa2c1"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"FveVOv2xzfkC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656681164007,"user_tz":180,"elapsed":400,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"fa6e8b2f-719d-436b-e19e-57c2c5a93850"},"source":["# Ensayo\n","x_test = 10\n","y_test = [x_test + 1, x_test + 2]\n","test_input = np.array([x_test])\n","test_input = test_input.reshape((1, seq_length, input_size))\n","test_input = torch.from_numpy(test_input.astype(np.float32))\n","\n","test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1)\n","\n","y_hat = model2(test_input)\n","\n","print(\"y_test:\", y_test)\n","print(\"y_hat:\", y_hat)\n","\n","loss = model2_criterion(y_hat, test_target).item()\n","print(\"loss:\", loss)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["y_test: [11, 12]\n","y_hat: tensor([[11.0020, 12.0384]], grad_fn=<AddmmBackward0>)\n","loss: 0.5189282298088074\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]}]},{"cell_type":"markdown","metadata":{"id":"zd1g5MZfz5qB"},"source":["### 4 - Conclusión\n","La unica diferencia que se debe tener en cuenta cuando hay más de una salida es que la cantidad de neuronas de la última capa debe coincidir con el tamaño de la secuencia de salida.\n","En este ejemplo, donde el problema es más complejo, hubo una diferencia apreciable entre utilizar una sola capa o varias LSTM."]}]}