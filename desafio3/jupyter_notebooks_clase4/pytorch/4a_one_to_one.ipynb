{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4a - one-to-one.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEnBiuLcukJc"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## RNN one-to-one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i96B2RF8uqEb"
      },
      "source": [
        "#### Datos\n",
        "El objecto es utilizar una serie de sucuencias númericas (datos sintéticos) para poner a prueba el uso de las redes RNN. Este ejemplo se inspiró en otro artículo, lo tienen como referencia en el siguiente link:\\\n",
        "[LINK](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx0HQ-1RvJw9"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torchsummar actualmente tiene un problema con las LSTM, por eso\n",
        "# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n",
        "!pip3 install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yONnycbZ9kZ",
        "outputId": "a332a4e5-0a23-499a-f5bd-63598b9e0530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "if os.access('torch_helpers.py', os.F_OK) is False:\n",
        "    if platform.system() == 'Windows':\n",
        "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
        "    else:\n",
        "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKC5SmeuTFPv",
        "outputId": "6996b8dc-23f9-4b7a-b9e8-f504c4a99625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-29 22:07:13--  http://torch_helpers.py/\n",
            "Resolving torch_helpers.py (torch_helpers.py)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘torch_helpers.py’\n",
            "--2022-05-29 22:07:13--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23883 (23K) [text/plain]\n",
            "Saving to: ‘torch_helpers.py’\n",
            "\n",
            "torch_helpers.py    100%[===================>]  23.32K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-05-29 22:07:14 (16.4 MB/s) - ‘torch_helpers.py’ saved [23883/23883]\n",
            "\n",
            "FINISHED --2022-05-29 22:07:14--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 23K in 0.001s (16.4 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
        "    # Defino listas para realizar graficas de los resultados\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "\n",
        "    # Defino mi loop de entrenamiento\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_accuracy = 0.0\n",
        "\n",
        "        for train_data, train_target in train_loader:\n",
        "\n",
        "            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n",
        "            # los va acumulando\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(train_data)\n",
        "\n",
        "            # Computo el error de la salida comparando contra las etiquetas\n",
        "            loss = criterion(output, train_target)\n",
        "\n",
        "            # Almaceno el error del batch para luego tener el error promedio de la epoca\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            # Computo el nuevo set de gradientes a lo largo de toda la red\n",
        "            loss.backward()\n",
        "\n",
        "            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculo la media de error para la epoca de entrenamiento.\n",
        "        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n",
        "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "\n",
        "        # Realizo el paso de validación computando error y accuracy, y\n",
        "        # almacenando los valores para imprimirlos y graficarlos\n",
        "        valid_data, valid_target = iter(valid_loader).next()\n",
        "        output = model(valid_data)\n",
        "        \n",
        "        epoch_valid_loss = criterion(output, valid_target).item()\n",
        "        valid_loss.append(epoch_valid_loss)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\")\n",
        "\n",
        "    history = {\n",
        "        \"loss\": train_loss,\n",
        "        \"val_loss\": valid_loss,\n",
        "    }\n",
        "    return history"
      ],
      "metadata": {
        "id": "YMFLzZx7cJET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10bFkG1YuaD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3722e580-d7c1-4c8b-dff7-65f4fcb262e5"
      },
      "source": [
        "# Generar datos sintéticos\n",
        "X = list()\n",
        "y = list()\n",
        "X = [x+1 for x in range(20)]\n",
        "\n",
        "# \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15\n",
        "y = [x * 15 for x in X]\n",
        "\n",
        "print(\"datos X:\", X)\n",
        "print(\"datos y:\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datos X: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "datos y: [15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqabd-kYvza9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6327459-cee3-41f1-a560-907d3d8abf53"
      },
      "source": [
        "# Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1)\n",
        "X = np.array(X).reshape(len(X), 1, 1)\n",
        "print(\"datos X:\", X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datos X: [[[ 1]]\n",
            "\n",
            " [[ 2]]\n",
            "\n",
            " [[ 3]]\n",
            "\n",
            " [[ 4]]\n",
            "\n",
            " [[ 5]]\n",
            "\n",
            " [[ 6]]\n",
            "\n",
            " [[ 7]]\n",
            "\n",
            " [[ 8]]\n",
            "\n",
            " [[ 9]]\n",
            "\n",
            " [[10]]\n",
            "\n",
            " [[11]]\n",
            "\n",
            " [[12]]\n",
            "\n",
            " [[13]]\n",
            "\n",
            " [[14]]\n",
            "\n",
            " [[15]]\n",
            "\n",
            " [[16]]\n",
            "\n",
            " [[17]]\n",
            "\n",
            " [[18]]\n",
            "\n",
            " [[19]]\n",
            "\n",
            " [[20]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (batch size, seq_len, input_size)\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PytzYIMsUS1T",
        "outputId": "32c554b6-1483-4645-85f3-b89d4a9091b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYz6XpuyxBbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9c900d-25a4-41dd-b0d0-3b4882d75ed2"
      },
      "source": [
        "y = np.asanyarray(y)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        # Convertir los arrays de numpy a tensores. \n",
        "        # pytorch espera en general entradas 32bits\n",
        "        self.x = torch.from_numpy(x.astype(np.float32))\n",
        "        # las loss unfction esperan la salida float\n",
        "        self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1)\n",
        "\n",
        "        self.len = self.y.shape[0]\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "data_set = Data(X, y)\n",
        "\n",
        "input_dim = data_set.x.shape[1:]\n",
        "seq_length = input_dim[0]\n",
        "input_size = input_dim[1]\n",
        "print(\"Input dim\", input_dim)\n",
        "print(\"seq_length:\", seq_length)\n",
        "print(\"input_size:\", input_size)\n",
        "\n",
        "output_dim = data_set.y.shape[1]\n",
        "print(\"Output dim\", output_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnrjCgx9TtOU",
        "outputId": "f92f58f5-033b-44f1-a73e-9ec59cfaf102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dim torch.Size([1, 1])\n",
            "seq_length: 1\n",
            "input_size: 1\n",
            "Output dim 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vggjoIXUIve",
        "outputId": "f67086aa-eb0b-4a5a-9c82-13b49b7faf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjl1f2gXUVHU",
        "outputId": "c4f7028d-52eb-49b7-b5b8-3a29959becc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "valid_set_size = int(data_set.len * 0.2)\n",
        "train_set_size = data_set.len - valid_set_size\n",
        "\n",
        "# Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos\n",
        "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
        "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
        "\n",
        "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
        "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j2mXBQhitKg",
        "outputId": "ba5c14b3-916a-452f-a044-263231b9e1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del conjunto de entrenamiento: 16\n",
            "Tamaño del conjunto de validacion: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3-d_NXwDGD"
      },
      "source": [
        "### 2 - Entrenar el modelo (RNN y LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_helpers import CustomRNN\n",
        "\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self, input_size, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer\n",
        "        # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor\n",
        "        # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la\n",
        "        # layer de Pytorch RNN no permite modificar la funcion de activacion\n",
        "        #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer\n",
        "        self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer\n",
        "        self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        lstm_output, _ = self.rnn1(x)\n",
        "        out = self.fc(lstm_output[:,-1,:]) # take last output (last seq)\n",
        "        return out\n",
        "\n",
        "model1 = Model1(input_size=input_size, output_dim=output_dim)\n",
        "\n",
        "# Crear el optimizador la una función de error\n",
        "model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
        "model1_criterion = nn.MSELoss()  # mean squared error\n",
        "\n",
        "summary(model1, input_size=(1, seq_length, input_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwEo7Sb7fY_l",
        "outputId": "f37fa063-4679-4aba-f558-b755219bfded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model1                                   [1, 1]                    --\n",
              "├─CustomRNN: 1-1                         [1, 1, 64]                4,224\n",
              "│    └─ReLU: 2-1                         [1, 64]                   --\n",
              "├─Linear: 1-2                            [1, 1]                    65\n",
              "==========================================================================================\n",
              "Total params: 4,289\n",
              "Trainable params: 4,289\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history1 = train(model1,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                model1_optimizer,\n",
        "                model1_criterion,\n",
        "                epochs=500\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJRYAPO6hlKO",
        "outputId": "aea4b31a-1981-4842-95e4-bc6c09a3b852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/500 - Train loss 21055.309 - Valid Loss 76896.086\n",
            "Epoch: 2/500 - Train loss 20922.094 - Valid Loss 76426.508\n",
            "Epoch: 3/500 - Train loss 20790.615 - Valid Loss 75947.938\n",
            "Epoch: 4/500 - Train loss 20654.363 - Valid Loss 75444.773\n",
            "Epoch: 5/500 - Train loss 20511.916 - Valid Loss 74911.000\n",
            "Epoch: 6/500 - Train loss 20361.346 - Valid Loss 74343.531\n",
            "Epoch: 7/500 - Train loss 20200.094 - Valid Loss 73724.578\n",
            "Epoch: 8/500 - Train loss 20024.904 - Valid Loss 73053.656\n",
            "Epoch: 9/500 - Train loss 19835.967 - Valid Loss 72327.805\n",
            "Epoch: 10/500 - Train loss 19631.406 - Valid Loss 71543.961\n",
            "Epoch: 11/500 - Train loss 19410.629 - Valid Loss 70700.086\n",
            "Epoch: 12/500 - Train loss 19172.973 - Valid Loss 69792.797\n",
            "Epoch: 13/500 - Train loss 18917.514 - Valid Loss 68820.148\n",
            "Epoch: 14/500 - Train loss 18643.729 - Valid Loss 67781.375\n",
            "Epoch: 15/500 - Train loss 18351.445 - Valid Loss 66675.812\n",
            "Epoch: 16/500 - Train loss 18040.500 - Valid Loss 65503.141\n",
            "Epoch: 17/500 - Train loss 17710.822 - Valid Loss 64263.367\n",
            "Epoch: 18/500 - Train loss 17362.438 - Valid Loss 62956.758\n",
            "Epoch: 19/500 - Train loss 16995.451 - Valid Loss 61583.887\n",
            "Epoch: 20/500 - Train loss 16610.055 - Valid Loss 60145.586\n",
            "Epoch: 21/500 - Train loss 16206.518 - Valid Loss 58642.969\n",
            "Epoch: 22/500 - Train loss 15785.188 - Valid Loss 57077.500\n",
            "Epoch: 23/500 - Train loss 15346.514 - Valid Loss 55450.945\n",
            "Epoch: 24/500 - Train loss 14891.035 - Valid Loss 53765.512\n",
            "Epoch: 25/500 - Train loss 14419.415 - Valid Loss 52023.797\n",
            "Epoch: 26/500 - Train loss 13932.429 - Valid Loss 50228.848\n",
            "Epoch: 27/500 - Train loss 13430.982 - Valid Loss 48384.180\n",
            "Epoch: 28/500 - Train loss 12916.114 - Valid Loss 46493.781\n",
            "Epoch: 29/500 - Train loss 12388.998 - Valid Loss 44562.086\n",
            "Epoch: 30/500 - Train loss 11850.939 - Valid Loss 42594.047\n",
            "Epoch: 31/500 - Train loss 11303.381 - Valid Loss 40595.047\n",
            "Epoch: 32/500 - Train loss 10747.900 - Valid Loss 38570.930\n",
            "Epoch: 33/500 - Train loss 10186.198 - Valid Loss 36527.984\n",
            "Epoch: 34/500 - Train loss 9620.107 - Valid Loss 34472.918\n",
            "Epoch: 35/500 - Train loss 9051.567 - Valid Loss 32412.832\n",
            "Epoch: 36/500 - Train loss 8482.641 - Valid Loss 30355.182\n",
            "Epoch: 37/500 - Train loss 7915.480 - Valid Loss 28307.754\n",
            "Epoch: 38/500 - Train loss 7352.331 - Valid Loss 26278.604\n",
            "Epoch: 39/500 - Train loss 6795.514 - Valid Loss 24276.002\n",
            "Epoch: 40/500 - Train loss 6247.401 - Valid Loss 22308.391\n",
            "Epoch: 41/500 - Train loss 5710.410 - Valid Loss 20384.266\n",
            "Epoch: 42/500 - Train loss 5186.968 - Valid Loss 18512.143\n",
            "Epoch: 43/500 - Train loss 4679.494 - Valid Loss 16700.412\n",
            "Epoch: 44/500 - Train loss 4190.366 - Valid Loss 14957.266\n",
            "Epoch: 45/500 - Train loss 3721.895 - Valid Loss 13290.571\n",
            "Epoch: 46/500 - Train loss 3276.281 - Valid Loss 11707.729\n",
            "Epoch: 47/500 - Train loss 2855.584 - Valid Loss 10215.559\n",
            "Epoch: 48/500 - Train loss 2461.677 - Valid Loss 8820.137\n",
            "Epoch: 49/500 - Train loss 2096.208 - Valid Loss 7526.665\n",
            "Epoch: 50/500 - Train loss 1760.554 - Valid Loss 6339.306\n",
            "Epoch: 51/500 - Train loss 1455.780 - Valid Loss 5261.061\n",
            "Epoch: 52/500 - Train loss 1182.596 - Valid Loss 4293.621\n",
            "Epoch: 53/500 - Train loss 941.323 - Valid Loss 3437.276\n",
            "Epoch: 54/500 - Train loss 731.858 - Valid Loss 2690.820\n",
            "Epoch: 55/500 - Train loss 553.655 - Valid Loss 2051.503\n",
            "Epoch: 56/500 - Train loss 405.708 - Valid Loss 1515.047\n",
            "Epoch: 57/500 - Train loss 286.564 - Valid Loss 1075.684\n",
            "Epoch: 58/500 - Train loss 194.329 - Valid Loss 726.272\n",
            "Epoch: 59/500 - Train loss 126.715 - Valid Loss 458.474\n",
            "Epoch: 60/500 - Train loss 81.091 - Valid Loss 262.985\n",
            "Epoch: 61/500 - Train loss 54.552 - Valid Loss 129.824\n",
            "Epoch: 62/500 - Train loss 44.017 - Valid Loss 48.652\n",
            "Epoch: 63/500 - Train loss 46.317 - Valid Loss 9.125\n",
            "Epoch: 64/500 - Train loss 58.312 - Valid Loss 1.239\n",
            "Epoch: 65/500 - Train loss 76.986 - Valid Loss 15.659\n",
            "Epoch: 66/500 - Train loss 99.556 - Valid Loss 43.999\n",
            "Epoch: 67/500 - Train loss 123.546 - Valid Loss 79.047\n",
            "Epoch: 68/500 - Train loss 146.861 - Valid Loss 114.902\n",
            "Epoch: 69/500 - Train loss 167.822 - Valid Loss 147.052\n",
            "Epoch: 70/500 - Train loss 185.190 - Valid Loss 172.349\n",
            "Epoch: 71/500 - Train loss 198.153 - Valid Loss 188.931\n",
            "Epoch: 72/500 - Train loss 206.302 - Valid Loss 196.076\n",
            "Epoch: 73/500 - Train loss 209.583 - Valid Loss 194.017\n",
            "Epoch: 74/500 - Train loss 208.241 - Valid Loss 183.736\n",
            "Epoch: 75/500 - Train loss 202.749 - Valid Loss 166.750\n",
            "Epoch: 76/500 - Train loss 193.748 - Valid Loss 144.898\n",
            "Epoch: 77/500 - Train loss 181.978 - Valid Loss 120.153\n",
            "Epoch: 78/500 - Train loss 168.217 - Valid Loss 94.467\n",
            "Epoch: 79/500 - Train loss 153.234 - Valid Loss 69.633\n",
            "Epoch: 80/500 - Train loss 137.747 - Valid Loss 47.194\n",
            "Epoch: 81/500 - Train loss 122.392 - Valid Loss 28.383\n",
            "Epoch: 82/500 - Train loss 107.702 - Valid Loss 14.088\n",
            "Epoch: 83/500 - Train loss 94.100 - Valid Loss 4.850\n",
            "Epoch: 84/500 - Train loss 81.892 - Valid Loss 0.879\n",
            "Epoch: 85/500 - Train loss 71.274 - Valid Loss 2.085\n",
            "Epoch: 86/500 - Train loss 62.341 - Valid Loss 8.128\n",
            "Epoch: 87/500 - Train loss 55.097 - Valid Loss 18.462\n",
            "Epoch: 88/500 - Train loss 49.476 - Valid Loss 32.397\n",
            "Epoch: 89/500 - Train loss 45.351 - Valid Loss 49.144\n",
            "Epoch: 90/500 - Train loss 42.555 - Valid Loss 67.871\n",
            "Epoch: 91/500 - Train loss 40.896 - Valid Loss 87.743\n",
            "Epoch: 92/500 - Train loss 40.168 - Valid Loss 107.961\n",
            "Epoch: 93/500 - Train loss 40.162 - Valid Loss 127.791\n",
            "Epoch: 94/500 - Train loss 40.681 - Valid Loss 146.589\n",
            "Epoch: 95/500 - Train loss 41.543 - Valid Loss 163.813\n",
            "Epoch: 96/500 - Train loss 42.586 - Valid Loss 179.034\n",
            "Epoch: 97/500 - Train loss 43.675 - Valid Loss 191.936\n",
            "Epoch: 98/500 - Train loss 44.701 - Valid Loss 202.317\n",
            "Epoch: 99/500 - Train loss 45.580 - Valid Loss 210.079\n",
            "Epoch: 100/500 - Train loss 46.256 - Valid Loss 215.219\n",
            "Epoch: 101/500 - Train loss 46.695 - Valid Loss 217.818\n",
            "Epoch: 102/500 - Train loss 46.885 - Valid Loss 218.025\n",
            "Epoch: 103/500 - Train loss 46.828 - Valid Loss 216.045\n",
            "Epoch: 104/500 - Train loss 46.544 - Valid Loss 212.123\n",
            "Epoch: 105/500 - Train loss 46.062 - Valid Loss 206.532\n",
            "Epoch: 106/500 - Train loss 45.417 - Valid Loss 199.557\n",
            "Epoch: 107/500 - Train loss 44.649 - Valid Loss 191.485\n",
            "Epoch: 108/500 - Train loss 43.797 - Valid Loss 182.602\n",
            "Epoch: 109/500 - Train loss 42.902 - Valid Loss 173.170\n",
            "Epoch: 110/500 - Train loss 41.999 - Valid Loss 163.437\n",
            "Epoch: 111/500 - Train loss 41.120 - Valid Loss 153.621\n",
            "Epoch: 112/500 - Train loss 40.289 - Valid Loss 143.912\n",
            "Epoch: 113/500 - Train loss 39.528 - Valid Loss 134.471\n",
            "Epoch: 114/500 - Train loss 38.849 - Valid Loss 125.428\n",
            "Epoch: 115/500 - Train loss 38.261 - Valid Loss 116.886\n",
            "Epoch: 116/500 - Train loss 37.764 - Valid Loss 108.919\n",
            "Epoch: 117/500 - Train loss 37.357 - Valid Loss 101.577\n",
            "Epoch: 118/500 - Train loss 37.033 - Valid Loss 94.890\n",
            "Epoch: 119/500 - Train loss 36.783 - Valid Loss 88.871\n",
            "Epoch: 120/500 - Train loss 36.595 - Valid Loss 83.517\n",
            "Epoch: 121/500 - Train loss 36.458 - Valid Loss 78.812\n",
            "Epoch: 122/500 - Train loss 36.358 - Valid Loss 74.735\n",
            "Epoch: 123/500 - Train loss 36.284 - Valid Loss 71.256\n",
            "Epoch: 124/500 - Train loss 36.224 - Valid Loss 68.343\n",
            "Epoch: 125/500 - Train loss 36.170 - Valid Loss 65.960\n",
            "Epoch: 126/500 - Train loss 36.111 - Valid Loss 64.070\n",
            "Epoch: 127/500 - Train loss 36.044 - Valid Loss 62.638\n",
            "Epoch: 128/500 - Train loss 35.963 - Valid Loss 61.625\n",
            "Epoch: 129/500 - Train loss 35.866 - Valid Loss 60.997\n",
            "Epoch: 130/500 - Train loss 35.752 - Valid Loss 60.718\n",
            "Epoch: 131/500 - Train loss 35.622 - Valid Loss 60.755\n",
            "Epoch: 132/500 - Train loss 35.477 - Valid Loss 61.073\n",
            "Epoch: 133/500 - Train loss 35.319 - Valid Loss 61.639\n",
            "Epoch: 134/500 - Train loss 35.151 - Valid Loss 62.420\n",
            "Epoch: 135/500 - Train loss 34.976 - Valid Loss 63.382\n",
            "Epoch: 136/500 - Train loss 34.797 - Valid Loss 64.494\n",
            "Epoch: 137/500 - Train loss 34.616 - Valid Loss 65.722\n",
            "Epoch: 138/500 - Train loss 34.437 - Valid Loss 67.036\n",
            "Epoch: 139/500 - Train loss 34.261 - Valid Loss 68.404\n",
            "Epoch: 140/500 - Train loss 34.090 - Valid Loss 69.795\n",
            "Epoch: 141/500 - Train loss 33.925 - Valid Loss 71.183\n",
            "Epoch: 142/500 - Train loss 33.766 - Valid Loss 72.540\n",
            "Epoch: 143/500 - Train loss 33.613 - Valid Loss 73.841\n",
            "Epoch: 144/500 - Train loss 33.466 - Valid Loss 75.063\n",
            "Epoch: 145/500 - Train loss 33.325 - Valid Loss 76.189\n",
            "Epoch: 146/500 - Train loss 33.189 - Valid Loss 77.202\n",
            "Epoch: 147/500 - Train loss 33.057 - Valid Loss 78.088\n",
            "Epoch: 148/500 - Train loss 32.928 - Valid Loss 78.838\n",
            "Epoch: 149/500 - Train loss 32.801 - Valid Loss 79.445\n",
            "Epoch: 150/500 - Train loss 32.675 - Valid Loss 79.904\n",
            "Epoch: 151/500 - Train loss 32.549 - Valid Loss 80.217\n",
            "Epoch: 152/500 - Train loss 32.424 - Valid Loss 80.385\n",
            "Epoch: 153/500 - Train loss 32.298 - Valid Loss 80.413\n",
            "Epoch: 154/500 - Train loss 32.170 - Valid Loss 80.307\n",
            "Epoch: 155/500 - Train loss 32.042 - Valid Loss 80.077\n",
            "Epoch: 156/500 - Train loss 31.912 - Valid Loss 79.734\n",
            "Epoch: 157/500 - Train loss 31.782 - Valid Loss 79.289\n",
            "Epoch: 158/500 - Train loss 31.650 - Valid Loss 78.754\n",
            "Epoch: 159/500 - Train loss 31.517 - Valid Loss 78.143\n",
            "Epoch: 160/500 - Train loss 31.384 - Valid Loss 77.469\n",
            "Epoch: 161/500 - Train loss 31.250 - Valid Loss 76.744\n",
            "Epoch: 162/500 - Train loss 31.117 - Valid Loss 75.982\n",
            "Epoch: 163/500 - Train loss 30.983 - Valid Loss 75.195\n",
            "Epoch: 164/500 - Train loss 30.850 - Valid Loss 74.394\n",
            "Epoch: 165/500 - Train loss 30.717 - Valid Loss 73.589\n",
            "Epoch: 166/500 - Train loss 30.584 - Valid Loss 72.790\n",
            "Epoch: 167/500 - Train loss 30.452 - Valid Loss 72.005\n",
            "Epoch: 168/500 - Train loss 30.321 - Valid Loss 71.242\n",
            "Epoch: 169/500 - Train loss 30.191 - Valid Loss 70.506\n",
            "Epoch: 170/500 - Train loss 30.061 - Valid Loss 69.803\n",
            "Epoch: 171/500 - Train loss 29.931 - Valid Loss 69.137\n",
            "Epoch: 172/500 - Train loss 29.802 - Valid Loss 68.510\n",
            "Epoch: 173/500 - Train loss 29.673 - Valid Loss 67.924\n",
            "Epoch: 174/500 - Train loss 29.545 - Valid Loss 67.381\n",
            "Epoch: 175/500 - Train loss 29.417 - Valid Loss 66.880\n",
            "Epoch: 176/500 - Train loss 29.289 - Valid Loss 66.421\n",
            "Epoch: 177/500 - Train loss 29.161 - Valid Loss 66.002\n",
            "Epoch: 178/500 - Train loss 29.033 - Valid Loss 65.622\n",
            "Epoch: 179/500 - Train loss 28.905 - Valid Loss 65.278\n",
            "Epoch: 180/500 - Train loss 28.778 - Valid Loss 64.968\n",
            "Epoch: 181/500 - Train loss 28.650 - Valid Loss 64.687\n",
            "Epoch: 182/500 - Train loss 28.523 - Valid Loss 64.434\n",
            "Epoch: 183/500 - Train loss 28.395 - Valid Loss 64.204\n",
            "Epoch: 184/500 - Train loss 28.268 - Valid Loss 63.994\n",
            "Epoch: 185/500 - Train loss 28.140 - Valid Loss 63.800\n",
            "Epoch: 186/500 - Train loss 28.013 - Valid Loss 63.618\n",
            "Epoch: 187/500 - Train loss 27.887 - Valid Loss 63.446\n",
            "Epoch: 188/500 - Train loss 27.760 - Valid Loss 63.280\n",
            "Epoch: 189/500 - Train loss 27.633 - Valid Loss 63.117\n",
            "Epoch: 190/500 - Train loss 27.507 - Valid Loss 62.953\n",
            "Epoch: 191/500 - Train loss 27.381 - Valid Loss 62.787\n",
            "Epoch: 192/500 - Train loss 27.255 - Valid Loss 62.617\n",
            "Epoch: 193/500 - Train loss 27.130 - Valid Loss 62.439\n",
            "Epoch: 194/500 - Train loss 27.004 - Valid Loss 62.253\n",
            "Epoch: 195/500 - Train loss 26.879 - Valid Loss 62.058\n",
            "Epoch: 196/500 - Train loss 26.754 - Valid Loss 61.852\n",
            "Epoch: 197/500 - Train loss 26.630 - Valid Loss 61.634\n",
            "Epoch: 198/500 - Train loss 26.506 - Valid Loss 61.405\n",
            "Epoch: 199/500 - Train loss 26.381 - Valid Loss 61.164\n",
            "Epoch: 200/500 - Train loss 26.257 - Valid Loss 60.911\n",
            "Epoch: 201/500 - Train loss 26.134 - Valid Loss 60.647\n",
            "Epoch: 202/500 - Train loss 26.010 - Valid Loss 60.373\n",
            "Epoch: 203/500 - Train loss 25.887 - Valid Loss 60.089\n",
            "Epoch: 204/500 - Train loss 25.764 - Valid Loss 59.795\n",
            "Epoch: 205/500 - Train loss 25.641 - Valid Loss 59.494\n",
            "Epoch: 206/500 - Train loss 25.519 - Valid Loss 59.186\n",
            "Epoch: 207/500 - Train loss 25.396 - Valid Loss 58.873\n",
            "Epoch: 208/500 - Train loss 25.274 - Valid Loss 58.554\n",
            "Epoch: 209/500 - Train loss 25.152 - Valid Loss 58.233\n",
            "Epoch: 210/500 - Train loss 25.031 - Valid Loss 57.909\n",
            "Epoch: 211/500 - Train loss 24.909 - Valid Loss 57.583\n",
            "Epoch: 212/500 - Train loss 24.788 - Valid Loss 57.258\n",
            "Epoch: 213/500 - Train loss 24.668 - Valid Loss 56.933\n",
            "Epoch: 214/500 - Train loss 24.547 - Valid Loss 56.610\n",
            "Epoch: 215/500 - Train loss 24.427 - Valid Loss 56.288\n",
            "Epoch: 216/500 - Train loss 24.307 - Valid Loss 55.970\n",
            "Epoch: 217/500 - Train loss 24.187 - Valid Loss 55.655\n",
            "Epoch: 218/500 - Train loss 24.067 - Valid Loss 55.344\n",
            "Epoch: 219/500 - Train loss 23.948 - Valid Loss 55.037\n",
            "Epoch: 220/500 - Train loss 23.829 - Valid Loss 54.734\n",
            "Epoch: 221/500 - Train loss 23.710 - Valid Loss 54.436\n",
            "Epoch: 222/500 - Train loss 23.592 - Valid Loss 54.143\n",
            "Epoch: 223/500 - Train loss 23.474 - Valid Loss 53.853\n",
            "Epoch: 224/500 - Train loss 23.356 - Valid Loss 53.568\n",
            "Epoch: 225/500 - Train loss 23.238 - Valid Loss 53.287\n",
            "Epoch: 226/500 - Train loss 23.121 - Valid Loss 53.010\n",
            "Epoch: 227/500 - Train loss 23.004 - Valid Loss 52.736\n",
            "Epoch: 228/500 - Train loss 22.887 - Valid Loss 52.466\n",
            "Epoch: 229/500 - Train loss 22.771 - Valid Loss 52.198\n",
            "Epoch: 230/500 - Train loss 22.655 - Valid Loss 51.933\n",
            "Epoch: 231/500 - Train loss 22.539 - Valid Loss 51.671\n",
            "Epoch: 232/500 - Train loss 22.423 - Valid Loss 51.410\n",
            "Epoch: 233/500 - Train loss 22.308 - Valid Loss 51.151\n",
            "Epoch: 234/500 - Train loss 22.193 - Valid Loss 50.892\n",
            "Epoch: 235/500 - Train loss 22.078 - Valid Loss 50.635\n",
            "Epoch: 236/500 - Train loss 21.964 - Valid Loss 50.378\n",
            "Epoch: 237/500 - Train loss 21.850 - Valid Loss 50.122\n",
            "Epoch: 238/500 - Train loss 21.736 - Valid Loss 49.866\n",
            "Epoch: 239/500 - Train loss 21.623 - Valid Loss 49.610\n",
            "Epoch: 240/500 - Train loss 21.510 - Valid Loss 49.353\n",
            "Epoch: 241/500 - Train loss 21.397 - Valid Loss 49.097\n",
            "Epoch: 242/500 - Train loss 21.284 - Valid Loss 48.840\n",
            "Epoch: 243/500 - Train loss 21.172 - Valid Loss 48.583\n",
            "Epoch: 244/500 - Train loss 21.060 - Valid Loss 48.325\n",
            "Epoch: 245/500 - Train loss 20.949 - Valid Loss 48.066\n",
            "Epoch: 246/500 - Train loss 20.837 - Valid Loss 47.808\n",
            "Epoch: 247/500 - Train loss 20.726 - Valid Loss 47.549\n",
            "Epoch: 248/500 - Train loss 20.616 - Valid Loss 47.290\n",
            "Epoch: 249/500 - Train loss 20.505 - Valid Loss 47.031\n",
            "Epoch: 250/500 - Train loss 20.396 - Valid Loss 46.772\n",
            "Epoch: 251/500 - Train loss 20.286 - Valid Loss 46.513\n",
            "Epoch: 252/500 - Train loss 20.177 - Valid Loss 46.255\n",
            "Epoch: 253/500 - Train loss 20.068 - Valid Loss 45.997\n",
            "Epoch: 254/500 - Train loss 19.959 - Valid Loss 45.739\n",
            "Epoch: 255/500 - Train loss 19.851 - Valid Loss 45.482\n",
            "Epoch: 256/500 - Train loss 19.742 - Valid Loss 45.225\n",
            "Epoch: 257/500 - Train loss 19.635 - Valid Loss 44.969\n",
            "Epoch: 258/500 - Train loss 19.527 - Valid Loss 44.714\n",
            "Epoch: 259/500 - Train loss 19.420 - Valid Loss 44.461\n",
            "Epoch: 260/500 - Train loss 19.314 - Valid Loss 44.208\n",
            "Epoch: 261/500 - Train loss 19.207 - Valid Loss 43.956\n",
            "Epoch: 262/500 - Train loss 19.101 - Valid Loss 43.706\n",
            "Epoch: 263/500 - Train loss 18.996 - Valid Loss 43.456\n",
            "Epoch: 264/500 - Train loss 18.890 - Valid Loss 43.208\n",
            "Epoch: 265/500 - Train loss 18.785 - Valid Loss 42.961\n",
            "Epoch: 266/500 - Train loss 18.681 - Valid Loss 42.715\n",
            "Epoch: 267/500 - Train loss 18.576 - Valid Loss 42.471\n",
            "Epoch: 268/500 - Train loss 18.472 - Valid Loss 42.227\n",
            "Epoch: 269/500 - Train loss 18.369 - Valid Loss 41.985\n",
            "Epoch: 270/500 - Train loss 18.265 - Valid Loss 41.744\n",
            "Epoch: 271/500 - Train loss 18.162 - Valid Loss 41.505\n",
            "Epoch: 272/500 - Train loss 18.060 - Valid Loss 41.266\n",
            "Epoch: 273/500 - Train loss 17.958 - Valid Loss 41.028\n",
            "Epoch: 274/500 - Train loss 17.856 - Valid Loss 40.791\n",
            "Epoch: 275/500 - Train loss 17.754 - Valid Loss 40.555\n",
            "Epoch: 276/500 - Train loss 17.653 - Valid Loss 40.320\n",
            "Epoch: 277/500 - Train loss 17.552 - Valid Loss 40.087\n",
            "Epoch: 278/500 - Train loss 17.452 - Valid Loss 39.853\n",
            "Epoch: 279/500 - Train loss 17.351 - Valid Loss 39.621\n",
            "Epoch: 280/500 - Train loss 17.252 - Valid Loss 39.390\n",
            "Epoch: 281/500 - Train loss 17.152 - Valid Loss 39.159\n",
            "Epoch: 282/500 - Train loss 17.053 - Valid Loss 38.929\n",
            "Epoch: 283/500 - Train loss 16.954 - Valid Loss 38.700\n",
            "Epoch: 284/500 - Train loss 16.856 - Valid Loss 38.472\n",
            "Epoch: 285/500 - Train loss 16.758 - Valid Loss 38.244\n",
            "Epoch: 286/500 - Train loss 16.660 - Valid Loss 38.017\n",
            "Epoch: 287/500 - Train loss 16.563 - Valid Loss 37.791\n",
            "Epoch: 288/500 - Train loss 16.466 - Valid Loss 37.566\n",
            "Epoch: 289/500 - Train loss 16.369 - Valid Loss 37.341\n",
            "Epoch: 290/500 - Train loss 16.273 - Valid Loss 37.117\n",
            "Epoch: 291/500 - Train loss 16.177 - Valid Loss 36.894\n",
            "Epoch: 292/500 - Train loss 16.081 - Valid Loss 36.672\n",
            "Epoch: 293/500 - Train loss 15.986 - Valid Loss 36.450\n",
            "Epoch: 294/500 - Train loss 15.891 - Valid Loss 36.230\n",
            "Epoch: 295/500 - Train loss 15.797 - Valid Loss 36.010\n",
            "Epoch: 296/500 - Train loss 15.703 - Valid Loss 35.791\n",
            "Epoch: 297/500 - Train loss 15.609 - Valid Loss 35.573\n",
            "Epoch: 298/500 - Train loss 15.516 - Valid Loss 35.355\n",
            "Epoch: 299/500 - Train loss 15.423 - Valid Loss 35.139\n",
            "Epoch: 300/500 - Train loss 15.330 - Valid Loss 34.923\n",
            "Epoch: 301/500 - Train loss 15.238 - Valid Loss 34.708\n",
            "Epoch: 302/500 - Train loss 15.146 - Valid Loss 34.495\n",
            "Epoch: 303/500 - Train loss 15.054 - Valid Loss 34.282\n",
            "Epoch: 304/500 - Train loss 14.963 - Valid Loss 34.070\n",
            "Epoch: 305/500 - Train loss 14.872 - Valid Loss 33.859\n",
            "Epoch: 306/500 - Train loss 14.781 - Valid Loss 33.649\n",
            "Epoch: 307/500 - Train loss 14.691 - Valid Loss 33.439\n",
            "Epoch: 308/500 - Train loss 14.601 - Valid Loss 33.231\n",
            "Epoch: 309/500 - Train loss 14.512 - Valid Loss 33.024\n",
            "Epoch: 310/500 - Train loss 14.423 - Valid Loss 32.817\n",
            "Epoch: 311/500 - Train loss 14.334 - Valid Loss 32.612\n",
            "Epoch: 312/500 - Train loss 14.246 - Valid Loss 32.407\n",
            "Epoch: 313/500 - Train loss 14.158 - Valid Loss 32.203\n",
            "Epoch: 314/500 - Train loss 14.070 - Valid Loss 32.001\n",
            "Epoch: 315/500 - Train loss 13.983 - Valid Loss 31.798\n",
            "Epoch: 316/500 - Train loss 13.896 - Valid Loss 31.597\n",
            "Epoch: 317/500 - Train loss 13.810 - Valid Loss 31.397\n",
            "Epoch: 318/500 - Train loss 13.723 - Valid Loss 31.198\n",
            "Epoch: 319/500 - Train loss 13.638 - Valid Loss 31.000\n",
            "Epoch: 320/500 - Train loss 13.552 - Valid Loss 30.802\n",
            "Epoch: 321/500 - Train loss 13.467 - Valid Loss 30.605\n",
            "Epoch: 322/500 - Train loss 13.382 - Valid Loss 30.409\n",
            "Epoch: 323/500 - Train loss 13.298 - Valid Loss 30.214\n",
            "Epoch: 324/500 - Train loss 13.214 - Valid Loss 30.020\n",
            "Epoch: 325/500 - Train loss 13.130 - Valid Loss 29.827\n",
            "Epoch: 326/500 - Train loss 13.047 - Valid Loss 29.634\n",
            "Epoch: 327/500 - Train loss 12.964 - Valid Loss 29.443\n",
            "Epoch: 328/500 - Train loss 12.882 - Valid Loss 29.252\n",
            "Epoch: 329/500 - Train loss 12.799 - Valid Loss 29.062\n",
            "Epoch: 330/500 - Train loss 12.718 - Valid Loss 28.873\n",
            "Epoch: 331/500 - Train loss 12.636 - Valid Loss 28.685\n",
            "Epoch: 332/500 - Train loss 12.555 - Valid Loss 28.498\n",
            "Epoch: 333/500 - Train loss 12.474 - Valid Loss 28.312\n",
            "Epoch: 334/500 - Train loss 12.394 - Valid Loss 28.126\n",
            "Epoch: 335/500 - Train loss 12.314 - Valid Loss 27.941\n",
            "Epoch: 336/500 - Train loss 12.234 - Valid Loss 27.757\n",
            "Epoch: 337/500 - Train loss 12.155 - Valid Loss 27.574\n",
            "Epoch: 338/500 - Train loss 12.076 - Valid Loss 27.392\n",
            "Epoch: 339/500 - Train loss 11.997 - Valid Loss 27.211\n",
            "Epoch: 340/500 - Train loss 11.919 - Valid Loss 27.030\n",
            "Epoch: 341/500 - Train loss 11.841 - Valid Loss 26.850\n",
            "Epoch: 342/500 - Train loss 11.764 - Valid Loss 26.672\n",
            "Epoch: 343/500 - Train loss 11.686 - Valid Loss 26.494\n",
            "Epoch: 344/500 - Train loss 11.610 - Valid Loss 26.317\n",
            "Epoch: 345/500 - Train loss 11.533 - Valid Loss 26.141\n",
            "Epoch: 346/500 - Train loss 11.457 - Valid Loss 25.965\n",
            "Epoch: 347/500 - Train loss 11.381 - Valid Loss 25.791\n",
            "Epoch: 348/500 - Train loss 11.306 - Valid Loss 25.617\n",
            "Epoch: 349/500 - Train loss 11.231 - Valid Loss 25.444\n",
            "Epoch: 350/500 - Train loss 11.156 - Valid Loss 25.272\n",
            "Epoch: 351/500 - Train loss 11.082 - Valid Loss 25.101\n",
            "Epoch: 352/500 - Train loss 11.008 - Valid Loss 24.931\n",
            "Epoch: 353/500 - Train loss 10.934 - Valid Loss 24.762\n",
            "Epoch: 354/500 - Train loss 10.861 - Valid Loss 24.593\n",
            "Epoch: 355/500 - Train loss 10.788 - Valid Loss 24.425\n",
            "Epoch: 356/500 - Train loss 10.716 - Valid Loss 24.258\n",
            "Epoch: 357/500 - Train loss 10.643 - Valid Loss 24.093\n",
            "Epoch: 358/500 - Train loss 10.572 - Valid Loss 23.927\n",
            "Epoch: 359/500 - Train loss 10.500 - Valid Loss 23.763\n",
            "Epoch: 360/500 - Train loss 10.429 - Valid Loss 23.599\n",
            "Epoch: 361/500 - Train loss 10.358 - Valid Loss 23.437\n",
            "Epoch: 362/500 - Train loss 10.288 - Valid Loss 23.274\n",
            "Epoch: 363/500 - Train loss 10.217 - Valid Loss 23.114\n",
            "Epoch: 364/500 - Train loss 10.148 - Valid Loss 22.953\n",
            "Epoch: 365/500 - Train loss 10.078 - Valid Loss 22.794\n",
            "Epoch: 366/500 - Train loss 10.009 - Valid Loss 22.635\n",
            "Epoch: 367/500 - Train loss 9.940 - Valid Loss 22.478\n",
            "Epoch: 368/500 - Train loss 9.872 - Valid Loss 22.321\n",
            "Epoch: 369/500 - Train loss 9.804 - Valid Loss 22.164\n",
            "Epoch: 370/500 - Train loss 9.736 - Valid Loss 22.009\n",
            "Epoch: 371/500 - Train loss 9.669 - Valid Loss 21.854\n",
            "Epoch: 372/500 - Train loss 9.602 - Valid Loss 21.701\n",
            "Epoch: 373/500 - Train loss 9.535 - Valid Loss 21.548\n",
            "Epoch: 374/500 - Train loss 9.469 - Valid Loss 21.396\n",
            "Epoch: 375/500 - Train loss 9.403 - Valid Loss 21.245\n",
            "Epoch: 376/500 - Train loss 9.337 - Valid Loss 21.094\n",
            "Epoch: 377/500 - Train loss 9.272 - Valid Loss 20.944\n",
            "Epoch: 378/500 - Train loss 9.207 - Valid Loss 20.795\n",
            "Epoch: 379/500 - Train loss 9.142 - Valid Loss 20.647\n",
            "Epoch: 380/500 - Train loss 9.078 - Valid Loss 20.500\n",
            "Epoch: 381/500 - Train loss 9.014 - Valid Loss 20.353\n",
            "Epoch: 382/500 - Train loss 8.951 - Valid Loss 20.207\n",
            "Epoch: 383/500 - Train loss 8.887 - Valid Loss 20.062\n",
            "Epoch: 384/500 - Train loss 8.824 - Valid Loss 19.918\n",
            "Epoch: 385/500 - Train loss 8.762 - Valid Loss 19.775\n",
            "Epoch: 386/500 - Train loss 8.699 - Valid Loss 19.632\n",
            "Epoch: 387/500 - Train loss 8.637 - Valid Loss 19.490\n",
            "Epoch: 388/500 - Train loss 8.576 - Valid Loss 19.349\n",
            "Epoch: 389/500 - Train loss 8.514 - Valid Loss 19.209\n",
            "Epoch: 390/500 - Train loss 8.453 - Valid Loss 19.070\n",
            "Epoch: 391/500 - Train loss 8.393 - Valid Loss 18.931\n",
            "Epoch: 392/500 - Train loss 8.333 - Valid Loss 18.793\n",
            "Epoch: 393/500 - Train loss 8.273 - Valid Loss 18.656\n",
            "Epoch: 394/500 - Train loss 8.213 - Valid Loss 18.519\n",
            "Epoch: 395/500 - Train loss 8.153 - Valid Loss 18.384\n",
            "Epoch: 396/500 - Train loss 8.094 - Valid Loss 18.249\n",
            "Epoch: 397/500 - Train loss 8.036 - Valid Loss 18.115\n",
            "Epoch: 398/500 - Train loss 7.977 - Valid Loss 17.981\n",
            "Epoch: 399/500 - Train loss 7.919 - Valid Loss 17.848\n",
            "Epoch: 400/500 - Train loss 7.862 - Valid Loss 17.717\n",
            "Epoch: 401/500 - Train loss 7.804 - Valid Loss 17.586\n",
            "Epoch: 402/500 - Train loss 7.747 - Valid Loss 17.455\n",
            "Epoch: 403/500 - Train loss 7.690 - Valid Loss 17.326\n",
            "Epoch: 404/500 - Train loss 7.634 - Valid Loss 17.197\n",
            "Epoch: 405/500 - Train loss 7.578 - Valid Loss 17.069\n",
            "Epoch: 406/500 - Train loss 7.522 - Valid Loss 16.941\n",
            "Epoch: 407/500 - Train loss 7.466 - Valid Loss 16.815\n",
            "Epoch: 408/500 - Train loss 7.411 - Valid Loss 16.689\n",
            "Epoch: 409/500 - Train loss 7.356 - Valid Loss 16.563\n",
            "Epoch: 410/500 - Train loss 7.302 - Valid Loss 16.439\n",
            "Epoch: 411/500 - Train loss 7.248 - Valid Loss 16.315\n",
            "Epoch: 412/500 - Train loss 7.194 - Valid Loss 16.192\n",
            "Epoch: 413/500 - Train loss 7.140 - Valid Loss 16.070\n",
            "Epoch: 414/500 - Train loss 7.087 - Valid Loss 15.948\n",
            "Epoch: 415/500 - Train loss 7.034 - Valid Loss 15.827\n",
            "Epoch: 416/500 - Train loss 6.981 - Valid Loss 15.707\n",
            "Epoch: 417/500 - Train loss 6.928 - Valid Loss 15.588\n",
            "Epoch: 418/500 - Train loss 6.876 - Valid Loss 15.469\n",
            "Epoch: 419/500 - Train loss 6.825 - Valid Loss 15.351\n",
            "Epoch: 420/500 - Train loss 6.773 - Valid Loss 15.234\n",
            "Epoch: 421/500 - Train loss 6.722 - Valid Loss 15.117\n",
            "Epoch: 422/500 - Train loss 6.671 - Valid Loss 15.001\n",
            "Epoch: 423/500 - Train loss 6.620 - Valid Loss 14.886\n",
            "Epoch: 424/500 - Train loss 6.570 - Valid Loss 14.772\n",
            "Epoch: 425/500 - Train loss 6.520 - Valid Loss 14.658\n",
            "Epoch: 426/500 - Train loss 6.470 - Valid Loss 14.545\n",
            "Epoch: 427/500 - Train loss 6.421 - Valid Loss 14.432\n",
            "Epoch: 428/500 - Train loss 6.372 - Valid Loss 14.320\n",
            "Epoch: 429/500 - Train loss 6.323 - Valid Loss 14.209\n",
            "Epoch: 430/500 - Train loss 6.274 - Valid Loss 14.099\n",
            "Epoch: 431/500 - Train loss 6.226 - Valid Loss 13.989\n",
            "Epoch: 432/500 - Train loss 6.178 - Valid Loss 13.880\n",
            "Epoch: 433/500 - Train loss 6.131 - Valid Loss 13.772\n",
            "Epoch: 434/500 - Train loss 6.083 - Valid Loss 13.664\n",
            "Epoch: 435/500 - Train loss 6.036 - Valid Loss 13.557\n",
            "Epoch: 436/500 - Train loss 5.989 - Valid Loss 13.451\n",
            "Epoch: 437/500 - Train loss 5.943 - Valid Loss 13.345\n",
            "Epoch: 438/500 - Train loss 5.897 - Valid Loss 13.240\n",
            "Epoch: 439/500 - Train loss 5.851 - Valid Loss 13.136\n",
            "Epoch: 440/500 - Train loss 5.805 - Valid Loss 13.032\n",
            "Epoch: 441/500 - Train loss 5.760 - Valid Loss 12.929\n",
            "Epoch: 442/500 - Train loss 5.715 - Valid Loss 12.827\n",
            "Epoch: 443/500 - Train loss 5.670 - Valid Loss 12.725\n",
            "Epoch: 444/500 - Train loss 5.625 - Valid Loss 12.624\n",
            "Epoch: 445/500 - Train loss 5.581 - Valid Loss 12.523\n",
            "Epoch: 446/500 - Train loss 5.537 - Valid Loss 12.423\n",
            "Epoch: 447/500 - Train loss 5.493 - Valid Loss 12.324\n",
            "Epoch: 448/500 - Train loss 5.450 - Valid Loss 12.226\n",
            "Epoch: 449/500 - Train loss 5.407 - Valid Loss 12.128\n",
            "Epoch: 450/500 - Train loss 5.364 - Valid Loss 12.031\n",
            "Epoch: 451/500 - Train loss 5.321 - Valid Loss 11.934\n",
            "Epoch: 452/500 - Train loss 5.279 - Valid Loss 11.838\n",
            "Epoch: 453/500 - Train loss 5.237 - Valid Loss 11.742\n",
            "Epoch: 454/500 - Train loss 5.195 - Valid Loss 11.648\n",
            "Epoch: 455/500 - Train loss 5.153 - Valid Loss 11.553\n",
            "Epoch: 456/500 - Train loss 5.112 - Valid Loss 11.460\n",
            "Epoch: 457/500 - Train loss 5.071 - Valid Loss 11.367\n",
            "Epoch: 458/500 - Train loss 5.030 - Valid Loss 11.275\n",
            "Epoch: 459/500 - Train loss 4.990 - Valid Loss 11.183\n",
            "Epoch: 460/500 - Train loss 4.950 - Valid Loss 11.092\n",
            "Epoch: 461/500 - Train loss 4.910 - Valid Loss 11.001\n",
            "Epoch: 462/500 - Train loss 4.870 - Valid Loss 10.911\n",
            "Epoch: 463/500 - Train loss 4.830 - Valid Loss 10.822\n",
            "Epoch: 464/500 - Train loss 4.791 - Valid Loss 10.733\n",
            "Epoch: 465/500 - Train loss 4.752 - Valid Loss 10.645\n",
            "Epoch: 466/500 - Train loss 4.714 - Valid Loss 10.557\n",
            "Epoch: 467/500 - Train loss 4.675 - Valid Loss 10.470\n",
            "Epoch: 468/500 - Train loss 4.637 - Valid Loss 10.384\n",
            "Epoch: 469/500 - Train loss 4.599 - Valid Loss 10.298\n",
            "Epoch: 470/500 - Train loss 4.561 - Valid Loss 10.213\n",
            "Epoch: 471/500 - Train loss 4.524 - Valid Loss 10.128\n",
            "Epoch: 472/500 - Train loss 4.487 - Valid Loss 10.044\n",
            "Epoch: 473/500 - Train loss 4.450 - Valid Loss 9.961\n",
            "Epoch: 474/500 - Train loss 4.413 - Valid Loss 9.878\n",
            "Epoch: 475/500 - Train loss 4.377 - Valid Loss 9.795\n",
            "Epoch: 476/500 - Train loss 4.341 - Valid Loss 9.713\n",
            "Epoch: 477/500 - Train loss 4.305 - Valid Loss 9.632\n",
            "Epoch: 478/500 - Train loss 4.269 - Valid Loss 9.552\n",
            "Epoch: 479/500 - Train loss 4.234 - Valid Loss 9.471\n",
            "Epoch: 480/500 - Train loss 4.198 - Valid Loss 9.392\n",
            "Epoch: 481/500 - Train loss 4.163 - Valid Loss 9.313\n",
            "Epoch: 482/500 - Train loss 4.129 - Valid Loss 9.234\n",
            "Epoch: 483/500 - Train loss 4.094 - Valid Loss 9.156\n",
            "Epoch: 484/500 - Train loss 4.060 - Valid Loss 9.079\n",
            "Epoch: 485/500 - Train loss 4.026 - Valid Loss 9.002\n",
            "Epoch: 486/500 - Train loss 3.992 - Valid Loss 8.926\n",
            "Epoch: 487/500 - Train loss 3.958 - Valid Loss 8.850\n",
            "Epoch: 488/500 - Train loss 3.925 - Valid Loss 8.775\n",
            "Epoch: 489/500 - Train loss 3.892 - Valid Loss 8.700\n",
            "Epoch: 490/500 - Train loss 3.859 - Valid Loss 8.626\n",
            "Epoch: 491/500 - Train loss 3.826 - Valid Loss 8.552\n",
            "Epoch: 492/500 - Train loss 3.794 - Valid Loss 8.479\n",
            "Epoch: 493/500 - Train loss 3.762 - Valid Loss 8.406\n",
            "Epoch: 494/500 - Train loss 3.730 - Valid Loss 8.334\n",
            "Epoch: 495/500 - Train loss 3.698 - Valid Loss 8.262\n",
            "Epoch: 496/500 - Train loss 3.666 - Valid Loss 8.191\n",
            "Epoch: 497/500 - Train loss 3.635 - Valid Loss 8.121\n",
            "Epoch: 498/500 - Train loss 3.604 - Valid Loss 8.050\n",
            "Epoch: 499/500 - Train loss 3.573 - Valid Loss 7.981\n",
            "Epoch: 500/500 - Train loss 3.542 - Valid Loss 7.912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history1['loss']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history1['loss'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history1['val_loss'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY39Ruoahrsk",
        "outputId": "519cfd1d-a024-4d18-c155-ebeaad665cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88tdVCOyxcuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f938373d-89e1-4bee-e54c-d96890a49736"
      },
      "source": [
        "# Ensayo\n",
        "# x = 30\n",
        "# y_test = x * 15\n",
        "\n",
        "x_test = 30\n",
        "y_test = x_test * 15\n",
        "test_input = np.array([x_test])\n",
        "test_input = test_input.reshape((1, seq_length, input_size))\n",
        "test_input = torch.from_numpy(test_input.astype(np.float32))\n",
        "\n",
        "test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1)\n",
        "\n",
        "y_hat = model1(test_input)\n",
        "\n",
        "print(\"y_test:\", y_test)\n",
        "print(\"y_hat:\", y_hat)\n",
        "\n",
        "loss = model1_criterion(y_hat, test_target).item()\n",
        "print(\"loss:\", loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test: 450\n",
            "y_hat: tensor([[443.0439]], grad_fn=<AddmmBackward0>)\n",
            "loss: 48.38754653930664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_helpers import CustomLSTM\n",
        "\n",
        "class Model2(nn.Module):\n",
        "    def __init__(self, input_size, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer\n",
        "        # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor\n",
        "        # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la\n",
        "        # layer de Pytorch LSTM no permite modificar la funcion de activacion\n",
        "        self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer\n",
        "        self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        lstm_output, _ = self.lstm1(x)\n",
        "        out = self.fc(lstm_output[:,-1,:]) # take last output (last seq)\n",
        "        return out\n",
        "\n",
        "model2 = Model2(input_size=input_size, output_dim=output_dim)\n",
        "\n",
        "# Crear el optimizador la una función de error\n",
        "model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "model2_criterion = nn.MSELoss()  # mean squared error\n",
        "\n",
        "summary(model2, input_size=(1, seq_length, input_size))"
      ],
      "metadata": {
        "id": "WxAPmVk_WoXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10fdb4af-bce7-498f-ff5d-bc949574c886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model2                                   [1, 1]                    --\n",
              "├─CustomLSTM: 1-1                        [1, 1, 64]                16,896\n",
              "│    └─Sigmoid: 2-1                      [1, 64]                   --\n",
              "│    └─Sigmoid: 2-2                      [1, 64]                   --\n",
              "│    └─ReLU: 2-3                         [1, 64]                   --\n",
              "│    └─Sigmoid: 2-4                      [1, 64]                   --\n",
              "│    └─ReLU: 2-5                         [1, 64]                   --\n",
              "├─Linear: 1-2                            [1, 1]                    65\n",
              "==========================================================================================\n",
              "Total params: 16,961\n",
              "Trainable params: 16,961\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = train(model2,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                model2_optimizer,\n",
        "                model2_criterion,\n",
        "                epochs=500\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72MoqYZObz2V",
        "outputId": "70d60579-8b35-4a24-8f37-8780f4b925fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/500 - Train loss 21066.238 - Valid Loss 77210.188\n",
            "Epoch: 2/500 - Train loss 21021.455 - Valid Loss 77020.969\n",
            "Epoch: 3/500 - Train loss 20973.658 - Valid Loss 76810.289\n",
            "Epoch: 4/500 - Train loss 20920.668 - Valid Loss 76561.188\n",
            "Epoch: 5/500 - Train loss 20858.859 - Valid Loss 76265.078\n",
            "Epoch: 6/500 - Train loss 20787.156 - Valid Loss 75918.555\n",
            "Epoch: 7/500 - Train loss 20703.875 - Valid Loss 75515.391\n",
            "Epoch: 8/500 - Train loss 20607.242 - Valid Loss 75048.805\n",
            "Epoch: 9/500 - Train loss 20495.682 - Valid Loss 74514.375\n",
            "Epoch: 10/500 - Train loss 20367.816 - Valid Loss 73908.453\n",
            "Epoch: 11/500 - Train loss 20222.318 - Valid Loss 73228.125\n",
            "Epoch: 12/500 - Train loss 20057.967 - Valid Loss 72471.617\n",
            "Epoch: 13/500 - Train loss 19873.768 - Valid Loss 71638.234\n",
            "Epoch: 14/500 - Train loss 19668.932 - Valid Loss 70728.258\n",
            "Epoch: 15/500 - Train loss 19442.809 - Valid Loss 69742.562\n",
            "Epoch: 16/500 - Train loss 19194.949 - Valid Loss 68682.812\n",
            "Epoch: 17/500 - Train loss 18925.117 - Valid Loss 67551.047\n",
            "Epoch: 18/500 - Train loss 18633.248 - Valid Loss 66349.555\n",
            "Epoch: 19/500 - Train loss 18319.453 - Valid Loss 65080.695\n",
            "Epoch: 20/500 - Train loss 17983.994 - Valid Loss 63746.773\n",
            "Epoch: 21/500 - Train loss 17627.270 - Valid Loss 62349.996\n",
            "Epoch: 22/500 - Train loss 17249.809 - Valid Loss 60892.531\n",
            "Epoch: 23/500 - Train loss 16852.242 - Valid Loss 59376.531\n",
            "Epoch: 24/500 - Train loss 16435.295 - Valid Loss 57804.250\n",
            "Epoch: 25/500 - Train loss 15999.784 - Valid Loss 56178.137\n",
            "Epoch: 26/500 - Train loss 15546.617 - Valid Loss 54500.844\n",
            "Epoch: 27/500 - Train loss 15076.765 - Valid Loss 52775.281\n",
            "Epoch: 28/500 - Train loss 14591.283 - Valid Loss 51004.555\n",
            "Epoch: 29/500 - Train loss 14091.293 - Valid Loss 49191.988\n",
            "Epoch: 30/500 - Train loss 13577.985 - Valid Loss 47341.004\n",
            "Epoch: 31/500 - Train loss 13052.609 - Valid Loss 45455.172\n",
            "Epoch: 32/500 - Train loss 12516.478 - Valid Loss 43538.172\n",
            "Epoch: 33/500 - Train loss 11970.955 - Valid Loss 41593.848\n",
            "Epoch: 34/500 - Train loss 11417.471 - Valid Loss 39626.195\n",
            "Epoch: 35/500 - Train loss 10857.512 - Valid Loss 37639.469\n",
            "Epoch: 36/500 - Train loss 10292.637 - Valid Loss 35638.242\n",
            "Epoch: 37/500 - Train loss 9724.476 - Valid Loss 33627.484\n",
            "Epoch: 38/500 - Train loss 9154.747 - Valid Loss 31612.725\n",
            "Epoch: 39/500 - Train loss 8585.259 - Valid Loss 29600.164\n",
            "Epoch: 40/500 - Train loss 8017.925 - Valid Loss 27596.791\n",
            "Epoch: 41/500 - Train loss 7454.771 - Valid Loss 25610.393\n",
            "Epoch: 42/500 - Train loss 6897.932 - Valid Loss 23649.484\n",
            "Epoch: 43/500 - Train loss 6349.647 - Valid Loss 21723.037\n",
            "Epoch: 44/500 - Train loss 5812.232 - Valid Loss 19840.188\n",
            "Epoch: 45/500 - Train loss 5288.053 - Valid Loss 18009.926\n",
            "Epoch: 46/500 - Train loss 4779.481 - Valid Loss 16240.815\n",
            "Epoch: 47/500 - Train loss 4288.851 - Valid Loss 14540.865\n",
            "Epoch: 48/500 - Train loss 3818.413 - Valid Loss 12917.450\n",
            "Epoch: 49/500 - Train loss 3370.293 - Valid Loss 11377.277\n",
            "Epoch: 50/500 - Train loss 2946.462 - Valid Loss 9926.388\n",
            "Epoch: 51/500 - Train loss 2548.693 - Valid Loss 8570.094\n",
            "Epoch: 52/500 - Train loss 2178.539 - Valid Loss 7312.919\n",
            "Epoch: 53/500 - Train loss 1837.298 - Valid Loss 6158.502\n",
            "Epoch: 54/500 - Train loss 1525.986 - Valid Loss 5109.481\n",
            "Epoch: 55/500 - Train loss 1245.303 - Valid Loss 4167.387\n",
            "Epoch: 56/500 - Train loss 995.603 - Valid Loss 3332.529\n",
            "Epoch: 57/500 - Train loss 776.870 - Valid Loss 2603.891\n",
            "Epoch: 58/500 - Train loss 588.691 - Valid Loss 1979.090\n",
            "Epoch: 59/500 - Train loss 430.246 - Valid Loss 1454.339\n",
            "Epoch: 60/500 - Train loss 300.302 - Valid Loss 1024.463\n",
            "Epoch: 61/500 - Train loss 197.221 - Valid Loss 682.979\n",
            "Epoch: 62/500 - Train loss 118.985 - Valid Loss 422.220\n",
            "Epoch: 63/500 - Train loss 63.237 - Valid Loss 233.512\n",
            "Epoch: 64/500 - Train loss 27.332 - Valid Loss 107.422\n",
            "Epoch: 65/500 - Train loss 8.418 - Valid Loss 34.040\n",
            "Epoch: 66/500 - Train loss 3.511 - Valid Loss 3.291\n",
            "Epoch: 67/500 - Train loss 9.598 - Valid Loss 5.272\n",
            "Epoch: 68/500 - Train loss 23.731 - Valid Loss 30.579\n",
            "Epoch: 69/500 - Train loss 43.126 - Valid Loss 70.603\n",
            "Epoch: 70/500 - Train loss 65.250 - Valid Loss 117.785\n",
            "Epoch: 71/500 - Train loss 87.896 - Valid Loss 165.795\n",
            "Epoch: 72/500 - Train loss 109.234 - Valid Loss 209.641\n",
            "Epoch: 73/500 - Train loss 127.846 - Valid Loss 245.698\n",
            "Epoch: 74/500 - Train loss 142.725 - Valid Loss 271.656\n",
            "Epoch: 75/500 - Train loss 153.268 - Valid Loss 286.407\n",
            "Epoch: 76/500 - Train loss 159.233 - Valid Loss 289.884\n",
            "Epoch: 77/500 - Train loss 160.693 - Valid Loss 282.856\n",
            "Epoch: 78/500 - Train loss 157.977 - Valid Loss 266.730\n",
            "Epoch: 79/500 - Train loss 151.602 - Valid Loss 243.331\n",
            "Epoch: 80/500 - Train loss 142.213 - Valid Loss 214.708\n",
            "Epoch: 81/500 - Train loss 130.524 - Valid Loss 182.965\n",
            "Epoch: 82/500 - Train loss 117.263 - Valid Loss 150.116\n",
            "Epoch: 83/500 - Train loss 103.130 - Valid Loss 117.974\n",
            "Epoch: 84/500 - Train loss 88.766 - Valid Loss 88.077\n",
            "Epoch: 85/500 - Train loss 74.725 - Valid Loss 61.642\n",
            "Epoch: 86/500 - Train loss 61.460 - Valid Loss 39.539\n",
            "Epoch: 87/500 - Train loss 49.321 - Valid Loss 22.306\n",
            "Epoch: 88/500 - Train loss 38.552 - Valid Loss 10.165\n",
            "Epoch: 89/500 - Train loss 29.296 - Valid Loss 3.058\n",
            "Epoch: 90/500 - Train loss 21.609 - Valid Loss 0.696\n",
            "Epoch: 91/500 - Train loss 15.468 - Valid Loss 2.605\n",
            "Epoch: 92/500 - Train loss 10.792 - Valid Loss 8.175\n",
            "Epoch: 93/500 - Train loss 7.450 - Valid Loss 16.715\n",
            "Epoch: 94/500 - Train loss 5.280 - Valid Loss 27.490\n",
            "Epoch: 95/500 - Train loss 4.099 - Valid Loss 39.766\n",
            "Epoch: 96/500 - Train loss 3.717 - Valid Loss 52.841\n",
            "Epoch: 97/500 - Train loss 3.947 - Valid Loss 66.070\n",
            "Epoch: 98/500 - Train loss 4.611 - Valid Loss 78.883\n",
            "Epoch: 99/500 - Train loss 5.550 - Valid Loss 90.803\n",
            "Epoch: 100/500 - Train loss 6.622 - Valid Loss 101.448\n",
            "Epoch: 101/500 - Train loss 7.712 - Valid Loss 110.534\n",
            "Epoch: 102/500 - Train loss 8.725 - Valid Loss 117.872\n",
            "Epoch: 103/500 - Train loss 9.594 - Valid Loss 123.362\n",
            "Epoch: 104/500 - Train loss 10.272 - Valid Loss 126.984\n",
            "Epoch: 105/500 - Train loss 10.733 - Valid Loss 128.787\n",
            "Epoch: 106/500 - Train loss 10.969 - Valid Loss 128.879\n",
            "Epoch: 107/500 - Train loss 10.987 - Valid Loss 127.410\n",
            "Epoch: 108/500 - Train loss 10.804 - Valid Loss 124.565\n",
            "Epoch: 109/500 - Train loss 10.449 - Valid Loss 120.549\n",
            "Epoch: 110/500 - Train loss 9.954 - Valid Loss 115.577\n",
            "Epoch: 111/500 - Train loss 9.354 - Valid Loss 109.868\n",
            "Epoch: 112/500 - Train loss 8.685 - Valid Loss 103.630\n",
            "Epoch: 113/500 - Train loss 7.982 - Valid Loss 97.061\n",
            "Epoch: 114/500 - Train loss 7.276 - Valid Loss 90.340\n",
            "Epoch: 115/500 - Train loss 6.594 - Valid Loss 83.625\n",
            "Epoch: 116/500 - Train loss 5.959 - Valid Loss 77.048\n",
            "Epoch: 117/500 - Train loss 5.387 - Valid Loss 70.720\n",
            "Epoch: 118/500 - Train loss 4.889 - Valid Loss 64.726\n",
            "Epoch: 119/500 - Train loss 4.472 - Valid Loss 59.131\n",
            "Epoch: 120/500 - Train loss 4.137 - Valid Loss 53.976\n",
            "Epoch: 121/500 - Train loss 3.882 - Valid Loss 49.287\n",
            "Epoch: 122/500 - Train loss 3.700 - Valid Loss 45.074\n",
            "Epoch: 123/500 - Train loss 3.584 - Valid Loss 41.332\n",
            "Epoch: 124/500 - Train loss 3.523 - Valid Loss 38.050\n",
            "Epoch: 125/500 - Train loss 3.507 - Valid Loss 35.208\n",
            "Epoch: 126/500 - Train loss 3.524 - Valid Loss 32.781\n",
            "Epoch: 127/500 - Train loss 3.565 - Valid Loss 30.742\n",
            "Epoch: 128/500 - Train loss 3.618 - Valid Loss 29.061\n",
            "Epoch: 129/500 - Train loss 3.676 - Valid Loss 27.711\n",
            "Epoch: 130/500 - Train loss 3.732 - Valid Loss 26.664\n",
            "Epoch: 131/500 - Train loss 3.780 - Valid Loss 25.893\n",
            "Epoch: 132/500 - Train loss 3.817 - Valid Loss 25.375\n",
            "Epoch: 133/500 - Train loss 3.840 - Valid Loss 25.086\n",
            "Epoch: 134/500 - Train loss 3.849 - Valid Loss 25.005\n",
            "Epoch: 135/500 - Train loss 3.843 - Valid Loss 25.113\n",
            "Epoch: 136/500 - Train loss 3.824 - Valid Loss 25.391\n",
            "Epoch: 137/500 - Train loss 3.794 - Valid Loss 25.820\n",
            "Epoch: 138/500 - Train loss 3.755 - Valid Loss 26.383\n",
            "Epoch: 139/500 - Train loss 3.709 - Valid Loss 27.061\n",
            "Epoch: 140/500 - Train loss 3.660 - Valid Loss 27.837\n",
            "Epoch: 141/500 - Train loss 3.609 - Valid Loss 28.693\n",
            "Epoch: 142/500 - Train loss 3.560 - Valid Loss 29.611\n",
            "Epoch: 143/500 - Train loss 3.512 - Valid Loss 30.572\n",
            "Epoch: 144/500 - Train loss 3.469 - Valid Loss 31.560\n",
            "Epoch: 145/500 - Train loss 3.430 - Valid Loss 32.555\n",
            "Epoch: 146/500 - Train loss 3.397 - Valid Loss 33.541\n",
            "Epoch: 147/500 - Train loss 3.369 - Valid Loss 34.503\n",
            "Epoch: 148/500 - Train loss 3.347 - Valid Loss 35.425\n",
            "Epoch: 149/500 - Train loss 3.330 - Valid Loss 36.294\n",
            "Epoch: 150/500 - Train loss 3.317 - Valid Loss 37.099\n",
            "Epoch: 151/500 - Train loss 3.307 - Valid Loss 37.831\n",
            "Epoch: 152/500 - Train loss 3.301 - Valid Loss 38.480\n",
            "Epoch: 153/500 - Train loss 3.296 - Valid Loss 39.041\n",
            "Epoch: 154/500 - Train loss 3.293 - Valid Loss 39.512\n",
            "Epoch: 155/500 - Train loss 3.291 - Valid Loss 39.889\n",
            "Epoch: 156/500 - Train loss 3.288 - Valid Loss 40.174\n",
            "Epoch: 157/500 - Train loss 3.285 - Valid Loss 40.368\n",
            "Epoch: 158/500 - Train loss 3.282 - Valid Loss 40.475\n",
            "Epoch: 159/500 - Train loss 3.277 - Valid Loss 40.500\n",
            "Epoch: 160/500 - Train loss 3.271 - Valid Loss 40.449\n",
            "Epoch: 161/500 - Train loss 3.264 - Valid Loss 40.329\n",
            "Epoch: 162/500 - Train loss 3.257 - Valid Loss 40.147\n",
            "Epoch: 163/500 - Train loss 3.248 - Valid Loss 39.913\n",
            "Epoch: 164/500 - Train loss 3.239 - Valid Loss 39.633\n",
            "Epoch: 165/500 - Train loss 3.230 - Valid Loss 39.317\n",
            "Epoch: 166/500 - Train loss 3.220 - Valid Loss 38.972\n",
            "Epoch: 167/500 - Train loss 3.210 - Valid Loss 38.607\n",
            "Epoch: 168/500 - Train loss 3.200 - Valid Loss 38.229\n",
            "Epoch: 169/500 - Train loss 3.191 - Valid Loss 37.845\n",
            "Epoch: 170/500 - Train loss 3.182 - Valid Loss 37.461\n",
            "Epoch: 171/500 - Train loss 3.173 - Valid Loss 37.084\n",
            "Epoch: 172/500 - Train loss 3.165 - Valid Loss 36.717\n",
            "Epoch: 173/500 - Train loss 3.157 - Valid Loss 36.366\n",
            "Epoch: 174/500 - Train loss 3.150 - Valid Loss 36.033\n",
            "Epoch: 175/500 - Train loss 3.143 - Valid Loss 35.722\n",
            "Epoch: 176/500 - Train loss 3.136 - Valid Loss 35.436\n",
            "Epoch: 177/500 - Train loss 3.129 - Valid Loss 35.175\n",
            "Epoch: 178/500 - Train loss 3.123 - Valid Loss 34.940\n",
            "Epoch: 179/500 - Train loss 3.117 - Valid Loss 34.732\n",
            "Epoch: 180/500 - Train loss 3.111 - Valid Loss 34.551\n",
            "Epoch: 181/500 - Train loss 3.105 - Valid Loss 34.396\n",
            "Epoch: 182/500 - Train loss 3.099 - Valid Loss 34.267\n",
            "Epoch: 183/500 - Train loss 3.093 - Valid Loss 34.162\n",
            "Epoch: 184/500 - Train loss 3.087 - Valid Loss 34.079\n",
            "Epoch: 185/500 - Train loss 3.081 - Valid Loss 34.016\n",
            "Epoch: 186/500 - Train loss 3.075 - Valid Loss 33.973\n",
            "Epoch: 187/500 - Train loss 3.069 - Valid Loss 33.946\n",
            "Epoch: 188/500 - Train loss 3.063 - Valid Loss 33.934\n",
            "Epoch: 189/500 - Train loss 3.057 - Valid Loss 33.934\n",
            "Epoch: 190/500 - Train loss 3.051 - Valid Loss 33.944\n",
            "Epoch: 191/500 - Train loss 3.044 - Valid Loss 33.961\n",
            "Epoch: 192/500 - Train loss 3.038 - Valid Loss 33.984\n",
            "Epoch: 193/500 - Train loss 3.032 - Valid Loss 34.011\n",
            "Epoch: 194/500 - Train loss 3.026 - Valid Loss 34.038\n",
            "Epoch: 195/500 - Train loss 3.020 - Valid Loss 34.066\n",
            "Epoch: 196/500 - Train loss 3.014 - Valid Loss 34.092\n",
            "Epoch: 197/500 - Train loss 3.008 - Valid Loss 34.114\n",
            "Epoch: 198/500 - Train loss 3.003 - Valid Loss 34.132\n",
            "Epoch: 199/500 - Train loss 2.997 - Valid Loss 34.144\n",
            "Epoch: 200/500 - Train loss 2.991 - Valid Loss 34.149\n",
            "Epoch: 201/500 - Train loss 2.986 - Valid Loss 34.148\n",
            "Epoch: 202/500 - Train loss 2.980 - Valid Loss 34.139\n",
            "Epoch: 203/500 - Train loss 2.974 - Valid Loss 34.123\n",
            "Epoch: 204/500 - Train loss 2.969 - Valid Loss 34.098\n",
            "Epoch: 205/500 - Train loss 2.964 - Valid Loss 34.066\n",
            "Epoch: 206/500 - Train loss 2.958 - Valid Loss 34.026\n",
            "Epoch: 207/500 - Train loss 2.953 - Valid Loss 33.979\n",
            "Epoch: 208/500 - Train loss 2.947 - Valid Loss 33.925\n",
            "Epoch: 209/500 - Train loss 2.942 - Valid Loss 33.866\n",
            "Epoch: 210/500 - Train loss 2.937 - Valid Loss 33.800\n",
            "Epoch: 211/500 - Train loss 2.931 - Valid Loss 33.731\n",
            "Epoch: 212/500 - Train loss 2.926 - Valid Loss 33.656\n",
            "Epoch: 213/500 - Train loss 2.921 - Valid Loss 33.579\n",
            "Epoch: 214/500 - Train loss 2.916 - Valid Loss 33.499\n",
            "Epoch: 215/500 - Train loss 2.911 - Valid Loss 33.416\n",
            "Epoch: 216/500 - Train loss 2.906 - Valid Loss 33.333\n",
            "Epoch: 217/500 - Train loss 2.900 - Valid Loss 33.249\n",
            "Epoch: 218/500 - Train loss 2.895 - Valid Loss 33.166\n",
            "Epoch: 219/500 - Train loss 2.890 - Valid Loss 33.083\n",
            "Epoch: 220/500 - Train loss 2.885 - Valid Loss 33.001\n",
            "Epoch: 221/500 - Train loss 2.880 - Valid Loss 32.920\n",
            "Epoch: 222/500 - Train loss 2.875 - Valid Loss 32.841\n",
            "Epoch: 223/500 - Train loss 2.870 - Valid Loss 32.765\n",
            "Epoch: 224/500 - Train loss 2.866 - Valid Loss 32.691\n",
            "Epoch: 225/500 - Train loss 2.861 - Valid Loss 32.619\n",
            "Epoch: 226/500 - Train loss 2.856 - Valid Loss 32.550\n",
            "Epoch: 227/500 - Train loss 2.851 - Valid Loss 32.484\n",
            "Epoch: 228/500 - Train loss 2.846 - Valid Loss 32.420\n",
            "Epoch: 229/500 - Train loss 2.842 - Valid Loss 32.359\n",
            "Epoch: 230/500 - Train loss 2.837 - Valid Loss 32.300\n",
            "Epoch: 231/500 - Train loss 2.832 - Valid Loss 32.244\n",
            "Epoch: 232/500 - Train loss 2.828 - Valid Loss 32.190\n",
            "Epoch: 233/500 - Train loss 2.823 - Valid Loss 32.137\n",
            "Epoch: 234/500 - Train loss 2.819 - Valid Loss 32.086\n",
            "Epoch: 235/500 - Train loss 2.814 - Valid Loss 32.037\n",
            "Epoch: 236/500 - Train loss 2.810 - Valid Loss 31.989\n",
            "Epoch: 237/500 - Train loss 2.805 - Valid Loss 31.942\n",
            "Epoch: 238/500 - Train loss 2.801 - Valid Loss 31.896\n",
            "Epoch: 239/500 - Train loss 2.796 - Valid Loss 31.850\n",
            "Epoch: 240/500 - Train loss 2.792 - Valid Loss 31.806\n",
            "Epoch: 241/500 - Train loss 2.787 - Valid Loss 31.761\n",
            "Epoch: 242/500 - Train loss 2.783 - Valid Loss 31.716\n",
            "Epoch: 243/500 - Train loss 2.779 - Valid Loss 31.670\n",
            "Epoch: 244/500 - Train loss 2.774 - Valid Loss 31.625\n",
            "Epoch: 245/500 - Train loss 2.770 - Valid Loss 31.579\n",
            "Epoch: 246/500 - Train loss 2.766 - Valid Loss 31.533\n",
            "Epoch: 247/500 - Train loss 2.762 - Valid Loss 31.486\n",
            "Epoch: 248/500 - Train loss 2.757 - Valid Loss 31.438\n",
            "Epoch: 249/500 - Train loss 2.753 - Valid Loss 31.390\n",
            "Epoch: 250/500 - Train loss 2.749 - Valid Loss 31.342\n",
            "Epoch: 251/500 - Train loss 2.745 - Valid Loss 31.293\n",
            "Epoch: 252/500 - Train loss 2.741 - Valid Loss 31.242\n",
            "Epoch: 253/500 - Train loss 2.737 - Valid Loss 31.192\n",
            "Epoch: 254/500 - Train loss 2.733 - Valid Loss 31.141\n",
            "Epoch: 255/500 - Train loss 2.729 - Valid Loss 31.090\n",
            "Epoch: 256/500 - Train loss 2.725 - Valid Loss 31.039\n",
            "Epoch: 257/500 - Train loss 2.721 - Valid Loss 30.987\n",
            "Epoch: 258/500 - Train loss 2.717 - Valid Loss 30.935\n",
            "Epoch: 259/500 - Train loss 2.713 - Valid Loss 30.882\n",
            "Epoch: 260/500 - Train loss 2.709 - Valid Loss 30.830\n",
            "Epoch: 261/500 - Train loss 2.705 - Valid Loss 30.778\n",
            "Epoch: 262/500 - Train loss 2.701 - Valid Loss 30.726\n",
            "Epoch: 263/500 - Train loss 2.697 - Valid Loss 30.674\n",
            "Epoch: 264/500 - Train loss 2.693 - Valid Loss 30.622\n",
            "Epoch: 265/500 - Train loss 2.690 - Valid Loss 30.571\n",
            "Epoch: 266/500 - Train loss 2.686 - Valid Loss 30.520\n",
            "Epoch: 267/500 - Train loss 2.682 - Valid Loss 30.469\n",
            "Epoch: 268/500 - Train loss 2.678 - Valid Loss 30.419\n",
            "Epoch: 269/500 - Train loss 2.675 - Valid Loss 30.369\n",
            "Epoch: 270/500 - Train loss 2.671 - Valid Loss 30.320\n",
            "Epoch: 271/500 - Train loss 2.667 - Valid Loss 30.271\n",
            "Epoch: 272/500 - Train loss 2.664 - Valid Loss 30.222\n",
            "Epoch: 273/500 - Train loss 2.660 - Valid Loss 30.174\n",
            "Epoch: 274/500 - Train loss 2.656 - Valid Loss 30.127\n",
            "Epoch: 275/500 - Train loss 2.653 - Valid Loss 30.079\n",
            "Epoch: 276/500 - Train loss 2.649 - Valid Loss 30.032\n",
            "Epoch: 277/500 - Train loss 2.646 - Valid Loss 29.986\n",
            "Epoch: 278/500 - Train loss 2.642 - Valid Loss 29.940\n",
            "Epoch: 279/500 - Train loss 2.639 - Valid Loss 29.894\n",
            "Epoch: 280/500 - Train loss 2.635 - Valid Loss 29.848\n",
            "Epoch: 281/500 - Train loss 2.632 - Valid Loss 29.803\n",
            "Epoch: 282/500 - Train loss 2.628 - Valid Loss 29.758\n",
            "Epoch: 283/500 - Train loss 2.625 - Valid Loss 29.713\n",
            "Epoch: 284/500 - Train loss 2.621 - Valid Loss 29.669\n",
            "Epoch: 285/500 - Train loss 2.618 - Valid Loss 29.624\n",
            "Epoch: 286/500 - Train loss 2.614 - Valid Loss 29.580\n",
            "Epoch: 287/500 - Train loss 2.611 - Valid Loss 29.536\n",
            "Epoch: 288/500 - Train loss 2.608 - Valid Loss 29.491\n",
            "Epoch: 289/500 - Train loss 2.604 - Valid Loss 29.447\n",
            "Epoch: 290/500 - Train loss 2.601 - Valid Loss 29.403\n",
            "Epoch: 291/500 - Train loss 2.598 - Valid Loss 29.360\n",
            "Epoch: 292/500 - Train loss 2.594 - Valid Loss 29.316\n",
            "Epoch: 293/500 - Train loss 2.591 - Valid Loss 29.272\n",
            "Epoch: 294/500 - Train loss 2.588 - Valid Loss 29.229\n",
            "Epoch: 295/500 - Train loss 2.585 - Valid Loss 29.185\n",
            "Epoch: 296/500 - Train loss 2.581 - Valid Loss 29.142\n",
            "Epoch: 297/500 - Train loss 2.578 - Valid Loss 29.098\n",
            "Epoch: 298/500 - Train loss 2.575 - Valid Loss 29.055\n",
            "Epoch: 299/500 - Train loss 2.572 - Valid Loss 29.012\n",
            "Epoch: 300/500 - Train loss 2.569 - Valid Loss 28.969\n",
            "Epoch: 301/500 - Train loss 2.565 - Valid Loss 28.926\n",
            "Epoch: 302/500 - Train loss 2.562 - Valid Loss 28.883\n",
            "Epoch: 303/500 - Train loss 2.559 - Valid Loss 28.841\n",
            "Epoch: 304/500 - Train loss 2.556 - Valid Loss 28.798\n",
            "Epoch: 305/500 - Train loss 2.553 - Valid Loss 28.756\n",
            "Epoch: 306/500 - Train loss 2.550 - Valid Loss 28.714\n",
            "Epoch: 307/500 - Train loss 2.547 - Valid Loss 28.672\n",
            "Epoch: 308/500 - Train loss 2.544 - Valid Loss 28.630\n",
            "Epoch: 309/500 - Train loss 2.541 - Valid Loss 28.589\n",
            "Epoch: 310/500 - Train loss 2.538 - Valid Loss 28.548\n",
            "Epoch: 311/500 - Train loss 2.535 - Valid Loss 28.506\n",
            "Epoch: 312/500 - Train loss 2.532 - Valid Loss 28.465\n",
            "Epoch: 313/500 - Train loss 2.529 - Valid Loss 28.425\n",
            "Epoch: 314/500 - Train loss 2.526 - Valid Loss 28.384\n",
            "Epoch: 315/500 - Train loss 2.523 - Valid Loss 28.344\n",
            "Epoch: 316/500 - Train loss 2.520 - Valid Loss 28.303\n",
            "Epoch: 317/500 - Train loss 2.517 - Valid Loss 28.263\n",
            "Epoch: 318/500 - Train loss 2.514 - Valid Loss 28.223\n",
            "Epoch: 319/500 - Train loss 2.511 - Valid Loss 28.184\n",
            "Epoch: 320/500 - Train loss 2.508 - Valid Loss 28.144\n",
            "Epoch: 321/500 - Train loss 2.505 - Valid Loss 28.105\n",
            "Epoch: 322/500 - Train loss 2.502 - Valid Loss 28.065\n",
            "Epoch: 323/500 - Train loss 2.499 - Valid Loss 28.026\n",
            "Epoch: 324/500 - Train loss 2.496 - Valid Loss 27.988\n",
            "Epoch: 325/500 - Train loss 2.493 - Valid Loss 27.949\n",
            "Epoch: 326/500 - Train loss 2.490 - Valid Loss 27.910\n",
            "Epoch: 327/500 - Train loss 2.488 - Valid Loss 27.872\n",
            "Epoch: 328/500 - Train loss 2.485 - Valid Loss 27.834\n",
            "Epoch: 329/500 - Train loss 2.482 - Valid Loss 27.796\n",
            "Epoch: 330/500 - Train loss 2.479 - Valid Loss 27.758\n",
            "Epoch: 331/500 - Train loss 2.476 - Valid Loss 27.720\n",
            "Epoch: 332/500 - Train loss 2.473 - Valid Loss 27.682\n",
            "Epoch: 333/500 - Train loss 2.471 - Valid Loss 27.645\n",
            "Epoch: 334/500 - Train loss 2.468 - Valid Loss 27.607\n",
            "Epoch: 335/500 - Train loss 2.465 - Valid Loss 27.570\n",
            "Epoch: 336/500 - Train loss 2.462 - Valid Loss 27.532\n",
            "Epoch: 337/500 - Train loss 2.460 - Valid Loss 27.495\n",
            "Epoch: 338/500 - Train loss 2.457 - Valid Loss 27.458\n",
            "Epoch: 339/500 - Train loss 2.454 - Valid Loss 27.422\n",
            "Epoch: 340/500 - Train loss 2.451 - Valid Loss 27.385\n",
            "Epoch: 341/500 - Train loss 2.449 - Valid Loss 27.348\n",
            "Epoch: 342/500 - Train loss 2.446 - Valid Loss 27.312\n",
            "Epoch: 343/500 - Train loss 2.443 - Valid Loss 27.276\n",
            "Epoch: 344/500 - Train loss 2.440 - Valid Loss 27.240\n",
            "Epoch: 345/500 - Train loss 2.438 - Valid Loss 27.204\n",
            "Epoch: 346/500 - Train loss 2.435 - Valid Loss 27.168\n",
            "Epoch: 347/500 - Train loss 2.432 - Valid Loss 27.132\n",
            "Epoch: 348/500 - Train loss 2.430 - Valid Loss 27.096\n",
            "Epoch: 349/500 - Train loss 2.427 - Valid Loss 27.061\n",
            "Epoch: 350/500 - Train loss 2.424 - Valid Loss 27.026\n",
            "Epoch: 351/500 - Train loss 2.421 - Valid Loss 26.990\n",
            "Epoch: 352/500 - Train loss 2.419 - Valid Loss 26.955\n",
            "Epoch: 353/500 - Train loss 2.416 - Valid Loss 26.920\n",
            "Epoch: 354/500 - Train loss 2.413 - Valid Loss 26.885\n",
            "Epoch: 355/500 - Train loss 2.411 - Valid Loss 26.851\n",
            "Epoch: 356/500 - Train loss 2.408 - Valid Loss 26.816\n",
            "Epoch: 357/500 - Train loss 2.406 - Valid Loss 26.782\n",
            "Epoch: 358/500 - Train loss 2.403 - Valid Loss 26.747\n",
            "Epoch: 359/500 - Train loss 2.400 - Valid Loss 26.713\n",
            "Epoch: 360/500 - Train loss 2.398 - Valid Loss 26.679\n",
            "Epoch: 361/500 - Train loss 2.395 - Valid Loss 26.645\n",
            "Epoch: 362/500 - Train loss 2.392 - Valid Loss 26.611\n",
            "Epoch: 363/500 - Train loss 2.390 - Valid Loss 26.578\n",
            "Epoch: 364/500 - Train loss 2.387 - Valid Loss 26.544\n",
            "Epoch: 365/500 - Train loss 2.385 - Valid Loss 26.511\n",
            "Epoch: 366/500 - Train loss 2.382 - Valid Loss 26.477\n",
            "Epoch: 367/500 - Train loss 2.379 - Valid Loss 26.444\n",
            "Epoch: 368/500 - Train loss 2.377 - Valid Loss 26.411\n",
            "Epoch: 369/500 - Train loss 2.374 - Valid Loss 26.378\n",
            "Epoch: 370/500 - Train loss 2.372 - Valid Loss 26.345\n",
            "Epoch: 371/500 - Train loss 2.369 - Valid Loss 26.312\n",
            "Epoch: 372/500 - Train loss 2.366 - Valid Loss 26.279\n",
            "Epoch: 373/500 - Train loss 2.364 - Valid Loss 26.247\n",
            "Epoch: 374/500 - Train loss 2.361 - Valid Loss 26.215\n",
            "Epoch: 375/500 - Train loss 2.359 - Valid Loss 26.182\n",
            "Epoch: 376/500 - Train loss 2.356 - Valid Loss 26.150\n",
            "Epoch: 377/500 - Train loss 2.354 - Valid Loss 26.118\n",
            "Epoch: 378/500 - Train loss 2.351 - Valid Loss 26.086\n",
            "Epoch: 379/500 - Train loss 2.348 - Valid Loss 26.054\n",
            "Epoch: 380/500 - Train loss 2.346 - Valid Loss 26.022\n",
            "Epoch: 381/500 - Train loss 2.343 - Valid Loss 25.990\n",
            "Epoch: 382/500 - Train loss 2.341 - Valid Loss 25.958\n",
            "Epoch: 383/500 - Train loss 2.338 - Valid Loss 25.927\n",
            "Epoch: 384/500 - Train loss 2.336 - Valid Loss 25.895\n",
            "Epoch: 385/500 - Train loss 2.333 - Valid Loss 25.864\n",
            "Epoch: 386/500 - Train loss 2.331 - Valid Loss 25.833\n",
            "Epoch: 387/500 - Train loss 2.328 - Valid Loss 25.802\n",
            "Epoch: 388/500 - Train loss 2.326 - Valid Loss 25.770\n",
            "Epoch: 389/500 - Train loss 2.323 - Valid Loss 25.740\n",
            "Epoch: 390/500 - Train loss 2.320 - Valid Loss 25.709\n",
            "Epoch: 391/500 - Train loss 2.318 - Valid Loss 25.678\n",
            "Epoch: 392/500 - Train loss 2.315 - Valid Loss 25.647\n",
            "Epoch: 393/500 - Train loss 2.313 - Valid Loss 25.616\n",
            "Epoch: 394/500 - Train loss 2.310 - Valid Loss 25.586\n",
            "Epoch: 395/500 - Train loss 2.308 - Valid Loss 25.556\n",
            "Epoch: 396/500 - Train loss 2.305 - Valid Loss 25.525\n",
            "Epoch: 397/500 - Train loss 2.303 - Valid Loss 25.495\n",
            "Epoch: 398/500 - Train loss 2.300 - Valid Loss 25.464\n",
            "Epoch: 399/500 - Train loss 2.298 - Valid Loss 25.434\n",
            "Epoch: 400/500 - Train loss 2.295 - Valid Loss 25.404\n",
            "Epoch: 401/500 - Train loss 2.293 - Valid Loss 25.374\n",
            "Epoch: 402/500 - Train loss 2.290 - Valid Loss 25.344\n",
            "Epoch: 403/500 - Train loss 2.288 - Valid Loss 25.315\n",
            "Epoch: 404/500 - Train loss 2.285 - Valid Loss 25.285\n",
            "Epoch: 405/500 - Train loss 2.283 - Valid Loss 25.255\n",
            "Epoch: 406/500 - Train loss 2.280 - Valid Loss 25.226\n",
            "Epoch: 407/500 - Train loss 2.278 - Valid Loss 25.196\n",
            "Epoch: 408/500 - Train loss 2.275 - Valid Loss 25.167\n",
            "Epoch: 409/500 - Train loss 2.273 - Valid Loss 25.137\n",
            "Epoch: 410/500 - Train loss 2.270 - Valid Loss 25.108\n",
            "Epoch: 411/500 - Train loss 2.267 - Valid Loss 25.079\n",
            "Epoch: 412/500 - Train loss 2.265 - Valid Loss 25.050\n",
            "Epoch: 413/500 - Train loss 2.262 - Valid Loss 25.020\n",
            "Epoch: 414/500 - Train loss 2.260 - Valid Loss 24.991\n",
            "Epoch: 415/500 - Train loss 2.257 - Valid Loss 24.962\n",
            "Epoch: 416/500 - Train loss 2.255 - Valid Loss 24.933\n",
            "Epoch: 417/500 - Train loss 2.252 - Valid Loss 24.905\n",
            "Epoch: 418/500 - Train loss 2.250 - Valid Loss 24.876\n",
            "Epoch: 419/500 - Train loss 2.247 - Valid Loss 24.847\n",
            "Epoch: 420/500 - Train loss 2.245 - Valid Loss 24.818\n",
            "Epoch: 421/500 - Train loss 2.242 - Valid Loss 24.790\n",
            "Epoch: 422/500 - Train loss 2.240 - Valid Loss 24.761\n",
            "Epoch: 423/500 - Train loss 2.237 - Valid Loss 24.733\n",
            "Epoch: 424/500 - Train loss 2.235 - Valid Loss 24.704\n",
            "Epoch: 425/500 - Train loss 2.232 - Valid Loss 24.676\n",
            "Epoch: 426/500 - Train loss 2.230 - Valid Loss 24.647\n",
            "Epoch: 427/500 - Train loss 2.227 - Valid Loss 24.619\n",
            "Epoch: 428/500 - Train loss 2.225 - Valid Loss 24.591\n",
            "Epoch: 429/500 - Train loss 2.222 - Valid Loss 24.562\n",
            "Epoch: 430/500 - Train loss 2.219 - Valid Loss 24.534\n",
            "Epoch: 431/500 - Train loss 2.217 - Valid Loss 24.506\n",
            "Epoch: 432/500 - Train loss 2.214 - Valid Loss 24.478\n",
            "Epoch: 433/500 - Train loss 2.212 - Valid Loss 24.450\n",
            "Epoch: 434/500 - Train loss 2.209 - Valid Loss 24.422\n",
            "Epoch: 435/500 - Train loss 2.207 - Valid Loss 24.394\n",
            "Epoch: 436/500 - Train loss 2.204 - Valid Loss 24.366\n",
            "Epoch: 437/500 - Train loss 2.202 - Valid Loss 24.338\n",
            "Epoch: 438/500 - Train loss 2.199 - Valid Loss 24.310\n",
            "Epoch: 439/500 - Train loss 2.197 - Valid Loss 24.282\n",
            "Epoch: 440/500 - Train loss 2.194 - Valid Loss 24.254\n",
            "Epoch: 441/500 - Train loss 2.191 - Valid Loss 24.227\n",
            "Epoch: 442/500 - Train loss 2.189 - Valid Loss 24.199\n",
            "Epoch: 443/500 - Train loss 2.186 - Valid Loss 24.171\n",
            "Epoch: 444/500 - Train loss 2.184 - Valid Loss 24.144\n",
            "Epoch: 445/500 - Train loss 2.181 - Valid Loss 24.116\n",
            "Epoch: 446/500 - Train loss 2.179 - Valid Loss 24.088\n",
            "Epoch: 447/500 - Train loss 2.176 - Valid Loss 24.060\n",
            "Epoch: 448/500 - Train loss 2.173 - Valid Loss 24.033\n",
            "Epoch: 449/500 - Train loss 2.171 - Valid Loss 24.005\n",
            "Epoch: 450/500 - Train loss 2.168 - Valid Loss 23.978\n",
            "Epoch: 451/500 - Train loss 2.166 - Valid Loss 23.950\n",
            "Epoch: 452/500 - Train loss 2.163 - Valid Loss 23.923\n",
            "Epoch: 453/500 - Train loss 2.160 - Valid Loss 23.895\n",
            "Epoch: 454/500 - Train loss 2.158 - Valid Loss 23.867\n",
            "Epoch: 455/500 - Train loss 2.155 - Valid Loss 23.840\n",
            "Epoch: 456/500 - Train loss 2.153 - Valid Loss 23.812\n",
            "Epoch: 457/500 - Train loss 2.150 - Valid Loss 23.785\n",
            "Epoch: 458/500 - Train loss 2.147 - Valid Loss 23.757\n",
            "Epoch: 459/500 - Train loss 2.145 - Valid Loss 23.730\n",
            "Epoch: 460/500 - Train loss 2.142 - Valid Loss 23.702\n",
            "Epoch: 461/500 - Train loss 2.140 - Valid Loss 23.675\n",
            "Epoch: 462/500 - Train loss 2.137 - Valid Loss 23.647\n",
            "Epoch: 463/500 - Train loss 2.134 - Valid Loss 23.620\n",
            "Epoch: 464/500 - Train loss 2.132 - Valid Loss 23.592\n",
            "Epoch: 465/500 - Train loss 2.129 - Valid Loss 23.564\n",
            "Epoch: 466/500 - Train loss 2.126 - Valid Loss 23.537\n",
            "Epoch: 467/500 - Train loss 2.124 - Valid Loss 23.509\n",
            "Epoch: 468/500 - Train loss 2.121 - Valid Loss 23.481\n",
            "Epoch: 469/500 - Train loss 2.118 - Valid Loss 23.454\n",
            "Epoch: 470/500 - Train loss 2.116 - Valid Loss 23.426\n",
            "Epoch: 471/500 - Train loss 2.113 - Valid Loss 23.399\n",
            "Epoch: 472/500 - Train loss 2.110 - Valid Loss 23.371\n",
            "Epoch: 473/500 - Train loss 2.107 - Valid Loss 23.343\n",
            "Epoch: 474/500 - Train loss 2.105 - Valid Loss 23.315\n",
            "Epoch: 475/500 - Train loss 2.102 - Valid Loss 23.288\n",
            "Epoch: 476/500 - Train loss 2.099 - Valid Loss 23.260\n",
            "Epoch: 477/500 - Train loss 2.097 - Valid Loss 23.232\n",
            "Epoch: 478/500 - Train loss 2.094 - Valid Loss 23.204\n",
            "Epoch: 479/500 - Train loss 2.091 - Valid Loss 23.176\n",
            "Epoch: 480/500 - Train loss 2.088 - Valid Loss 23.148\n",
            "Epoch: 481/500 - Train loss 2.086 - Valid Loss 23.120\n",
            "Epoch: 482/500 - Train loss 2.083 - Valid Loss 23.092\n",
            "Epoch: 483/500 - Train loss 2.080 - Valid Loss 23.063\n",
            "Epoch: 484/500 - Train loss 2.077 - Valid Loss 23.035\n",
            "Epoch: 485/500 - Train loss 2.074 - Valid Loss 23.007\n",
            "Epoch: 486/500 - Train loss 2.072 - Valid Loss 22.979\n",
            "Epoch: 487/500 - Train loss 2.069 - Valid Loss 22.950\n",
            "Epoch: 488/500 - Train loss 2.066 - Valid Loss 22.922\n",
            "Epoch: 489/500 - Train loss 2.063 - Valid Loss 22.893\n",
            "Epoch: 490/500 - Train loss 2.060 - Valid Loss 22.864\n",
            "Epoch: 491/500 - Train loss 2.057 - Valid Loss 22.835\n",
            "Epoch: 492/500 - Train loss 2.054 - Valid Loss 22.806\n",
            "Epoch: 493/500 - Train loss 2.052 - Valid Loss 22.778\n",
            "Epoch: 494/500 - Train loss 2.049 - Valid Loss 22.748\n",
            "Epoch: 495/500 - Train loss 2.046 - Valid Loss 22.719\n",
            "Epoch: 496/500 - Train loss 2.043 - Valid Loss 22.690\n",
            "Epoch: 497/500 - Train loss 2.040 - Valid Loss 22.661\n",
            "Epoch: 498/500 - Train loss 2.037 - Valid Loss 22.631\n",
            "Epoch: 499/500 - Train loss 2.034 - Valid Loss 22.602\n",
            "Epoch: 500/500 - Train loss 2.031 - Valid Loss 22.572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history2['loss']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history2['loss'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history2['val_loss'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyYR9z-qgdMq",
        "outputId": "178a36e1-ed14-4bda-e48d-e381384c9d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensayo\n",
        "# x = 30\n",
        "# y_test = x * 15\n",
        "\n",
        "x_test = 30\n",
        "y_test = x_test * 15\n",
        "test_input = np.array([x_test])\n",
        "test_input = test_input.reshape((1, seq_length, input_size))\n",
        "test_input = torch.from_numpy(test_input.astype(np.float32))\n",
        "\n",
        "test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1)\n",
        "\n",
        "y_hat = model2(test_input)\n",
        "\n",
        "print(\"y_test:\", y_test)\n",
        "print(\"y_hat:\", y_hat)\n",
        "\n",
        "loss = model2_criterion(y_hat, test_target).item()\n",
        "print(\"loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Z_hJ6Ig4Of",
        "outputId": "4b0c027b-d94f-41bf-a236-ad8e7c684b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test: 450\n",
            "y_hat: tensor([[434.6496]], grad_fn=<AddmmBackward0>)\n",
            "loss: 235.63580322265625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEI5TjSFWeY8"
      },
      "source": [
        "Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos más parámetros que la RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT8b9EfGyshD"
      },
      "source": [
        "### 3 - Multi-layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_helpers import CustomLSTM\n",
        "\n",
        "# En esta oportunidad se utilizarán dos layer LSTM\n",
        "class Model3(nn.Module):\n",
        "    def __init__(self, input_size, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer\n",
        "        self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer\n",
        "        self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        lstm_output, _ = self.lstm1(x)\n",
        "        lstm_output, _ = self.lstm2(lstm_output)\n",
        "        out = self.fc(lstm_output[:,-1,:]) # take last output (last seq)\n",
        "        return out\n",
        "\n",
        "model3 = Model3(input_size=input_size, output_dim=output_dim)\n",
        "\n",
        "# Crear el optimizador la una función de error\n",
        "model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01)\n",
        "model3_criterion = nn.MSELoss()  # mean squared error\n",
        "\n",
        "summary(model3, input_size=(1, seq_length, input_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xVfTMScdFqR",
        "outputId": "ab320db5-0817-4250-b6fe-ff8fc7df0b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model3                                   [1, 1]                    --\n",
              "├─CustomLSTM: 1-1                        [1, 1, 64]                16,896\n",
              "├─CustomLSTM: 1                          --                        --\n",
              "│    └─Sigmoid: 2-1                      [1, 64]                   --\n",
              "│    └─Sigmoid: 2-2                      [1, 64]                   --\n",
              "├─CustomLSTM: 1                          --                        --\n",
              "│    └─ReLU: 2-3                         [1, 64]                   --\n",
              "├─CustomLSTM: 1                          --                        --\n",
              "│    └─Sigmoid: 2-4                      [1, 64]                   --\n",
              "├─CustomLSTM: 1                          --                        --\n",
              "│    └─ReLU: 2-5                         [1, 64]                   --\n",
              "├─CustomLSTM: 1-2                        [1, 1, 64]                33,024\n",
              "│    └─Sigmoid: 2-6                      [1, 64]                   --\n",
              "│    └─Sigmoid: 2-7                      [1, 64]                   --\n",
              "│    └─ReLU: 2-8                         [1, 64]                   --\n",
              "│    └─Sigmoid: 2-9                      [1, 64]                   --\n",
              "│    └─ReLU: 2-10                        [1, 64]                   --\n",
              "├─Linear: 1-3                            [1, 1]                    65\n",
              "==========================================================================================\n",
              "Total params: 49,985\n",
              "Trainable params: 49,985\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history3 = train(model3,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                model3_optimizer,\n",
        "                model3_criterion,\n",
        "                epochs=500\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtoKCzXrei60",
        "outputId": "d968f5bb-d5ae-4ccb-af64-8d0d289865c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/500 - Train loss 21060.648 - Valid Loss 77298.312\n",
            "Epoch: 2/500 - Train loss 21046.730 - Valid Loss 77250.477\n",
            "Epoch: 3/500 - Train loss 21032.021 - Valid Loss 77168.922\n",
            "Epoch: 4/500 - Train loss 21010.311 - Valid Loss 77025.742\n",
            "Epoch: 5/500 - Train loss 20976.109 - Valid Loss 76770.383\n",
            "Epoch: 6/500 - Train loss 20920.223 - Valid Loss 76333.898\n",
            "Epoch: 7/500 - Train loss 20830.010 - Valid Loss 75630.602\n",
            "Epoch: 8/500 - Train loss 20688.035 - Valid Loss 74578.234\n",
            "Epoch: 9/500 - Train loss 20472.715 - Valid Loss 73123.422\n",
            "Epoch: 10/500 - Train loss 20161.223 - Valid Loss 71247.359\n",
            "Epoch: 11/500 - Train loss 19733.873 - Valid Loss 68939.000\n",
            "Epoch: 12/500 - Train loss 19177.111 - Valid Loss 66161.914\n",
            "Epoch: 13/500 - Train loss 18481.977 - Valid Loss 62845.211\n",
            "Epoch: 14/500 - Train loss 17639.088 - Valid Loss 58900.336\n",
            "Epoch: 15/500 - Train loss 16639.480 - Valid Loss 54279.516\n",
            "Epoch: 16/500 - Train loss 15471.961 - Valid Loss 48960.660\n",
            "Epoch: 17/500 - Train loss 14127.237 - Valid Loss 42968.281\n",
            "Epoch: 18/500 - Train loss 12603.189 - Valid Loss 36391.781\n",
            "Epoch: 19/500 - Train loss 10910.635 - Valid Loss 29394.633\n",
            "Epoch: 20/500 - Train loss 9077.312 - Valid Loss 22227.477\n",
            "Epoch: 21/500 - Train loss 7153.472 - Valid Loss 15244.207\n",
            "Epoch: 22/500 - Train loss 5217.515 - Valid Loss 8915.375\n",
            "Epoch: 23/500 - Train loss 3381.644 - Valid Loss 3832.784\n",
            "Epoch: 24/500 - Train loss 1795.471 - Valid Loss 686.404\n",
            "Epoch: 25/500 - Train loss 643.045 - Valid Loss 147.206\n",
            "Epoch: 26/500 - Train loss 117.157 - Valid Loss 2543.587\n",
            "Epoch: 27/500 - Train loss 340.068 - Valid Loss 7107.216\n",
            "Epoch: 28/500 - Train loss 1183.024 - Valid Loss 11791.682\n",
            "Epoch: 29/500 - Train loss 2180.765 - Valid Loss 14517.024\n",
            "Epoch: 30/500 - Train loss 2811.366 - Valid Loss 14559.043\n",
            "Epoch: 31/500 - Train loss 2861.127 - Valid Loss 12468.320\n",
            "Epoch: 32/500 - Train loss 2432.352 - Valid Loss 9283.969\n",
            "Epoch: 33/500 - Train loss 1760.585 - Valid Loss 5974.452\n",
            "Epoch: 34/500 - Train loss 1073.146 - Valid Loss 3206.052\n",
            "Epoch: 35/500 - Train loss 525.974 - Valid Loss 1299.333\n",
            "Epoch: 36/500 - Train loss 190.166 - Valid Loss 284.314\n",
            "Epoch: 37/500 - Train loss 65.552 - Valid Loss 0.791\n",
            "Epoch: 38/500 - Train loss 106.382 - Valid Loss 203.501\n",
            "Epoch: 39/500 - Train loss 247.994 - Valid Loss 644.344\n",
            "Epoch: 40/500 - Train loss 427.183 - Valid Loss 1120.793\n",
            "Epoch: 41/500 - Train loss 593.732 - Valid Loss 1494.391\n",
            "Epoch: 42/500 - Train loss 714.408 - Valid Loss 1690.008\n",
            "Epoch: 43/500 - Train loss 772.307 - Valid Loss 1686.107\n",
            "Epoch: 44/500 - Train loss 764.093 - Valid Loss 1502.496\n",
            "Epoch: 45/500 - Train loss 696.862 - Valid Loss 1188.376\n",
            "Epoch: 46/500 - Train loss 585.014 - Valid Loss 811.190\n",
            "Epoch: 47/500 - Train loss 447.133 - Valid Loss 439.767\n",
            "Epoch: 48/500 - Train loss 304.438 - Valid Loss 152.186\n",
            "Epoch: 49/500 - Train loss 179.489 - Valid Loss 9.532\n",
            "Epoch: 50/500 - Train loss 89.902 - Valid Loss 41.334\n",
            "Epoch: 51/500 - Train loss 46.193 - Valid Loss 238.458\n",
            "Epoch: 52/500 - Train loss 49.153 - Valid Loss 548.325\n",
            "Epoch: 53/500 - Train loss 88.476 - Valid Loss 883.574\n",
            "Epoch: 54/500 - Train loss 144.325 - Valid Loss 1154.586\n",
            "Epoch: 55/500 - Train loss 194.761 - Valid Loss 1298.221\n",
            "Epoch: 56/500 - Train loss 223.435 - Valid Loss 1290.978\n",
            "Epoch: 57/500 - Train loss 223.382 - Valid Loss 1149.254\n",
            "Epoch: 58/500 - Train loss 197.302 - Valid Loss 918.165\n",
            "Epoch: 59/500 - Train loss 154.867 - Valid Loss 654.087\n",
            "Epoch: 60/500 - Train loss 108.433 - Valid Loss 407.827\n",
            "Epoch: 61/500 - Train loss 68.909 - Valid Loss 213.545\n",
            "Epoch: 62/500 - Train loss 43.089 - Valid Loss 85.297\n",
            "Epoch: 63/500 - Train loss 32.867 - Valid Loss 20.315\n",
            "Epoch: 64/500 - Train loss 36.032 - Valid Loss 0.361\n",
            "Epoch: 65/500 - Train loss 47.439 - Valid Loss 7.114\n",
            "Epoch: 66/500 - Train loss 61.552 - Valid Loss 23.124\n",
            "Epoch: 67/500 - Train loss 73.625 - Valid Loss 35.514\n",
            "Epoch: 68/500 - Train loss 80.389 - Valid Loss 37.760\n",
            "Epoch: 69/500 - Train loss 80.441 - Valid Loss 29.658\n",
            "Epoch: 70/500 - Train loss 74.222 - Valid Loss 15.883\n",
            "Epoch: 71/500 - Train loss 63.575 - Valid Loss 3.706\n",
            "Epoch: 72/500 - Train loss 51.129 - Valid Loss 0.429\n",
            "Epoch: 73/500 - Train loss 39.594 - Valid Loss 11.078\n",
            "Epoch: 74/500 - Train loss 31.112 - Valid Loss 36.865\n",
            "Epoch: 75/500 - Train loss 26.803 - Valid Loss 74.755\n",
            "Epoch: 76/500 - Train loss 26.610 - Valid Loss 118.232\n",
            "Epoch: 77/500 - Train loss 29.441 - Valid Loss 159.038\n",
            "Epoch: 78/500 - Train loss 33.588 - Valid Loss 189.374\n",
            "Epoch: 79/500 - Train loss 37.273 - Valid Loss 203.872\n",
            "Epoch: 80/500 - Train loss 39.153 - Valid Loss 200.787\n",
            "Epoch: 81/500 - Train loss 38.644 - Valid Loss 182.055\n",
            "Epoch: 82/500 - Train loss 35.966 - Valid Loss 152.389\n",
            "Epoch: 83/500 - Train loss 31.944 - Valid Loss 117.755\n",
            "Epoch: 84/500 - Train loss 27.660 - Valid Loss 83.806\n",
            "Epoch: 85/500 - Train loss 24.093 - Valid Loss 54.707\n",
            "Epoch: 86/500 - Train loss 21.852 - Valid Loss 32.601\n",
            "Epoch: 87/500 - Train loss 21.071 - Valid Loss 17.705\n",
            "Epoch: 88/500 - Train loss 21.465 - Valid Loss 8.870\n",
            "Epoch: 89/500 - Train loss 22.486 - Valid Loss 4.329\n",
            "Epoch: 90/500 - Train loss 23.525 - Valid Loss 2.382\n",
            "Epoch: 91/500 - Train loss 24.093 - Valid Loss 1.866\n",
            "Epoch: 92/500 - Train loss 23.930 - Valid Loss 2.318\n",
            "Epoch: 93/500 - Train loss 23.036 - Valid Loss 3.866\n",
            "Epoch: 94/500 - Train loss 21.626 - Valid Loss 6.929\n",
            "Epoch: 95/500 - Train loss 20.033 - Valid Loss 11.863\n",
            "Epoch: 96/500 - Train loss 18.593 - Valid Loss 18.667\n",
            "Epoch: 97/500 - Train loss 17.551 - Valid Loss 26.830\n",
            "Epoch: 98/500 - Train loss 16.998 - Valid Loss 35.390\n",
            "Epoch: 99/500 - Train loss 16.873 - Valid Loss 43.125\n",
            "Epoch: 100/500 - Train loss 17.003 - Valid Loss 48.864\n",
            "Epoch: 101/500 - Train loss 17.171 - Valid Loss 51.761\n",
            "Epoch: 102/500 - Train loss 17.191 - Valid Loss 51.495\n",
            "Epoch: 103/500 - Train loss 16.958 - Valid Loss 48.302\n",
            "Epoch: 104/500 - Train loss 16.470 - Valid Loss 42.878\n",
            "Epoch: 105/500 - Train loss 15.809 - Valid Loss 36.171\n",
            "Epoch: 106/500 - Train loss 15.099 - Valid Loss 29.155\n",
            "Epoch: 107/500 - Train loss 14.462 - Valid Loss 22.629\n",
            "Epoch: 108/500 - Train loss 13.979 - Valid Loss 17.119\n",
            "Epoch: 109/500 - Train loss 13.671 - Valid Loss 12.853\n",
            "Epoch: 110/500 - Train loss 13.501 - Valid Loss 9.821\n",
            "Epoch: 111/500 - Train loss 13.402 - Valid Loss 7.876\n",
            "Epoch: 112/500 - Train loss 13.296 - Valid Loss 6.828\n",
            "Epoch: 113/500 - Train loss 13.127 - Valid Loss 6.511\n",
            "Epoch: 114/500 - Train loss 12.868 - Valid Loss 6.809\n",
            "Epoch: 115/500 - Train loss 12.532 - Valid Loss 7.642\n",
            "Epoch: 116/500 - Train loss 12.153 - Valid Loss 8.930\n",
            "Epoch: 117/500 - Train loss 11.777 - Valid Loss 10.560\n",
            "Epoch: 118/500 - Train loss 11.443 - Valid Loss 12.362\n",
            "Epoch: 119/500 - Train loss 11.172 - Valid Loss 14.117\n",
            "Epoch: 120/500 - Train loss 10.960 - Valid Loss 15.590\n",
            "Epoch: 121/500 - Train loss 10.790 - Valid Loss 16.571\n",
            "Epoch: 122/500 - Train loss 10.633 - Valid Loss 16.924\n",
            "Epoch: 123/500 - Train loss 10.464 - Valid Loss 16.605\n",
            "Epoch: 124/500 - Train loss 10.268 - Valid Loss 15.676\n",
            "Epoch: 125/500 - Train loss 10.043 - Valid Loss 14.278\n",
            "Epoch: 126/500 - Train loss 9.799 - Valid Loss 12.602\n",
            "Epoch: 127/500 - Train loss 9.553 - Valid Loss 10.843\n",
            "Epoch: 128/500 - Train loss 9.321 - Valid Loss 9.169\n",
            "Epoch: 129/500 - Train loss 9.112 - Valid Loss 7.700\n",
            "Epoch: 130/500 - Train loss 8.928 - Valid Loss 6.504\n",
            "Epoch: 131/500 - Train loss 8.764 - Valid Loss 5.600\n",
            "Epoch: 132/500 - Train loss 8.608 - Valid Loss 4.979\n",
            "Epoch: 133/500 - Train loss 8.451 - Valid Loss 4.611\n",
            "Epoch: 134/500 - Train loss 8.288 - Valid Loss 4.460\n",
            "Epoch: 135/500 - Train loss 8.115 - Valid Loss 4.487\n",
            "Epoch: 136/500 - Train loss 7.937 - Valid Loss 4.651\n",
            "Epoch: 137/500 - Train loss 7.759 - Valid Loss 4.904\n",
            "Epoch: 138/500 - Train loss 7.588 - Valid Loss 5.196\n",
            "Epoch: 139/500 - Train loss 7.426 - Valid Loss 5.470\n",
            "Epoch: 140/500 - Train loss 7.276 - Valid Loss 5.675\n",
            "Epoch: 141/500 - Train loss 7.135 - Valid Loss 5.767\n",
            "Epoch: 142/500 - Train loss 6.998 - Valid Loss 5.719\n",
            "Epoch: 143/500 - Train loss 6.864 - Valid Loss 5.525\n",
            "Epoch: 144/500 - Train loss 6.728 - Valid Loss 5.201\n",
            "Epoch: 145/500 - Train loss 6.590 - Valid Loss 4.778\n",
            "Epoch: 146/500 - Train loss 6.452 - Valid Loss 4.296\n",
            "Epoch: 147/500 - Train loss 6.317 - Valid Loss 3.799\n",
            "Epoch: 148/500 - Train loss 6.185 - Valid Loss 3.324\n",
            "Epoch: 149/500 - Train loss 6.059 - Valid Loss 2.899\n",
            "Epoch: 150/500 - Train loss 5.938 - Valid Loss 2.541\n",
            "Epoch: 151/500 - Train loss 5.822 - Valid Loss 2.257\n",
            "Epoch: 152/500 - Train loss 5.708 - Valid Loss 2.044\n",
            "Epoch: 153/500 - Train loss 5.597 - Valid Loss 1.896\n",
            "Epoch: 154/500 - Train loss 5.486 - Valid Loss 1.803\n",
            "Epoch: 155/500 - Train loss 5.377 - Valid Loss 1.752\n",
            "Epoch: 156/500 - Train loss 5.268 - Valid Loss 1.730\n",
            "Epoch: 157/500 - Train loss 5.162 - Valid Loss 1.724\n",
            "Epoch: 158/500 - Train loss 5.059 - Valid Loss 1.721\n",
            "Epoch: 159/500 - Train loss 4.959 - Valid Loss 1.707\n",
            "Epoch: 160/500 - Train loss 4.863 - Valid Loss 1.674\n",
            "Epoch: 161/500 - Train loss 4.769 - Valid Loss 1.615\n",
            "Epoch: 162/500 - Train loss 4.677 - Valid Loss 1.530\n",
            "Epoch: 163/500 - Train loss 4.587 - Valid Loss 1.420\n",
            "Epoch: 164/500 - Train loss 4.498 - Valid Loss 1.292\n",
            "Epoch: 165/500 - Train loss 4.411 - Valid Loss 1.155\n",
            "Epoch: 166/500 - Train loss 4.326 - Valid Loss 1.017\n",
            "Epoch: 167/500 - Train loss 4.243 - Valid Loss 0.886\n",
            "Epoch: 168/500 - Train loss 4.161 - Valid Loss 0.769\n",
            "Epoch: 169/500 - Train loss 4.083 - Valid Loss 0.668\n",
            "Epoch: 170/500 - Train loss 4.006 - Valid Loss 0.585\n",
            "Epoch: 171/500 - Train loss 3.931 - Valid Loss 0.519\n",
            "Epoch: 172/500 - Train loss 3.858 - Valid Loss 0.468\n",
            "Epoch: 173/500 - Train loss 3.786 - Valid Loss 0.429\n",
            "Epoch: 174/500 - Train loss 3.716 - Valid Loss 0.400\n",
            "Epoch: 175/500 - Train loss 3.647 - Valid Loss 0.377\n",
            "Epoch: 176/500 - Train loss 3.580 - Valid Loss 0.357\n",
            "Epoch: 177/500 - Train loss 3.515 - Valid Loss 0.339\n",
            "Epoch: 178/500 - Train loss 3.451 - Valid Loss 0.321\n",
            "Epoch: 179/500 - Train loss 3.389 - Valid Loss 0.301\n",
            "Epoch: 180/500 - Train loss 3.329 - Valid Loss 0.280\n",
            "Epoch: 181/500 - Train loss 3.270 - Valid Loss 0.257\n",
            "Epoch: 182/500 - Train loss 3.212 - Valid Loss 0.234\n",
            "Epoch: 183/500 - Train loss 3.156 - Valid Loss 0.213\n",
            "Epoch: 184/500 - Train loss 3.100 - Valid Loss 0.193\n",
            "Epoch: 185/500 - Train loss 3.047 - Valid Loss 0.178\n",
            "Epoch: 186/500 - Train loss 2.994 - Valid Loss 0.167\n",
            "Epoch: 187/500 - Train loss 2.943 - Valid Loss 0.161\n",
            "Epoch: 188/500 - Train loss 2.893 - Valid Loss 0.159\n",
            "Epoch: 189/500 - Train loss 2.845 - Valid Loss 0.160\n",
            "Epoch: 190/500 - Train loss 2.797 - Valid Loss 0.165\n",
            "Epoch: 191/500 - Train loss 2.751 - Valid Loss 0.171\n",
            "Epoch: 192/500 - Train loss 2.706 - Valid Loss 0.179\n",
            "Epoch: 193/500 - Train loss 2.662 - Valid Loss 0.188\n",
            "Epoch: 194/500 - Train loss 2.619 - Valid Loss 0.198\n",
            "Epoch: 195/500 - Train loss 2.577 - Valid Loss 0.210\n",
            "Epoch: 196/500 - Train loss 2.537 - Valid Loss 0.222\n",
            "Epoch: 197/500 - Train loss 2.497 - Valid Loss 0.236\n",
            "Epoch: 198/500 - Train loss 2.458 - Valid Loss 0.253\n",
            "Epoch: 199/500 - Train loss 2.420 - Valid Loss 0.272\n",
            "Epoch: 200/500 - Train loss 2.384 - Valid Loss 0.293\n",
            "Epoch: 201/500 - Train loss 2.348 - Valid Loss 0.318\n",
            "Epoch: 202/500 - Train loss 2.313 - Valid Loss 0.345\n",
            "Epoch: 203/500 - Train loss 2.279 - Valid Loss 0.375\n",
            "Epoch: 204/500 - Train loss 2.246 - Valid Loss 0.406\n",
            "Epoch: 205/500 - Train loss 2.213 - Valid Loss 0.440\n",
            "Epoch: 206/500 - Train loss 2.182 - Valid Loss 0.474\n",
            "Epoch: 207/500 - Train loss 2.151 - Valid Loss 0.509\n",
            "Epoch: 208/500 - Train loss 2.121 - Valid Loss 0.544\n",
            "Epoch: 209/500 - Train loss 2.092 - Valid Loss 0.578\n",
            "Epoch: 210/500 - Train loss 2.063 - Valid Loss 0.612\n",
            "Epoch: 211/500 - Train loss 2.036 - Valid Loss 0.646\n",
            "Epoch: 212/500 - Train loss 2.009 - Valid Loss 0.680\n",
            "Epoch: 213/500 - Train loss 1.983 - Valid Loss 0.714\n",
            "Epoch: 214/500 - Train loss 1.957 - Valid Loss 0.749\n",
            "Epoch: 215/500 - Train loss 1.932 - Valid Loss 0.786\n",
            "Epoch: 216/500 - Train loss 1.908 - Valid Loss 0.825\n",
            "Epoch: 217/500 - Train loss 1.884 - Valid Loss 0.865\n",
            "Epoch: 218/500 - Train loss 1.861 - Valid Loss 0.907\n",
            "Epoch: 219/500 - Train loss 1.839 - Valid Loss 0.952\n",
            "Epoch: 220/500 - Train loss 1.817 - Valid Loss 0.997\n",
            "Epoch: 221/500 - Train loss 1.796 - Valid Loss 1.044\n",
            "Epoch: 222/500 - Train loss 1.775 - Valid Loss 1.092\n",
            "Epoch: 223/500 - Train loss 1.755 - Valid Loss 1.140\n",
            "Epoch: 224/500 - Train loss 1.735 - Valid Loss 1.188\n",
            "Epoch: 225/500 - Train loss 1.716 - Valid Loss 1.235\n",
            "Epoch: 226/500 - Train loss 1.698 - Valid Loss 1.282\n",
            "Epoch: 227/500 - Train loss 1.679 - Valid Loss 1.328\n",
            "Epoch: 228/500 - Train loss 1.662 - Valid Loss 1.373\n",
            "Epoch: 229/500 - Train loss 1.645 - Valid Loss 1.418\n",
            "Epoch: 230/500 - Train loss 1.628 - Valid Loss 1.463\n",
            "Epoch: 231/500 - Train loss 1.612 - Valid Loss 1.508\n",
            "Epoch: 232/500 - Train loss 1.596 - Valid Loss 1.554\n",
            "Epoch: 233/500 - Train loss 1.580 - Valid Loss 1.601\n",
            "Epoch: 234/500 - Train loss 1.565 - Valid Loss 1.648\n",
            "Epoch: 235/500 - Train loss 1.551 - Valid Loss 1.696\n",
            "Epoch: 236/500 - Train loss 1.537 - Valid Loss 1.746\n",
            "Epoch: 237/500 - Train loss 1.523 - Valid Loss 1.796\n",
            "Epoch: 238/500 - Train loss 1.509 - Valid Loss 1.846\n",
            "Epoch: 239/500 - Train loss 1.496 - Valid Loss 1.897\n",
            "Epoch: 240/500 - Train loss 1.483 - Valid Loss 1.947\n",
            "Epoch: 241/500 - Train loss 1.471 - Valid Loss 1.997\n",
            "Epoch: 242/500 - Train loss 1.459 - Valid Loss 2.047\n",
            "Epoch: 243/500 - Train loss 1.447 - Valid Loss 2.096\n",
            "Epoch: 244/500 - Train loss 1.435 - Valid Loss 2.144\n",
            "Epoch: 245/500 - Train loss 1.424 - Valid Loss 2.192\n",
            "Epoch: 246/500 - Train loss 1.413 - Valid Loss 2.239\n",
            "Epoch: 247/500 - Train loss 1.403 - Valid Loss 2.286\n",
            "Epoch: 248/500 - Train loss 1.392 - Valid Loss 2.332\n",
            "Epoch: 249/500 - Train loss 1.382 - Valid Loss 2.379\n",
            "Epoch: 250/500 - Train loss 1.373 - Valid Loss 2.426\n",
            "Epoch: 251/500 - Train loss 1.363 - Valid Loss 2.473\n",
            "Epoch: 252/500 - Train loss 1.354 - Valid Loss 2.521\n",
            "Epoch: 253/500 - Train loss 1.345 - Valid Loss 2.569\n",
            "Epoch: 254/500 - Train loss 1.336 - Valid Loss 2.616\n",
            "Epoch: 255/500 - Train loss 1.327 - Valid Loss 2.664\n",
            "Epoch: 256/500 - Train loss 1.319 - Valid Loss 2.712\n",
            "Epoch: 257/500 - Train loss 1.311 - Valid Loss 2.759\n",
            "Epoch: 258/500 - Train loss 1.303 - Valid Loss 2.806\n",
            "Epoch: 259/500 - Train loss 1.295 - Valid Loss 2.852\n",
            "Epoch: 260/500 - Train loss 1.287 - Valid Loss 2.897\n",
            "Epoch: 261/500 - Train loss 1.280 - Valid Loss 2.942\n",
            "Epoch: 262/500 - Train loss 1.273 - Valid Loss 2.987\n",
            "Epoch: 263/500 - Train loss 1.266 - Valid Loss 3.031\n",
            "Epoch: 264/500 - Train loss 1.259 - Valid Loss 3.074\n",
            "Epoch: 265/500 - Train loss 1.252 - Valid Loss 3.118\n",
            "Epoch: 266/500 - Train loss 1.246 - Valid Loss 3.161\n",
            "Epoch: 267/500 - Train loss 1.240 - Valid Loss 3.204\n",
            "Epoch: 268/500 - Train loss 1.234 - Valid Loss 3.247\n",
            "Epoch: 269/500 - Train loss 1.227 - Valid Loss 3.289\n",
            "Epoch: 270/500 - Train loss 1.222 - Valid Loss 3.332\n",
            "Epoch: 271/500 - Train loss 1.216 - Valid Loss 3.374\n",
            "Epoch: 272/500 - Train loss 1.210 - Valid Loss 3.416\n",
            "Epoch: 273/500 - Train loss 1.205 - Valid Loss 3.458\n",
            "Epoch: 274/500 - Train loss 1.199 - Valid Loss 3.499\n",
            "Epoch: 275/500 - Train loss 1.194 - Valid Loss 3.540\n",
            "Epoch: 276/500 - Train loss 1.189 - Valid Loss 3.580\n",
            "Epoch: 277/500 - Train loss 1.184 - Valid Loss 3.620\n",
            "Epoch: 278/500 - Train loss 1.179 - Valid Loss 3.659\n",
            "Epoch: 279/500 - Train loss 1.175 - Valid Loss 3.697\n",
            "Epoch: 280/500 - Train loss 1.170 - Valid Loss 3.736\n",
            "Epoch: 281/500 - Train loss 1.165 - Valid Loss 3.773\n",
            "Epoch: 282/500 - Train loss 1.161 - Valid Loss 3.811\n",
            "Epoch: 283/500 - Train loss 1.157 - Valid Loss 3.848\n",
            "Epoch: 284/500 - Train loss 1.152 - Valid Loss 3.885\n",
            "Epoch: 285/500 - Train loss 1.148 - Valid Loss 3.921\n",
            "Epoch: 286/500 - Train loss 1.144 - Valid Loss 3.958\n",
            "Epoch: 287/500 - Train loss 1.140 - Valid Loss 3.993\n",
            "Epoch: 288/500 - Train loss 1.136 - Valid Loss 4.029\n",
            "Epoch: 289/500 - Train loss 1.132 - Valid Loss 4.064\n",
            "Epoch: 290/500 - Train loss 1.129 - Valid Loss 4.099\n",
            "Epoch: 291/500 - Train loss 1.125 - Valid Loss 4.133\n",
            "Epoch: 292/500 - Train loss 1.121 - Valid Loss 4.167\n",
            "Epoch: 293/500 - Train loss 1.118 - Valid Loss 4.201\n",
            "Epoch: 294/500 - Train loss 1.114 - Valid Loss 4.233\n",
            "Epoch: 295/500 - Train loss 1.111 - Valid Loss 4.266\n",
            "Epoch: 296/500 - Train loss 1.108 - Valid Loss 4.298\n",
            "Epoch: 297/500 - Train loss 1.104 - Valid Loss 4.329\n",
            "Epoch: 298/500 - Train loss 1.101 - Valid Loss 4.361\n",
            "Epoch: 299/500 - Train loss 1.098 - Valid Loss 4.392\n",
            "Epoch: 300/500 - Train loss 1.095 - Valid Loss 4.422\n",
            "Epoch: 301/500 - Train loss 1.092 - Valid Loss 4.452\n",
            "Epoch: 302/500 - Train loss 1.089 - Valid Loss 4.482\n",
            "Epoch: 303/500 - Train loss 1.086 - Valid Loss 4.512\n",
            "Epoch: 304/500 - Train loss 1.083 - Valid Loss 4.541\n",
            "Epoch: 305/500 - Train loss 1.080 - Valid Loss 4.569\n",
            "Epoch: 306/500 - Train loss 1.077 - Valid Loss 4.598\n",
            "Epoch: 307/500 - Train loss 1.075 - Valid Loss 4.626\n",
            "Epoch: 308/500 - Train loss 1.072 - Valid Loss 4.653\n",
            "Epoch: 309/500 - Train loss 1.069 - Valid Loss 4.680\n",
            "Epoch: 310/500 - Train loss 1.067 - Valid Loss 4.707\n",
            "Epoch: 311/500 - Train loss 1.064 - Valid Loss 4.733\n",
            "Epoch: 312/500 - Train loss 1.062 - Valid Loss 4.759\n",
            "Epoch: 313/500 - Train loss 1.059 - Valid Loss 4.785\n",
            "Epoch: 314/500 - Train loss 1.057 - Valid Loss 4.810\n",
            "Epoch: 315/500 - Train loss 1.054 - Valid Loss 4.835\n",
            "Epoch: 316/500 - Train loss 1.052 - Valid Loss 4.859\n",
            "Epoch: 317/500 - Train loss 1.049 - Valid Loss 4.884\n",
            "Epoch: 318/500 - Train loss 1.047 - Valid Loss 4.908\n",
            "Epoch: 319/500 - Train loss 1.045 - Valid Loss 4.931\n",
            "Epoch: 320/500 - Train loss 1.042 - Valid Loss 4.954\n",
            "Epoch: 321/500 - Train loss 1.040 - Valid Loss 4.977\n",
            "Epoch: 322/500 - Train loss 1.038 - Valid Loss 5.000\n",
            "Epoch: 323/500 - Train loss 1.035 - Valid Loss 5.022\n",
            "Epoch: 324/500 - Train loss 1.033 - Valid Loss 5.044\n",
            "Epoch: 325/500 - Train loss 1.031 - Valid Loss 5.065\n",
            "Epoch: 326/500 - Train loss 1.029 - Valid Loss 5.086\n",
            "Epoch: 327/500 - Train loss 1.027 - Valid Loss 5.107\n",
            "Epoch: 328/500 - Train loss 1.025 - Valid Loss 5.128\n",
            "Epoch: 329/500 - Train loss 1.023 - Valid Loss 5.148\n",
            "Epoch: 330/500 - Train loss 1.020 - Valid Loss 5.168\n",
            "Epoch: 331/500 - Train loss 1.018 - Valid Loss 5.187\n",
            "Epoch: 332/500 - Train loss 1.016 - Valid Loss 5.206\n",
            "Epoch: 333/500 - Train loss 1.014 - Valid Loss 5.225\n",
            "Epoch: 334/500 - Train loss 1.012 - Valid Loss 5.244\n",
            "Epoch: 335/500 - Train loss 1.010 - Valid Loss 5.262\n",
            "Epoch: 336/500 - Train loss 1.008 - Valid Loss 5.280\n",
            "Epoch: 337/500 - Train loss 1.006 - Valid Loss 5.298\n",
            "Epoch: 338/500 - Train loss 1.004 - Valid Loss 5.316\n",
            "Epoch: 339/500 - Train loss 1.002 - Valid Loss 5.333\n",
            "Epoch: 340/500 - Train loss 1.000 - Valid Loss 5.350\n",
            "Epoch: 341/500 - Train loss 0.999 - Valid Loss 5.366\n",
            "Epoch: 342/500 - Train loss 0.997 - Valid Loss 5.383\n",
            "Epoch: 343/500 - Train loss 0.995 - Valid Loss 5.399\n",
            "Epoch: 344/500 - Train loss 0.993 - Valid Loss 5.415\n",
            "Epoch: 345/500 - Train loss 0.991 - Valid Loss 5.430\n",
            "Epoch: 346/500 - Train loss 0.989 - Valid Loss 5.446\n",
            "Epoch: 347/500 - Train loss 0.987 - Valid Loss 5.461\n",
            "Epoch: 348/500 - Train loss 0.985 - Valid Loss 5.475\n",
            "Epoch: 349/500 - Train loss 0.984 - Valid Loss 5.490\n",
            "Epoch: 350/500 - Train loss 0.982 - Valid Loss 5.504\n",
            "Epoch: 351/500 - Train loss 0.980 - Valid Loss 5.518\n",
            "Epoch: 352/500 - Train loss 0.978 - Valid Loss 5.532\n",
            "Epoch: 353/500 - Train loss 0.976 - Valid Loss 5.546\n",
            "Epoch: 354/500 - Train loss 0.975 - Valid Loss 5.559\n",
            "Epoch: 355/500 - Train loss 0.973 - Valid Loss 5.572\n",
            "Epoch: 356/500 - Train loss 0.971 - Valid Loss 5.585\n",
            "Epoch: 357/500 - Train loss 0.969 - Valid Loss 5.598\n",
            "Epoch: 358/500 - Train loss 0.967 - Valid Loss 5.610\n",
            "Epoch: 359/500 - Train loss 0.966 - Valid Loss 5.622\n",
            "Epoch: 360/500 - Train loss 0.964 - Valid Loss 5.634\n",
            "Epoch: 361/500 - Train loss 0.962 - Valid Loss 5.646\n",
            "Epoch: 362/500 - Train loss 0.960 - Valid Loss 5.658\n",
            "Epoch: 363/500 - Train loss 0.959 - Valid Loss 5.669\n",
            "Epoch: 364/500 - Train loss 0.957 - Valid Loss 5.680\n",
            "Epoch: 365/500 - Train loss 0.955 - Valid Loss 5.691\n",
            "Epoch: 366/500 - Train loss 0.953 - Valid Loss 5.702\n",
            "Epoch: 367/500 - Train loss 0.952 - Valid Loss 5.713\n",
            "Epoch: 368/500 - Train loss 0.950 - Valid Loss 5.723\n",
            "Epoch: 369/500 - Train loss 0.948 - Valid Loss 5.733\n",
            "Epoch: 370/500 - Train loss 0.947 - Valid Loss 5.743\n",
            "Epoch: 371/500 - Train loss 0.945 - Valid Loss 5.753\n",
            "Epoch: 372/500 - Train loss 0.943 - Valid Loss 5.763\n",
            "Epoch: 373/500 - Train loss 0.941 - Valid Loss 5.772\n",
            "Epoch: 374/500 - Train loss 0.940 - Valid Loss 5.781\n",
            "Epoch: 375/500 - Train loss 0.938 - Valid Loss 5.790\n",
            "Epoch: 376/500 - Train loss 0.936 - Valid Loss 5.799\n",
            "Epoch: 377/500 - Train loss 0.935 - Valid Loss 5.808\n",
            "Epoch: 378/500 - Train loss 0.933 - Valid Loss 5.817\n",
            "Epoch: 379/500 - Train loss 0.931 - Valid Loss 5.825\n",
            "Epoch: 380/500 - Train loss 0.930 - Valid Loss 5.834\n",
            "Epoch: 381/500 - Train loss 0.928 - Valid Loss 5.842\n",
            "Epoch: 382/500 - Train loss 0.926 - Valid Loss 5.850\n",
            "Epoch: 383/500 - Train loss 0.925 - Valid Loss 5.857\n",
            "Epoch: 384/500 - Train loss 0.923 - Valid Loss 5.865\n",
            "Epoch: 385/500 - Train loss 0.921 - Valid Loss 5.873\n",
            "Epoch: 386/500 - Train loss 0.920 - Valid Loss 5.880\n",
            "Epoch: 387/500 - Train loss 0.918 - Valid Loss 5.887\n",
            "Epoch: 388/500 - Train loss 0.916 - Valid Loss 5.894\n",
            "Epoch: 389/500 - Train loss 0.915 - Valid Loss 5.901\n",
            "Epoch: 390/500 - Train loss 0.913 - Valid Loss 5.908\n",
            "Epoch: 391/500 - Train loss 0.911 - Valid Loss 5.915\n",
            "Epoch: 392/500 - Train loss 0.910 - Valid Loss 5.922\n",
            "Epoch: 393/500 - Train loss 0.908 - Valid Loss 5.928\n",
            "Epoch: 394/500 - Train loss 0.906 - Valid Loss 5.934\n",
            "Epoch: 395/500 - Train loss 0.904 - Valid Loss 5.940\n",
            "Epoch: 396/500 - Train loss 0.903 - Valid Loss 5.947\n",
            "Epoch: 397/500 - Train loss 0.901 - Valid Loss 5.952\n",
            "Epoch: 398/500 - Train loss 0.899 - Valid Loss 5.958\n",
            "Epoch: 399/500 - Train loss 0.898 - Valid Loss 5.964\n",
            "Epoch: 400/500 - Train loss 0.896 - Valid Loss 5.970\n",
            "Epoch: 401/500 - Train loss 0.894 - Valid Loss 5.975\n",
            "Epoch: 402/500 - Train loss 0.893 - Valid Loss 5.980\n",
            "Epoch: 403/500 - Train loss 0.891 - Valid Loss 5.986\n",
            "Epoch: 404/500 - Train loss 0.889 - Valid Loss 5.991\n",
            "Epoch: 405/500 - Train loss 0.888 - Valid Loss 5.996\n",
            "Epoch: 406/500 - Train loss 0.886 - Valid Loss 6.001\n",
            "Epoch: 407/500 - Train loss 0.884 - Valid Loss 6.006\n",
            "Epoch: 408/500 - Train loss 0.883 - Valid Loss 6.010\n",
            "Epoch: 409/500 - Train loss 0.881 - Valid Loss 6.015\n",
            "Epoch: 410/500 - Train loss 0.879 - Valid Loss 6.020\n",
            "Epoch: 411/500 - Train loss 0.878 - Valid Loss 6.024\n",
            "Epoch: 412/500 - Train loss 0.876 - Valid Loss 6.028\n",
            "Epoch: 413/500 - Train loss 0.875 - Valid Loss 6.033\n",
            "Epoch: 414/500 - Train loss 0.873 - Valid Loss 6.037\n",
            "Epoch: 415/500 - Train loss 0.871 - Valid Loss 6.041\n",
            "Epoch: 416/500 - Train loss 0.869 - Valid Loss 6.045\n",
            "Epoch: 417/500 - Train loss 0.868 - Valid Loss 6.049\n",
            "Epoch: 418/500 - Train loss 0.866 - Valid Loss 6.053\n",
            "Epoch: 419/500 - Train loss 0.865 - Valid Loss 6.057\n",
            "Epoch: 420/500 - Train loss 0.863 - Valid Loss 6.060\n",
            "Epoch: 421/500 - Train loss 0.861 - Valid Loss 6.064\n",
            "Epoch: 422/500 - Train loss 0.860 - Valid Loss 6.068\n",
            "Epoch: 423/500 - Train loss 0.858 - Valid Loss 6.071\n",
            "Epoch: 424/500 - Train loss 0.856 - Valid Loss 6.074\n",
            "Epoch: 425/500 - Train loss 0.855 - Valid Loss 6.078\n",
            "Epoch: 426/500 - Train loss 0.853 - Valid Loss 6.081\n",
            "Epoch: 427/500 - Train loss 0.851 - Valid Loss 6.084\n",
            "Epoch: 428/500 - Train loss 0.850 - Valid Loss 6.087\n",
            "Epoch: 429/500 - Train loss 0.848 - Valid Loss 6.090\n",
            "Epoch: 430/500 - Train loss 0.846 - Valid Loss 6.093\n",
            "Epoch: 431/500 - Train loss 0.844 - Valid Loss 6.096\n",
            "Epoch: 432/500 - Train loss 0.843 - Valid Loss 6.099\n",
            "Epoch: 433/500 - Train loss 0.841 - Valid Loss 6.102\n",
            "Epoch: 434/500 - Train loss 0.839 - Valid Loss 6.104\n",
            "Epoch: 435/500 - Train loss 0.838 - Valid Loss 6.107\n",
            "Epoch: 436/500 - Train loss 0.836 - Valid Loss 6.110\n",
            "Epoch: 437/500 - Train loss 0.834 - Valid Loss 6.112\n",
            "Epoch: 438/500 - Train loss 0.833 - Valid Loss 6.115\n",
            "Epoch: 439/500 - Train loss 0.831 - Valid Loss 6.117\n",
            "Epoch: 440/500 - Train loss 0.829 - Valid Loss 6.120\n",
            "Epoch: 441/500 - Train loss 0.828 - Valid Loss 6.122\n",
            "Epoch: 442/500 - Train loss 0.826 - Valid Loss 6.124\n",
            "Epoch: 443/500 - Train loss 0.824 - Valid Loss 6.126\n",
            "Epoch: 444/500 - Train loss 0.823 - Valid Loss 6.128\n",
            "Epoch: 445/500 - Train loss 0.821 - Valid Loss 6.131\n",
            "Epoch: 446/500 - Train loss 0.819 - Valid Loss 6.133\n",
            "Epoch: 447/500 - Train loss 0.818 - Valid Loss 6.135\n",
            "Epoch: 448/500 - Train loss 0.816 - Valid Loss 6.136\n",
            "Epoch: 449/500 - Train loss 0.814 - Valid Loss 6.138\n",
            "Epoch: 450/500 - Train loss 0.813 - Valid Loss 6.140\n",
            "Epoch: 451/500 - Train loss 0.811 - Valid Loss 6.142\n",
            "Epoch: 452/500 - Train loss 0.809 - Valid Loss 6.144\n",
            "Epoch: 453/500 - Train loss 0.808 - Valid Loss 6.146\n",
            "Epoch: 454/500 - Train loss 0.806 - Valid Loss 6.147\n",
            "Epoch: 455/500 - Train loss 0.804 - Valid Loss 6.149\n",
            "Epoch: 456/500 - Train loss 0.803 - Valid Loss 6.151\n",
            "Epoch: 457/500 - Train loss 0.801 - Valid Loss 6.152\n",
            "Epoch: 458/500 - Train loss 0.799 - Valid Loss 6.154\n",
            "Epoch: 459/500 - Train loss 0.798 - Valid Loss 6.155\n",
            "Epoch: 460/500 - Train loss 0.796 - Valid Loss 6.157\n",
            "Epoch: 461/500 - Train loss 0.794 - Valid Loss 6.158\n",
            "Epoch: 462/500 - Train loss 0.793 - Valid Loss 6.159\n",
            "Epoch: 463/500 - Train loss 0.791 - Valid Loss 6.161\n",
            "Epoch: 464/500 - Train loss 0.789 - Valid Loss 6.162\n",
            "Epoch: 465/500 - Train loss 0.788 - Valid Loss 6.163\n",
            "Epoch: 466/500 - Train loss 0.786 - Valid Loss 6.165\n",
            "Epoch: 467/500 - Train loss 0.784 - Valid Loss 6.166\n",
            "Epoch: 468/500 - Train loss 0.782 - Valid Loss 6.167\n",
            "Epoch: 469/500 - Train loss 0.781 - Valid Loss 6.168\n",
            "Epoch: 470/500 - Train loss 0.779 - Valid Loss 6.169\n",
            "Epoch: 471/500 - Train loss 0.777 - Valid Loss 6.170\n",
            "Epoch: 472/500 - Train loss 0.776 - Valid Loss 6.171\n",
            "Epoch: 473/500 - Train loss 0.774 - Valid Loss 6.173\n",
            "Epoch: 474/500 - Train loss 0.772 - Valid Loss 6.173\n",
            "Epoch: 475/500 - Train loss 0.771 - Valid Loss 6.174\n",
            "Epoch: 476/500 - Train loss 0.769 - Valid Loss 6.175\n",
            "Epoch: 477/500 - Train loss 0.767 - Valid Loss 6.176\n",
            "Epoch: 478/500 - Train loss 0.766 - Valid Loss 6.177\n",
            "Epoch: 479/500 - Train loss 0.764 - Valid Loss 6.178\n",
            "Epoch: 480/500 - Train loss 0.762 - Valid Loss 6.179\n",
            "Epoch: 481/500 - Train loss 0.761 - Valid Loss 6.179\n",
            "Epoch: 482/500 - Train loss 0.759 - Valid Loss 6.180\n",
            "Epoch: 483/500 - Train loss 0.757 - Valid Loss 6.181\n",
            "Epoch: 484/500 - Train loss 0.756 - Valid Loss 6.182\n",
            "Epoch: 485/500 - Train loss 0.754 - Valid Loss 6.182\n",
            "Epoch: 486/500 - Train loss 0.752 - Valid Loss 6.183\n",
            "Epoch: 487/500 - Train loss 0.751 - Valid Loss 6.184\n",
            "Epoch: 488/500 - Train loss 0.749 - Valid Loss 6.184\n",
            "Epoch: 489/500 - Train loss 0.747 - Valid Loss 6.185\n",
            "Epoch: 490/500 - Train loss 0.745 - Valid Loss 6.186\n",
            "Epoch: 491/500 - Train loss 0.744 - Valid Loss 6.186\n",
            "Epoch: 492/500 - Train loss 0.742 - Valid Loss 6.187\n",
            "Epoch: 493/500 - Train loss 0.740 - Valid Loss 6.187\n",
            "Epoch: 494/500 - Train loss 0.739 - Valid Loss 6.188\n",
            "Epoch: 495/500 - Train loss 0.737 - Valid Loss 6.188\n",
            "Epoch: 496/500 - Train loss 0.735 - Valid Loss 6.189\n",
            "Epoch: 497/500 - Train loss 0.734 - Valid Loss 6.189\n",
            "Epoch: 498/500 - Train loss 0.732 - Valid Loss 6.189\n",
            "Epoch: 499/500 - Train loss 0.730 - Valid Loss 6.190\n",
            "Epoch: 500/500 - Train loss 0.729 - Valid Loss 6.190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history3['loss']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history3['loss'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history3['val_loss'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KOSVDSBeo1A",
        "outputId": "376ebf2b-4ca7-494a-869e-b7531314b41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FveVOv2xzfkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9f8d81-e544-4049-f14e-1bc1679c08ac"
      },
      "source": [
        "# Ensayo\n",
        "# x = 30\n",
        "# y_test = x * 15\n",
        "\n",
        "x_test = 30\n",
        "y_test = x_test * 15\n",
        "test_input = np.array([x_test])\n",
        "test_input = test_input.reshape((1, seq_length, input_size))\n",
        "test_input = torch.from_numpy(test_input.astype(np.float32))\n",
        "\n",
        "test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1)\n",
        "\n",
        "y_hat = model3(test_input)\n",
        "\n",
        "print(\"y_test:\", y_test)\n",
        "print(\"y_hat:\", y_hat)\n",
        "\n",
        "loss = model3_criterion(y_hat, test_target).item()\n",
        "print(\"loss:\", loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test: 450\n",
            "y_hat: tensor([[437.8927]], grad_fn=<AddmmBackward0>)\n",
            "loss: 146.5874481201172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1g5MZfz5qB"
      },
      "source": [
        "### 4 - Conclusión\n",
        "El resultado alcanzado es bueno pero podría mejorarse agregando más layer LSTM o más layer fully connected."
      ]
    }
  ]
}