{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4c - many-to-one.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9Kjgbdqumzklc2IkxnLcX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NEnBiuLcukJc"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## RNN many-to-one"]},{"cell_type":"markdown","metadata":{"id":"i96B2RF8uqEb"},"source":["#### Datos\n","El objecto es utilizar una serie de sucuencias númericas (datos sintéticos) para poner a prueba el uso de las redes RNN. Este ejemplo se inspiró en otro artículo, lo tienen como referencia en el siguiente link:\\\n","[LINK](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/)"]},{"cell_type":"code","metadata":{"id":"Lx0HQ-1RvJw9"},"source":["import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torchsummar actualmente tiene un problema con las LSTM, por eso\n","# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n","!pip3 install torchinfo\n","from torchinfo import summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLLqv_UJrdUM","executionInfo":{"status":"ok","timestamp":1653859049212,"user_tz":180,"elapsed":8006,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"f381e49c-a19f-4f52-b613-c2ef75bcfaba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.0\n"]}]},{"cell_type":"code","source":["import os\n","import platform\n","\n","if os.access('torch_helpers.py', os.F_OK) is False:\n","    if platform.system() == 'Windows':\n","        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n","    else:\n","        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XYqsq1nnrftF","executionInfo":{"status":"ok","timestamp":1653859049727,"user_tz":180,"elapsed":529,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"a6dd8c77-2161-4a43-f2f2-b0cccb756770"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-29 21:17:28--  http://torch_helpers.py/\n","Resolving torch_helpers.py (torch_helpers.py)... failed: Name or service not known.\n","wget: unable to resolve host address ‘torch_helpers.py’\n","--2022-05-29 21:17:28--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 23883 (23K) [text/plain]\n","Saving to: ‘torch_helpers.py’\n","\n","\rtorch_helpers.py      0%[                    ]       0  --.-KB/s               \rtorch_helpers.py    100%[===================>]  23.32K  --.-KB/s    in 0s      \n","\n","2022-05-29 21:17:29 (91.1 MB/s) - ‘torch_helpers.py’ saved [23883/23883]\n","\n","FINISHED --2022-05-29 21:17:29--\n","Total wall clock time: 0.1s\n","Downloaded: 1 files, 23K in 0s (91.1 MB/s)\n"]}]},{"cell_type":"code","source":["def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n","    # Defino listas para realizar graficas de los resultados\n","    train_loss = []\n","    valid_loss = []\n","\n","    # Defino mi loop de entrenamiento\n","\n","    for epoch in range(epochs):\n","\n","        epoch_train_loss = 0.0\n","        epoch_train_accuracy = 0.0\n","\n","        for train_data, train_target in train_loader:\n","\n","            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n","            # los va acumulando\n","            optimizer.zero_grad()\n","\n","            output = model(train_data)\n","\n","            # Computo el error de la salida comparando contra las etiquetas\n","            loss = criterion(output, train_target)\n","\n","            # Almaceno el error del batch para luego tener el error promedio de la epoca\n","            epoch_train_loss += loss.item()\n","\n","            # Computo el nuevo set de gradientes a lo largo de toda la red\n","            loss.backward()\n","\n","            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n","            optimizer.step()\n","\n","        # Calculo la media de error para la epoca de entrenamiento.\n","        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n","        epoch_train_loss = epoch_train_loss / len(train_loader)\n","        train_loss.append(epoch_train_loss)\n","\n","        # Realizo el paso de validación computando error y accuracy, y\n","        # almacenando los valores para imprimirlos y graficarlos\n","        valid_data, valid_target = iter(valid_loader).next()\n","        output = model(valid_data)\n","        \n","        epoch_valid_loss = criterion(output, valid_target).item()\n","        valid_loss.append(epoch_valid_loss)\n","\n","        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\")\n","\n","    history = {\n","        \"loss\": train_loss,\n","        \"val_loss\": valid_loss,\n","    }\n","    return history"],"metadata":{"id":"11s_raTIrkQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"10bFkG1YuaD9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653859059040,"user_tz":180,"elapsed":4,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"e6bdadb8-0d71-4ebe-a3e0-32927ae5c0d7"},"source":["# Generar datos sintéticos\n","X = list()\n","y = list()\n","\n","# X será una lista de 1 a 45 agrupado de a 3 números consecutivos\n","# [ [1, 2, 3], [4, 5, 6], ....]\n","X = [ [x, x+1, x+2] for x in range(1, 46, 3)]\n","\n","# \"y\" (target) se obtiene como la suma de cada grupo de 3 números de entrada\n","y = [sum(x) for x in X]\n","\n","print(\"datos X:\", X)\n","print(\"datos y:\", y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["datos X: [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29, 30], [31, 32, 33], [34, 35, 36], [37, 38, 39], [40, 41, 42], [43, 44, 45]]\n","datos y: [6, 15, 24, 33, 42, 51, 60, 69, 78, 87, 96, 105, 114, 123, 132]\n"]}]},{"cell_type":"code","metadata":{"id":"Oqabd-kYvza9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653859061715,"user_tz":180,"elapsed":11,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"c287310c-50c3-47ed-ce9e-5e6b85ed37e6"},"source":["# Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1)\n","X = np.array(X).reshape(len(X), len(X[0]), 1)\n","print(\"datos X:\", X)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["datos X: [[[ 1]\n","  [ 2]\n","  [ 3]]\n","\n"," [[ 4]\n","  [ 5]\n","  [ 6]]\n","\n"," [[ 7]\n","  [ 8]\n","  [ 9]]\n","\n"," [[10]\n","  [11]\n","  [12]]\n","\n"," [[13]\n","  [14]\n","  [15]]\n","\n"," [[16]\n","  [17]\n","  [18]]\n","\n"," [[19]\n","  [20]\n","  [21]]\n","\n"," [[22]\n","  [23]\n","  [24]]\n","\n"," [[25]\n","  [26]\n","  [27]]\n","\n"," [[28]\n","  [29]\n","  [30]]\n","\n"," [[31]\n","  [32]\n","  [33]]\n","\n"," [[34]\n","  [35]\n","  [36]]\n","\n"," [[37]\n","  [38]\n","  [39]]\n","\n"," [[40]\n","  [41]\n","  [42]]\n","\n"," [[43]\n","  [44]\n","  [45]]]\n"]}]},{"cell_type":"code","source":["# (batch size, seq_len, input_size)\n","X.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWRNYsZqrs7T","executionInfo":{"status":"ok","timestamp":1653859079425,"user_tz":180,"elapsed":274,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"149cd2b0-a8bc-4ce9-90a8-d1cb546b0d4b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15, 3, 1)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"gYz6XpuyxBbQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653859066211,"user_tz":180,"elapsed":317,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"67fc99a4-5594-4955-89db-efcd793e0c5b"},"source":["y = np.asanyarray(y)\n","y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15,)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["class Data(Dataset):\n","    def __init__(self, x, y):\n","        # Convertir los arrays de numpy a tensores. \n","        # pytorch espera en general entradas 32bits\n","        self.x = torch.from_numpy(x.astype(np.float32))\n","        # las loss unfction esperan la salida float\n","        self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1)\n","\n","        self.len = self.y.shape[0]\n","\n","    def __getitem__(self,index):\n","        return self.x[index], self.y[index]\n","\n","    def __len__(self):\n","        return self.len\n","\n","data_set = Data(X, y)\n","\n","input_dim = data_set.x.shape[1:]\n","seq_length = input_dim[0]\n","input_size = input_dim[1]\n","print(\"Input dim\", input_dim)\n","print(\"seq_length:\", seq_length)\n","print(\"input_size:\", input_size)\n","\n","output_dim = data_set.y.shape[1]\n","print(\"Output dim\", output_dim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ir01D5tprqiV","executionInfo":{"status":"ok","timestamp":1653859092157,"user_tz":180,"elapsed":275,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"1d903746-057f-4bca-f01e-d0674c798464"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input dim torch.Size([3, 1])\n","seq_length: 3\n","input_size: 1\n","Output dim 1\n"]}]},{"cell_type":"code","source":["torch.manual_seed(42)\n","valid_set_size = int(data_set.len * 0.2)\n","train_set_size = data_set.len - valid_set_size\n","\n","# Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos\n","train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n","valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n","\n","print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n","print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n","valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIuvj4TPr0Qy","executionInfo":{"status":"ok","timestamp":1653859111324,"user_tz":180,"elapsed":5,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"019a5bc5-2b48-4bd8-e2dc-a3661e281d50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del conjunto de entrenamiento: 12\n","Tamaño del conjunto de validacion: 3\n"]}]},{"cell_type":"markdown","metadata":{"id":"VG3-d_NXwDGD"},"source":["### 2 - Entrenar el modelo"]},{"cell_type":"code","source":["from torch_helpers import CustomLSTM\n","\n","class Model1(nn.Module):\n","    def __init__(self, input_size, output_dim):\n","        super().__init__()\n","        self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer\n","        self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer\n","        \n","    def forward(self, x):\n","        lstm_output, _ = self.lstm1(x)\n","        out = self.fc(lstm_output[:,-1,:]) # take last output (last seq)\n","        return out\n","\n","model1 = Model1(input_size=input_size, output_dim=output_dim)\n","\n","# Crear el optimizador la una función de error\n","model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)\n","model1_criterion = nn.MSELoss()  # mean squared error\n","\n","summary(model1, input_size=(1, seq_length, input_size))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDX7KwRRr8A5","executionInfo":{"status":"ok","timestamp":1653859206567,"user_tz":180,"elapsed":10,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"c7145068-3276-449c-9a47-09a9c3740f10"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Model1                                   [1, 1]                    --\n","├─CustomLSTM: 1-1                        [1, 3, 64]                16,896\n","│    └─Sigmoid: 2-1                      [1, 64]                   --\n","│    └─Sigmoid: 2-2                      [1, 64]                   --\n","│    └─ReLU: 2-3                         [1, 64]                   --\n","│    └─Sigmoid: 2-4                      [1, 64]                   --\n","│    └─ReLU: 2-5                         [1, 64]                   --\n","│    └─Sigmoid: 2-6                      [1, 64]                   --\n","│    └─Sigmoid: 2-7                      [1, 64]                   --\n","│    └─ReLU: 2-8                         [1, 64]                   --\n","│    └─Sigmoid: 2-9                      [1, 64]                   --\n","│    └─ReLU: 2-10                        [1, 64]                   --\n","│    └─Sigmoid: 2-11                     [1, 64]                   --\n","│    └─Sigmoid: 2-12                     [1, 64]                   --\n","│    └─ReLU: 2-13                        [1, 64]                   --\n","│    └─Sigmoid: 2-14                     [1, 64]                   --\n","│    └─ReLU: 2-15                        [1, 64]                   --\n","├─Linear: 1-2                            [1, 1]                    65\n","==========================================================================================\n","Total params: 16,961\n","Trainable params: 16,961\n","Non-trainable params: 0\n","Total mult-adds (M): 0.00\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.00\n","=========================================================================================="]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["history1 = train(model1,\n","                train_loader,\n","                valid_loader,\n","                model1_optimizer,\n","                model1_criterion,\n","                epochs=500\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ty7Te1kosG5I","executionInfo":{"status":"ok","timestamp":1653859217068,"user_tz":180,"elapsed":2902,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"94f25321-f58a-4c3a-c3e9-66e8fa18961f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/500 - Train loss 4042.838 - Valid Loss 14747.605\n","Epoch: 2/500 - Train loss 3918.166 - Valid Loss 13865.262\n","Epoch: 3/500 - Train loss 3714.442 - Valid Loss 12082.421\n","Epoch: 4/500 - Train loss 3325.547 - Valid Loss 9114.566\n","Epoch: 5/500 - Train loss 2650.400 - Valid Loss 5604.476\n","Epoch: 6/500 - Train loss 1753.709 - Valid Loss 2343.243\n","Epoch: 7/500 - Train loss 843.680 - Valid Loss 133.690\n","Epoch: 8/500 - Train loss 152.409 - Valid Loss 1326.499\n","Epoch: 9/500 - Train loss 206.707 - Valid Loss 4150.042\n","Epoch: 10/500 - Train loss 771.371 - Valid Loss 3626.978\n","Epoch: 11/500 - Train loss 683.090 - Valid Loss 1775.977\n","Epoch: 12/500 - Train loss 316.066 - Valid Loss 423.718\n","Epoch: 13/500 - Train loss 67.760 - Valid Loss 2.913\n","Epoch: 14/500 - Train loss 22.983 - Valid Loss 155.414\n","Epoch: 15/500 - Train loss 98.022 - Valid Loss 457.292\n","Epoch: 16/500 - Train loss 193.492 - Valid Loss 675.073\n","Epoch: 17/500 - Train loss 256.423 - Valid Loss 728.231\n","Epoch: 18/500 - Train loss 269.966 - Valid Loss 621.978\n","Epoch: 19/500 - Train loss 237.134 - Valid Loss 410.884\n","Epoch: 20/500 - Train loss 172.606 - Valid Loss 181.042\n","Epoch: 21/500 - Train loss 98.545 - Valid Loss 25.693\n","Epoch: 22/500 - Train loss 38.388 - Valid Loss 18.987\n","Epoch: 23/500 - Train loss 11.960 - Valid Loss 182.276\n","Epoch: 24/500 - Train loss 27.487 - Valid Loss 437.890\n","Epoch: 25/500 - Train loss 70.244 - Valid Loss 632.878\n","Epoch: 26/500 - Train loss 107.019 - Valid Loss 647.890\n","Epoch: 27/500 - Train loss 110.493 - Valid Loss 489.762\n","Epoch: 28/500 - Train loss 80.905 - Valid Loss 266.086\n","Epoch: 29/500 - Train loss 40.942 - Valid Loss 88.622\n","Epoch: 30/500 - Train loss 13.967 - Valid Loss 7.432\n","Epoch: 31/500 - Train loss 9.149 - Valid Loss 8.717\n","Epoch: 32/500 - Train loss 21.272 - Valid Loss 47.752\n","Epoch: 33/500 - Train loss 38.575 - Valid Loss 81.593\n","Epoch: 34/500 - Train loss 50.390 - Valid Loss 86.649\n","Epoch: 35/500 - Train loss 51.145 - Valid Loss 62.280\n","Epoch: 36/500 - Train loss 41.147 - Valid Loss 25.583\n","Epoch: 37/500 - Train loss 25.285 - Valid Loss 1.548\n","Epoch: 38/500 - Train loss 10.733 - Valid Loss 10.478\n","Epoch: 39/500 - Train loss 4.016 - Valid Loss 61.361\n","Epoch: 40/500 - Train loss 9.485 - Valid Loss 110.289\n","Epoch: 41/500 - Train loss 18.662 - Valid Loss 130.445\n","Epoch: 42/500 - Train loss 21.770 - Valid Loss 119.344\n","Epoch: 43/500 - Train loss 18.609 - Valid Loss 79.032\n","Epoch: 44/500 - Train loss 11.131 - Valid Loss 29.594\n","Epoch: 45/500 - Train loss 4.206 - Valid Loss 3.034\n","Epoch: 46/500 - Train loss 2.848 - Valid Loss 1.295\n","Epoch: 47/500 - Train loss 6.506 - Valid Loss 6.295\n","Epoch: 48/500 - Train loss 9.595 - Valid Loss 2.998\n","Epoch: 49/500 - Train loss 8.094 - Valid Loss 0.189\n","Epoch: 50/500 - Train loss 4.781 - Valid Loss 1.570\n","Epoch: 51/500 - Train loss 2.989 - Valid Loss 5.398\n","Epoch: 52/500 - Train loss 1.978 - Valid Loss 11.605\n","Epoch: 53/500 - Train loss 1.722 - Valid Loss 18.357\n","Epoch: 54/500 - Train loss 2.021 - Valid Loss 22.933\n","Epoch: 55/500 - Train loss 2.380 - Valid Loss 23.349\n","Epoch: 56/500 - Train loss 2.343 - Valid Loss 19.568\n","Epoch: 57/500 - Train loss 1.811 - Valid Loss 13.412\n","Epoch: 58/500 - Train loss 1.064 - Valid Loss 7.332\n","Epoch: 59/500 - Train loss 0.494 - Valid Loss 3.033\n","Epoch: 60/500 - Train loss 0.311 - Valid Loss 0.867\n","Epoch: 61/500 - Train loss 0.432 - Valid Loss 0.176\n","Epoch: 62/500 - Train loss 0.619 - Valid Loss 0.078\n","Epoch: 63/500 - Train loss 0.738 - Valid Loss 0.063\n","Epoch: 64/500 - Train loss 0.927 - Valid Loss 0.063\n","Epoch: 65/500 - Train loss 0.948 - Valid Loss 0.068\n","Epoch: 66/500 - Train loss 0.705 - Valid Loss 0.113\n","Epoch: 67/500 - Train loss 0.519 - Valid Loss 0.356\n","Epoch: 68/500 - Train loss 0.334 - Valid Loss 1.018\n","Epoch: 69/500 - Train loss 0.184 - Valid Loss 2.158\n","Epoch: 70/500 - Train loss 0.131 - Valid Loss 3.521\n","Epoch: 71/500 - Train loss 0.177 - Valid Loss 4.616\n","Epoch: 72/500 - Train loss 0.255 - Valid Loss 5.012\n","Epoch: 73/500 - Train loss 0.288 - Valid Loss 4.615\n","Epoch: 74/500 - Train loss 0.256 - Valid Loss 3.696\n","Epoch: 75/500 - Train loss 0.202 - Valid Loss 2.671\n","Epoch: 76/500 - Train loss 0.179 - Valid Loss 1.851\n","Epoch: 77/500 - Train loss 0.198 - Valid Loss 1.354\n","Epoch: 78/500 - Train loss 0.226 - Valid Loss 1.160\n","Epoch: 79/500 - Train loss 0.225 - Valid Loss 1.214\n","Epoch: 80/500 - Train loss 0.184 - Valid Loss 1.467\n","Epoch: 81/500 - Train loss 0.132 - Valid Loss 1.833\n","Epoch: 82/500 - Train loss 0.104 - Valid Loss 2.156\n","Epoch: 83/500 - Train loss 0.108 - Valid Loss 2.241\n","Epoch: 84/500 - Train loss 0.118 - Valid Loss 1.988\n","Epoch: 85/500 - Train loss 0.105 - Valid Loss 1.481\n","Epoch: 86/500 - Train loss 0.071 - Valid Loss 0.936\n","Epoch: 87/500 - Train loss 0.047 - Valid Loss 0.528\n","Epoch: 88/500 - Train loss 0.049 - Valid Loss 0.307\n","Epoch: 89/500 - Train loss 0.063 - Valid Loss 0.228\n","Epoch: 90/500 - Train loss 0.064 - Valid Loss 0.245\n","Epoch: 91/500 - Train loss 0.051 - Valid Loss 0.333\n","Epoch: 92/500 - Train loss 0.040 - Valid Loss 0.450\n","Epoch: 93/500 - Train loss 0.045 - Valid Loss 0.517\n","Epoch: 94/500 - Train loss 0.056 - Valid Loss 0.470\n","Epoch: 95/500 - Train loss 0.054 - Valid Loss 0.333\n","Epoch: 96/500 - Train loss 0.041 - Valid Loss 0.194\n","Epoch: 97/500 - Train loss 0.034 - Valid Loss 0.110\n","Epoch: 98/500 - Train loss 0.038 - Valid Loss 0.083\n","Epoch: 99/500 - Train loss 0.039 - Valid Loss 0.100\n","Epoch: 100/500 - Train loss 0.032 - Valid Loss 0.161\n","Epoch: 101/500 - Train loss 0.023 - Valid Loss 0.249\n","Epoch: 102/500 - Train loss 0.022 - Valid Loss 0.316\n","Epoch: 103/500 - Train loss 0.025 - Valid Loss 0.311\n","Epoch: 104/500 - Train loss 0.023 - Valid Loss 0.244\n","Epoch: 105/500 - Train loss 0.018 - Valid Loss 0.165\n","Epoch: 106/500 - Train loss 0.016 - Valid Loss 0.116\n","Epoch: 107/500 - Train loss 0.019 - Valid Loss 0.104\n","Epoch: 108/500 - Train loss 0.020 - Valid Loss 0.127\n","Epoch: 109/500 - Train loss 0.017 - Valid Loss 0.176\n","Epoch: 110/500 - Train loss 0.014 - Valid Loss 0.224\n","Epoch: 111/500 - Train loss 0.016 - Valid Loss 0.237\n","Epoch: 112/500 - Train loss 0.017 - Valid Loss 0.202\n","Epoch: 113/500 - Train loss 0.015 - Valid Loss 0.145\n","Epoch: 114/500 - Train loss 0.013 - Valid Loss 0.099\n","Epoch: 115/500 - Train loss 0.013 - Valid Loss 0.079\n","Epoch: 116/500 - Train loss 0.014 - Valid Loss 0.081\n","Epoch: 117/500 - Train loss 0.012 - Valid Loss 0.101\n","Epoch: 118/500 - Train loss 0.010 - Valid Loss 0.122\n","Epoch: 119/500 - Train loss 0.010 - Valid Loss 0.127\n","Epoch: 120/500 - Train loss 0.010 - Valid Loss 0.106\n","Epoch: 121/500 - Train loss 0.009 - Valid Loss 0.075\n","Epoch: 122/500 - Train loss 0.008 - Valid Loss 0.050\n","Epoch: 123/500 - Train loss 0.008 - Valid Loss 0.039\n","Epoch: 124/500 - Train loss 0.008 - Valid Loss 0.041\n","Epoch: 125/500 - Train loss 0.008 - Valid Loss 0.051\n","Epoch: 126/500 - Train loss 0.007 - Valid Loss 0.061\n","Epoch: 127/500 - Train loss 0.007 - Valid Loss 0.061\n","Epoch: 128/500 - Train loss 0.007 - Valid Loss 0.050\n","Epoch: 129/500 - Train loss 0.007 - Valid Loss 0.035\n","Epoch: 130/500 - Train loss 0.006 - Valid Loss 0.026\n","Epoch: 131/500 - Train loss 0.006 - Valid Loss 0.024\n","Epoch: 132/500 - Train loss 0.006 - Valid Loss 0.028\n","Epoch: 133/500 - Train loss 0.006 - Valid Loss 0.035\n","Epoch: 134/500 - Train loss 0.005 - Valid Loss 0.040\n","Epoch: 135/500 - Train loss 0.005 - Valid Loss 0.038\n","Epoch: 136/500 - Train loss 0.005 - Valid Loss 0.030\n","Epoch: 137/500 - Train loss 0.005 - Valid Loss 0.023\n","Epoch: 138/500 - Train loss 0.004 - Valid Loss 0.020\n","Epoch: 139/500 - Train loss 0.004 - Valid Loss 0.022\n","Epoch: 140/500 - Train loss 0.004 - Valid Loss 0.027\n","Epoch: 141/500 - Train loss 0.004 - Valid Loss 0.031\n","Epoch: 142/500 - Train loss 0.004 - Valid Loss 0.031\n","Epoch: 143/500 - Train loss 0.004 - Valid Loss 0.026\n","Epoch: 144/500 - Train loss 0.004 - Valid Loss 0.021\n","Epoch: 145/500 - Train loss 0.003 - Valid Loss 0.018\n","Epoch: 146/500 - Train loss 0.003 - Valid Loss 0.018\n","Epoch: 147/500 - Train loss 0.003 - Valid Loss 0.020\n","Epoch: 148/500 - Train loss 0.003 - Valid Loss 0.022\n","Epoch: 149/500 - Train loss 0.003 - Valid Loss 0.022\n","Epoch: 150/500 - Train loss 0.003 - Valid Loss 0.019\n","Epoch: 151/500 - Train loss 0.003 - Valid Loss 0.015\n","Epoch: 152/500 - Train loss 0.003 - Valid Loss 0.013\n","Epoch: 153/500 - Train loss 0.003 - Valid Loss 0.013\n","Epoch: 154/500 - Train loss 0.003 - Valid Loss 0.013\n","Epoch: 155/500 - Train loss 0.002 - Valid Loss 0.014\n","Epoch: 156/500 - Train loss 0.002 - Valid Loss 0.014\n","Epoch: 157/500 - Train loss 0.002 - Valid Loss 0.012\n","Epoch: 158/500 - Train loss 0.002 - Valid Loss 0.010\n","Epoch: 159/500 - Train loss 0.002 - Valid Loss 0.009\n","Epoch: 160/500 - Train loss 0.002 - Valid Loss 0.008\n","Epoch: 161/500 - Train loss 0.002 - Valid Loss 0.009\n","Epoch: 162/500 - Train loss 0.002 - Valid Loss 0.009\n","Epoch: 163/500 - Train loss 0.002 - Valid Loss 0.009\n","Epoch: 164/500 - Train loss 0.002 - Valid Loss 0.008\n","Epoch: 165/500 - Train loss 0.002 - Valid Loss 0.007\n","Epoch: 166/500 - Train loss 0.002 - Valid Loss 0.006\n","Epoch: 167/500 - Train loss 0.002 - Valid Loss 0.006\n","Epoch: 168/500 - Train loss 0.002 - Valid Loss 0.006\n","Epoch: 169/500 - Train loss 0.002 - Valid Loss 0.007\n","Epoch: 170/500 - Train loss 0.002 - Valid Loss 0.006\n","Epoch: 171/500 - Train loss 0.001 - Valid Loss 0.006\n","Epoch: 172/500 - Train loss 0.001 - Valid Loss 0.005\n","Epoch: 173/500 - Train loss 0.001 - Valid Loss 0.005\n","Epoch: 174/500 - Train loss 0.001 - Valid Loss 0.005\n","Epoch: 175/500 - Train loss 0.001 - Valid Loss 0.005\n","Epoch: 176/500 - Train loss 0.001 - Valid Loss 0.005\n","Epoch: 177/500 - Train loss 0.001 - Valid Loss 0.005\n","Epoch: 178/500 - Train loss 0.001 - Valid Loss 0.004\n","Epoch: 179/500 - Train loss 0.001 - Valid Loss 0.004\n","Epoch: 180/500 - Train loss 0.001 - Valid Loss 0.004\n","Epoch: 181/500 - Train loss 0.001 - Valid Loss 0.004\n","Epoch: 182/500 - Train loss 0.001 - Valid Loss 0.004\n","Epoch: 183/500 - Train loss 0.001 - Valid Loss 0.004\n","Epoch: 184/500 - Train loss 0.001 - Valid Loss 0.003\n","Epoch: 185/500 - Train loss 0.001 - Valid Loss 0.003\n","Epoch: 186/500 - Train loss 0.001 - Valid Loss 0.003\n","Epoch: 187/500 - Train loss 0.001 - Valid Loss 0.003\n","Epoch: 188/500 - Train loss 0.001 - Valid Loss 0.003\n","Epoch: 189/500 - Train loss 0.001 - Valid Loss 0.003\n","Epoch: 190/500 - Train loss 0.001 - Valid Loss 0.003\n","Epoch: 191/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 192/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 193/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 194/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 195/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 196/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 197/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 198/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 199/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 200/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 201/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 202/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 203/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 204/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 205/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 206/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 207/500 - Train loss 0.001 - Valid Loss 0.002\n","Epoch: 208/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 209/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 210/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 211/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 212/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 213/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 214/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 215/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 216/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 217/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 218/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 219/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 220/500 - Train loss 0.001 - Valid Loss 0.001\n","Epoch: 221/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 222/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 223/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 224/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 225/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 226/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 227/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 228/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 229/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 230/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 231/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 232/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 233/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 234/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 235/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 236/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 237/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 238/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 239/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 240/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 241/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 242/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 243/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 244/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 245/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 246/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 247/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 248/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 249/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 250/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 251/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 252/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 253/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 254/500 - Train loss 0.000 - Valid Loss 0.001\n","Epoch: 255/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 256/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 257/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 258/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 259/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 260/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 261/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 262/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 263/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 264/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 265/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 266/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 267/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 268/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 269/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 270/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 271/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 272/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 273/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 274/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 275/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 276/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 277/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 278/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 279/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 280/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 281/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 282/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 283/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 284/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 285/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 286/500 - Train loss 0.000 - Valid Loss 0.002\n","Epoch: 287/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 288/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 289/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 290/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 291/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 292/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 293/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 294/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 295/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 296/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 297/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 298/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 299/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 300/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 301/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 302/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 303/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 304/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 305/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 306/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 307/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 308/500 - Train loss 0.000 - Valid Loss 0.003\n","Epoch: 309/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 310/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 311/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 312/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 313/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 314/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 315/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 316/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 317/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 318/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 319/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 320/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 321/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 322/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 323/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 324/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 325/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 326/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 327/500 - Train loss 0.000 - Valid Loss 0.004\n","Epoch: 328/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 329/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 330/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 331/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 332/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 333/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 334/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 335/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 336/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 337/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 338/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 339/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 340/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 341/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 342/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 343/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 344/500 - Train loss 0.000 - Valid Loss 0.005\n","Epoch: 345/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 346/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 347/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 348/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 349/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 350/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 351/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 352/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 353/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 354/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 355/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 356/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 357/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 358/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 359/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 360/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 361/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 362/500 - Train loss 0.000 - Valid Loss 0.006\n","Epoch: 363/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 364/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 365/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 366/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 367/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 368/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 369/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 370/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 371/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 372/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 373/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 374/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 375/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 376/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 377/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 378/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 379/500 - Train loss 0.000 - Valid Loss 0.007\n","Epoch: 380/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 381/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 382/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 383/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 384/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 385/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 386/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 387/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 388/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 389/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 390/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 391/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 392/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 393/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 394/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 395/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 396/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 397/500 - Train loss 0.000 - Valid Loss 0.008\n","Epoch: 398/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 399/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 400/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 401/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 402/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 403/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 404/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 405/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 406/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 407/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 408/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 409/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 410/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 411/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 412/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 413/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 414/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 415/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 416/500 - Train loss 0.000 - Valid Loss 0.009\n","Epoch: 417/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 418/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 419/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 420/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 421/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 422/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 423/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 424/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 425/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 426/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 427/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 428/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 429/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 430/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 431/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 432/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 433/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 434/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 435/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 436/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 437/500 - Train loss 0.000 - Valid Loss 0.010\n","Epoch: 438/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 439/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 440/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 441/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 442/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 443/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 444/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 445/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 446/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 447/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 448/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 449/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 450/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 451/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 452/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 453/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 454/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 455/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 456/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 457/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 458/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 459/500 - Train loss 0.000 - Valid Loss 0.011\n","Epoch: 460/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 461/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 462/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 463/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 464/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 465/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 466/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 467/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 468/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 469/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 470/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 471/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 472/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 473/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 474/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 475/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 476/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 477/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 478/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 479/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 480/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 481/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 482/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 483/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 484/500 - Train loss 0.000 - Valid Loss 0.012\n","Epoch: 485/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 486/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 487/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 488/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 489/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 490/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 491/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 492/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 493/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 494/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 495/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 496/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 497/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 498/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 499/500 - Train loss 0.000 - Valid Loss 0.013\n","Epoch: 500/500 - Train loss 0.000 - Valid Loss 0.013\n"]}]},{"cell_type":"code","source":["epoch_count = range(1, len(history1['loss']) + 1)\n","sns.lineplot(x=epoch_count,  y=history1['loss'], label='train')\n","sns.lineplot(x=epoch_count,  y=history1['val_loss'], label='valid')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"gfxQerp2sSLT","executionInfo":{"status":"ok","timestamp":1653859237411,"user_tz":180,"elapsed":969,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"40c88f53-f366-4d26-a7c1-1dbff0547b36"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# Ensayo\n","x_test = [50, 51, 52]\n","y_test = sum(x_test)\n","test_input = np.array([x_test])\n","test_input = test_input.reshape((1, seq_length, input_size))\n","test_input = torch.from_numpy(test_input.astype(np.float32))\n","\n","test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1)\n","\n","y_hat = model1(test_input)\n","\n","print(\"y_test:\", y_test)\n","print(\"y_hat:\", y_hat)\n","\n","loss = model1_criterion(y_hat, test_target).item()\n","print(\"loss:\", loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bNrCfcctsWWv","executionInfo":{"status":"ok","timestamp":1653859275790,"user_tz":180,"elapsed":266,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"22c8fe88-2f82-4a0f-904d-f0c4d270cbf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["y_test: 153\n","y_hat: tensor([[153.4888]], grad_fn=<AddmmBackward0>)\n","loss: 0.2389553189277649\n"]}]},{"cell_type":"markdown","metadata":{"id":"AT8b9EfGyshD"},"source":["### 3 - Bidirectional RNN (BRNN)"]},{"cell_type":"markdown","source":["Como la implementación de \"CustomLSTM\" no soporta el flag de \"bidirectional\" que trae Pytorch, utilizaremos la layer por defecto de LSTM. Por eso notará que la cantidad de parámetros con bidireccional no es exactamente el doble (porque pytorch agrega otro bias a la ecuación tradicional).\\\n","La única desventaja en este caso, al tratarse de una serie temporal no podremos utilizar la función de activación \"relu\" y por lo tanto el resultado alcanzado no será tan bueno como con la CustomLSTM."],"metadata":{"id":"XSnk8lSyul2u"}},{"cell_type":"code","source":["from torch_helpers import CustomLSTM\n","\n","# En esta oportunidad se utilizará Bidirectional, dentro se especifica\n","# que lo que se desea hacer bidireccional es una capa LSTM\n","\n","# En el summary se puede observar que la cantidad de parámetros\n","# de nuestor nueva capa LSTM bidireccional es el doble que la anterior\n","\n","class Model2(nn.Module):\n","    def __init__(self, input_size, output_dim):\n","        super().__init__()\n","        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(in_features=2*64, out_features=output_dim) #  # Fully connected layer\n","        \n","    def forward(self, x):\n","        lstm_output, _ = self.lstm1(x)\n","        out = self.fc(lstm_output[:,-1,:]) # take last output (last seq)\n","        return out\n","\n","model2 = Model2(input_size=input_size, output_dim=output_dim)\n","\n","# Crear el optimizador la una función de error\n","model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n","model2_criterion = nn.MSELoss()  # mean squared error\n","\n","summary(model2, input_size=(1, seq_length, input_size))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gg3ygYOsfjD","executionInfo":{"status":"ok","timestamp":1653859466907,"user_tz":180,"elapsed":314,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"e9dc474b-be4f-44ed-f894-d10f4eefa280"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Model2                                   [1, 1]                    --\n","├─LSTM: 1-1                              [1, 3, 128]               34,304\n","├─Linear: 1-2                            [1, 1]                    129\n","==========================================================================================\n","Total params: 34,433\n","Trainable params: 34,433\n","Non-trainable params: 0\n","Total mult-adds (M): 0.10\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.14\n","Estimated Total Size (MB): 0.14\n","=========================================================================================="]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["history2 = train(model2,\n","                train_loader,\n","                valid_loader,\n","                model2_optimizer,\n","                model2_criterion,\n","                epochs=500\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NFC2NszLtRzp","executionInfo":{"status":"ok","timestamp":1653859520563,"user_tz":180,"elapsed":3000,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"31bcaef1-2873-46f9-d42e-4b228f7c056c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/500 - Train loss 4005.948 - Valid Loss 14907.582\n","Epoch: 2/500 - Train loss 3921.674 - Valid Loss 14727.104\n","Epoch: 3/500 - Train loss 3836.231 - Valid Loss 14530.659\n","Epoch: 4/500 - Train loss 3746.846 - Valid Loss 14321.375\n","Epoch: 5/500 - Train loss 3652.673 - Valid Loss 14093.188\n","Epoch: 6/500 - Train loss 3552.495 - Valid Loss 13848.782\n","Epoch: 7/500 - Train loss 3449.546 - Valid Loss 13596.863\n","Epoch: 8/500 - Train loss 3347.648 - Valid Loss 13344.054\n","Epoch: 9/500 - Train loss 3249.293 - Valid Loss 13100.435\n","Epoch: 10/500 - Train loss 3154.894 - Valid Loss 12865.003\n","Epoch: 11/500 - Train loss 3062.719 - Valid Loss 12629.790\n","Epoch: 12/500 - Train loss 2971.169 - Valid Loss 12391.567\n","Epoch: 13/500 - Train loss 2879.835 - Valid Loss 12151.559\n","Epoch: 14/500 - Train loss 2789.029 - Valid Loss 11913.071\n","Epoch: 15/500 - Train loss 2699.112 - Valid Loss 11677.581\n","Epoch: 16/500 - Train loss 2610.190 - Valid Loss 11445.011\n","Epoch: 17/500 - Train loss 2522.455 - Valid Loss 11215.792\n","Epoch: 18/500 - Train loss 2436.222 - Valid Loss 10989.417\n","Epoch: 19/500 - Train loss 2351.813 - Valid Loss 10764.810\n","Epoch: 20/500 - Train loss 2269.672 - Valid Loss 10542.406\n","Epoch: 21/500 - Train loss 2190.444 - Valid Loss 10324.433\n","Epoch: 22/500 - Train loss 2114.622 - Valid Loss 10113.466\n","Epoch: 23/500 - Train loss 2042.138 - Valid Loss 9909.125\n","Epoch: 24/500 - Train loss 1972.604 - Valid Loss 9709.570\n","Epoch: 25/500 - Train loss 1905.751 - Valid Loss 9513.835\n","Epoch: 26/500 - Train loss 1841.447 - Valid Loss 9321.655\n","Epoch: 27/500 - Train loss 1779.630 - Valid Loss 9133.169\n","Epoch: 28/500 - Train loss 1720.256 - Valid Loss 8948.614\n","Epoch: 29/500 - Train loss 1663.276 - Valid Loss 8767.980\n","Epoch: 30/500 - Train loss 1608.604 - Valid Loss 8590.901\n","Epoch: 31/500 - Train loss 1556.105 - Valid Loss 8416.784\n","Epoch: 32/500 - Train loss 1505.610 - Valid Loss 8245.009\n","Epoch: 33/500 - Train loss 1456.929 - Valid Loss 8075.089\n","Epoch: 34/500 - Train loss 1409.877 - Valid Loss 7906.820\n","Epoch: 35/500 - Train loss 1364.297 - Valid Loss 7740.372\n","Epoch: 36/500 - Train loss 1320.077 - Valid Loss 7576.333\n","Epoch: 37/500 - Train loss 1277.147 - Valid Loss 7415.643\n","Epoch: 38/500 - Train loss 1235.443 - Valid Loss 7259.346\n","Epoch: 39/500 - Train loss 1194.839 - Valid Loss 7108.262\n","Epoch: 40/500 - Train loss 1155.095 - Valid Loss 6962.811\n","Epoch: 41/500 - Train loss 1115.859 - Valid Loss 6822.916\n","Epoch: 42/500 - Train loss 1076.488 - Valid Loss 6688.228\n","Epoch: 43/500 - Train loss 1035.863 - Valid Loss 6558.237\n","Epoch: 44/500 - Train loss 993.597 - Valid Loss 6431.613\n","Epoch: 45/500 - Train loss 951.046 - Valid Loss 6306.719\n","Epoch: 46/500 - Train loss 912.237 - Valid Loss 6183.609\n","Epoch: 47/500 - Train loss 872.453 - Valid Loss 6062.750\n","Epoch: 48/500 - Train loss 834.668 - Valid Loss 5943.133\n","Epoch: 49/500 - Train loss 799.592 - Valid Loss 5824.471\n","Epoch: 50/500 - Train loss 768.775 - Valid Loss 5707.107\n","Epoch: 51/500 - Train loss 737.728 - Valid Loss 5591.771\n","Epoch: 52/500 - Train loss 706.622 - Valid Loss 5479.592\n","Epoch: 53/500 - Train loss 681.275 - Valid Loss 5370.753\n","Epoch: 54/500 - Train loss 658.218 - Valid Loss 5264.472\n","Epoch: 55/500 - Train loss 632.562 - Valid Loss 5161.309\n","Epoch: 56/500 - Train loss 609.211 - Valid Loss 5060.317\n","Epoch: 57/500 - Train loss 586.614 - Valid Loss 4961.404\n","Epoch: 58/500 - Train loss 564.485 - Valid Loss 4864.887\n","Epoch: 59/500 - Train loss 543.783 - Valid Loss 4770.509\n","Epoch: 60/500 - Train loss 522.828 - Valid Loss 4677.591\n","Epoch: 61/500 - Train loss 502.486 - Valid Loss 4585.707\n","Epoch: 62/500 - Train loss 482.837 - Valid Loss 4495.557\n","Epoch: 63/500 - Train loss 464.128 - Valid Loss 4407.746\n","Epoch: 64/500 - Train loss 446.537 - Valid Loss 4322.236\n","Epoch: 65/500 - Train loss 429.830 - Valid Loss 4238.557\n","Epoch: 66/500 - Train loss 413.789 - Valid Loss 4156.336\n","Epoch: 67/500 - Train loss 398.106 - Valid Loss 4074.816\n","Epoch: 68/500 - Train loss 382.789 - Valid Loss 3992.298\n","Epoch: 69/500 - Train loss 367.692 - Valid Loss 3907.764\n","Epoch: 70/500 - Train loss 353.041 - Valid Loss 3824.200\n","Epoch: 71/500 - Train loss 338.942 - Valid Loss 3743.652\n","Epoch: 72/500 - Train loss 325.462 - Valid Loss 3666.709\n","Epoch: 73/500 - Train loss 312.554 - Valid Loss 3592.698\n","Epoch: 74/500 - Train loss 300.211 - Valid Loss 3520.826\n","Epoch: 75/500 - Train loss 288.373 - Valid Loss 3450.726\n","Epoch: 76/500 - Train loss 276.984 - Valid Loss 3382.221\n","Epoch: 77/500 - Train loss 265.974 - Valid Loss 3314.566\n","Epoch: 78/500 - Train loss 255.266 - Valid Loss 3247.358\n","Epoch: 79/500 - Train loss 244.958 - Valid Loss 3181.972\n","Epoch: 80/500 - Train loss 235.263 - Valid Loss 3119.093\n","Epoch: 81/500 - Train loss 226.000 - Valid Loss 3058.227\n","Epoch: 82/500 - Train loss 217.146 - Valid Loss 2998.982\n","Epoch: 83/500 - Train loss 208.607 - Valid Loss 2941.379\n","Epoch: 84/500 - Train loss 200.447 - Valid Loss 2885.432\n","Epoch: 85/500 - Train loss 192.646 - Valid Loss 2830.905\n","Epoch: 86/500 - Train loss 185.162 - Valid Loss 2777.934\n","Epoch: 87/500 - Train loss 178.016 - Valid Loss 2726.481\n","Epoch: 88/500 - Train loss 171.185 - Valid Loss 2676.340\n","Epoch: 89/500 - Train loss 164.650 - Valid Loss 2627.797\n","Epoch: 90/500 - Train loss 158.422 - Valid Loss 2580.444\n","Epoch: 91/500 - Train loss 152.473 - Valid Loss 2534.607\n","Epoch: 92/500 - Train loss 146.798 - Valid Loss 2489.985\n","Epoch: 93/500 - Train loss 141.392 - Valid Loss 2446.615\n","Epoch: 94/500 - Train loss 136.245 - Valid Loss 2404.577\n","Epoch: 95/500 - Train loss 131.313 - Valid Loss 2363.615\n","Epoch: 96/500 - Train loss 126.578 - Valid Loss 2323.851\n","Epoch: 97/500 - Train loss 122.052 - Valid Loss 2285.245\n","Epoch: 98/500 - Train loss 117.716 - Valid Loss 2247.600\n","Epoch: 99/500 - Train loss 113.537 - Valid Loss 2211.119\n","Epoch: 100/500 - Train loss 109.520 - Valid Loss 2175.483\n","Epoch: 101/500 - Train loss 105.669 - Valid Loss 2140.910\n","Epoch: 102/500 - Train loss 101.987 - Valid Loss 2107.062\n","Epoch: 103/500 - Train loss 98.467 - Valid Loss 2074.272\n","Epoch: 104/500 - Train loss 95.138 - Valid Loss 2041.877\n","Epoch: 105/500 - Train loss 92.126 - Valid Loss 2010.868\n","Epoch: 106/500 - Train loss 89.425 - Valid Loss 1979.599\n","Epoch: 107/500 - Train loss 86.672 - Valid Loss 1949.905\n","Epoch: 108/500 - Train loss 82.917 - Valid Loss 1920.563\n","Epoch: 109/500 - Train loss 80.220 - Valid Loss 1891.593\n","Epoch: 110/500 - Train loss 78.003 - Valid Loss 1864.088\n","Epoch: 111/500 - Train loss 74.863 - Valid Loss 1837.155\n","Epoch: 112/500 - Train loss 72.576 - Valid Loss 1810.609\n","Epoch: 113/500 - Train loss 70.364 - Valid Loss 1785.301\n","Epoch: 114/500 - Train loss 67.746 - Valid Loss 1760.714\n","Epoch: 115/500 - Train loss 65.904 - Valid Loss 1736.292\n","Epoch: 116/500 - Train loss 63.620 - Valid Loss 1712.758\n","Epoch: 117/500 - Train loss 61.662 - Valid Loss 1690.085\n","Epoch: 118/500 - Train loss 59.827 - Valid Loss 1667.574\n","Epoch: 119/500 - Train loss 57.788 - Valid Loss 1645.581\n","Epoch: 120/500 - Train loss 56.181 - Valid Loss 1624.442\n","Epoch: 121/500 - Train loss 54.319 - Valid Loss 1603.646\n","Epoch: 122/500 - Train loss 52.715 - Valid Loss 1583.100\n","Epoch: 123/500 - Train loss 51.189 - Valid Loss 1563.337\n","Epoch: 124/500 - Train loss 49.512 - Valid Loss 1544.023\n","Epoch: 125/500 - Train loss 48.163 - Valid Loss 1524.833\n","Epoch: 126/500 - Train loss 46.740 - Valid Loss 1506.391\n","Epoch: 127/500 - Train loss 45.223 - Valid Loss 1488.375\n","Epoch: 128/500 - Train loss 44.027 - Valid Loss 1470.426\n","Epoch: 129/500 - Train loss 42.804 - Valid Loss 1453.267\n","Epoch: 130/500 - Train loss 41.386 - Valid Loss 1436.366\n","Epoch: 131/500 - Train loss 40.238 - Valid Loss 1419.601\n","Epoch: 132/500 - Train loss 39.239 - Valid Loss 1403.632\n","Epoch: 133/500 - Train loss 38.034 - Valid Loss 1387.653\n","Epoch: 134/500 - Train loss 36.845 - Valid Loss 1372.139\n","Epoch: 135/500 - Train loss 35.823 - Valid Loss 1357.119\n","Epoch: 136/500 - Train loss 34.925 - Valid Loss 1342.071\n","Epoch: 137/500 - Train loss 34.034 - Valid Loss 1327.767\n","Epoch: 138/500 - Train loss 33.046 - Valid Loss 1313.399\n","Epoch: 139/500 - Train loss 32.058 - Valid Loss 1299.550\n","Epoch: 140/500 - Train loss 31.148 - Valid Loss 1285.969\n","Epoch: 141/500 - Train loss 30.340 - Valid Loss 1272.529\n","Epoch: 142/500 - Train loss 29.598 - Valid Loss 1259.618\n","Epoch: 143/500 - Train loss 28.857 - Valid Loss 1246.670\n","Epoch: 144/500 - Train loss 28.080 - Valid Loss 1234.229\n","Epoch: 145/500 - Train loss 27.293 - Valid Loss 1221.884\n","Epoch: 146/500 - Train loss 26.555 - Valid Loss 1209.793\n","Epoch: 147/500 - Train loss 25.897 - Valid Loss 1198.001\n","Epoch: 148/500 - Train loss 25.276 - Valid Loss 1186.277\n","Epoch: 149/500 - Train loss 24.632 - Valid Loss 1174.845\n","Epoch: 150/500 - Train loss 23.975 - Valid Loss 1163.520\n","Epoch: 151/500 - Train loss 23.355 - Valid Loss 1152.350\n","Epoch: 152/500 - Train loss 22.787 - Valid Loss 1141.359\n","Epoch: 153/500 - Train loss 22.227 - Valid Loss 1130.457\n","Epoch: 154/500 - Train loss 21.651 - Valid Loss 1119.751\n","Epoch: 155/500 - Train loss 21.089 - Valid Loss 1109.221\n","Epoch: 156/500 - Train loss 20.569 - Valid Loss 1098.848\n","Epoch: 157/500 - Train loss 20.066 - Valid Loss 1088.759\n","Epoch: 158/500 - Train loss 19.553 - Valid Loss 1078.828\n","Epoch: 159/500 - Train loss 19.056 - Valid Loss 1069.105\n","Epoch: 160/500 - Train loss 18.589 - Valid Loss 1059.616\n","Epoch: 161/500 - Train loss 18.134 - Valid Loss 1050.236\n","Epoch: 162/500 - Train loss 17.678 - Valid Loss 1041.044\n","Epoch: 163/500 - Train loss 17.235 - Valid Loss 1031.977\n","Epoch: 164/500 - Train loss 16.813 - Valid Loss 1023.001\n","Epoch: 165/500 - Train loss 16.404 - Valid Loss 1014.183\n","Epoch: 166/500 - Train loss 15.997 - Valid Loss 1005.461\n","Epoch: 167/500 - Train loss 15.597 - Valid Loss 996.910\n","Epoch: 168/500 - Train loss 15.211 - Valid Loss 988.522\n","Epoch: 169/500 - Train loss 14.840 - Valid Loss 980.260\n","Epoch: 170/500 - Train loss 14.479 - Valid Loss 972.190\n","Epoch: 171/500 - Train loss 14.124 - Valid Loss 964.225\n","Epoch: 172/500 - Train loss 13.777 - Valid Loss 956.434\n","Epoch: 173/500 - Train loss 13.436 - Valid Loss 948.744\n","Epoch: 174/500 - Train loss 13.106 - Valid Loss 941.183\n","Epoch: 175/500 - Train loss 12.784 - Valid Loss 933.718\n","Epoch: 176/500 - Train loss 12.471 - Valid Loss 926.365\n","Epoch: 177/500 - Train loss 12.167 - Valid Loss 919.121\n","Epoch: 178/500 - Train loss 11.870 - Valid Loss 911.988\n","Epoch: 179/500 - Train loss 11.582 - Valid Loss 904.968\n","Epoch: 180/500 - Train loss 11.302 - Valid Loss 898.058\n","Epoch: 181/500 - Train loss 11.030 - Valid Loss 891.266\n","Epoch: 182/500 - Train loss 10.766 - Valid Loss 884.565\n","Epoch: 183/500 - Train loss 10.512 - Valid Loss 877.999\n","Epoch: 184/500 - Train loss 10.286 - Valid Loss 871.441\n","Epoch: 185/500 - Train loss 10.198 - Valid Loss 865.280\n","Epoch: 186/500 - Train loss 10.786 - Valid Loss 858.427\n","Epoch: 187/500 - Train loss 13.931 - Valid Loss 852.996\n","Epoch: 188/500 - Train loss 11.843 - Valid Loss 846.298\n","Epoch: 189/500 - Train loss 9.289 - Valid Loss 840.219\n","Epoch: 190/500 - Train loss 9.924 - Valid Loss 834.996\n","Epoch: 191/500 - Train loss 9.956 - Valid Loss 829.097\n","Epoch: 192/500 - Train loss 8.591 - Valid Loss 823.512\n","Epoch: 193/500 - Train loss 9.315 - Valid Loss 818.464\n","Epoch: 194/500 - Train loss 8.606 - Valid Loss 813.166\n","Epoch: 195/500 - Train loss 8.208 - Valid Loss 807.798\n","Epoch: 196/500 - Train loss 8.562 - Valid Loss 802.839\n","Epoch: 197/500 - Train loss 7.709 - Valid Loss 797.954\n","Epoch: 198/500 - Train loss 7.964 - Valid Loss 792.835\n","Epoch: 199/500 - Train loss 7.648 - Valid Loss 787.827\n","Epoch: 200/500 - Train loss 7.267 - Valid Loss 782.940\n","Epoch: 201/500 - Train loss 7.453 - Valid Loss 777.824\n","Epoch: 202/500 - Train loss 6.919 - Valid Loss 772.914\n","Epoch: 203/500 - Train loss 7.005 - Valid Loss 768.298\n","Epoch: 204/500 - Train loss 6.765 - Valid Loss 763.532\n","Epoch: 205/500 - Train loss 6.526 - Valid Loss 758.719\n","Epoch: 206/500 - Train loss 6.571 - Valid Loss 754.240\n","Epoch: 207/500 - Train loss 6.205 - Valid Loss 749.841\n","Epoch: 208/500 - Train loss 6.245 - Valid Loss 745.263\n","Epoch: 209/500 - Train loss 6.024 - Valid Loss 740.966\n","Epoch: 210/500 - Train loss 5.876 - Valid Loss 736.873\n","Epoch: 211/500 - Train loss 5.862 - Valid Loss 732.569\n","Epoch: 212/500 - Train loss 5.599 - Valid Loss 728.394\n","Epoch: 213/500 - Train loss 5.591 - Valid Loss 724.526\n","Epoch: 214/500 - Train loss 5.444 - Valid Loss 720.524\n","Epoch: 215/500 - Train loss 5.290 - Valid Loss 716.501\n","Epoch: 216/500 - Train loss 5.280 - Valid Loss 712.771\n","Epoch: 217/500 - Train loss 5.101 - Valid Loss 708.977\n","Epoch: 218/500 - Train loss 5.020 - Valid Loss 705.099\n","Epoch: 219/500 - Train loss 4.971 - Valid Loss 701.518\n","Epoch: 220/500 - Train loss 4.815 - Valid Loss 697.927\n","Epoch: 221/500 - Train loss 4.763 - Valid Loss 694.211\n","Epoch: 222/500 - Train loss 4.693 - Valid Loss 690.763\n","Epoch: 223/500 - Train loss 4.567 - Valid Loss 687.335\n","Epoch: 224/500 - Train loss 4.519 - Valid Loss 683.780\n","Epoch: 225/500 - Train loss 4.441 - Valid Loss 680.460\n","Epoch: 226/500 - Train loss 4.340 - Valid Loss 677.195\n","Epoch: 227/500 - Train loss 4.292 - Valid Loss 673.816\n","Epoch: 228/500 - Train loss 4.213 - Valid Loss 670.632\n","Epoch: 229/500 - Train loss 4.131 - Valid Loss 667.534\n","Epoch: 230/500 - Train loss 4.081 - Valid Loss 664.329\n","Epoch: 231/500 - Train loss 4.000 - Valid Loss 661.265\n","Epoch: 232/500 - Train loss 3.934 - Valid Loss 658.321\n","Epoch: 233/500 - Train loss 3.877 - Valid Loss 655.293\n","Epoch: 234/500 - Train loss 3.801 - Valid Loss 652.339\n","Epoch: 235/500 - Train loss 3.745 - Valid Loss 649.516\n","Epoch: 236/500 - Train loss 3.684 - Valid Loss 646.637\n","Epoch: 237/500 - Train loss 3.619 - Valid Loss 643.779\n","Epoch: 238/500 - Train loss 3.569 - Valid Loss 641.049\n","Epoch: 239/500 - Train loss 3.507 - Valid Loss 638.294\n","Epoch: 240/500 - Train loss 3.450 - Valid Loss 635.517\n","Epoch: 241/500 - Train loss 3.396 - Valid Loss 632.853\n","Epoch: 242/500 - Train loss 3.336 - Valid Loss 630.214\n","Epoch: 243/500 - Train loss 3.286 - Valid Loss 627.541\n","Epoch: 244/500 - Train loss 3.232 - Valid Loss 624.953\n","Epoch: 245/500 - Train loss 3.178 - Valid Loss 622.411\n","Epoch: 246/500 - Train loss 3.130 - Valid Loss 619.830\n","Epoch: 247/500 - Train loss 3.076 - Valid Loss 617.310\n","Epoch: 248/500 - Train loss 3.027 - Valid Loss 614.858\n","Epoch: 249/500 - Train loss 2.978 - Valid Loss 612.378\n","Epoch: 250/500 - Train loss 2.928 - Valid Loss 609.929\n","Epoch: 251/500 - Train loss 2.881 - Valid Loss 607.552\n","Epoch: 252/500 - Train loss 2.834 - Valid Loss 605.164\n","Epoch: 253/500 - Train loss 2.786 - Valid Loss 602.800\n","Epoch: 254/500 - Train loss 2.741 - Valid Loss 600.508\n","Epoch: 255/500 - Train loss 2.695 - Valid Loss 598.219\n","Epoch: 256/500 - Train loss 2.650 - Valid Loss 595.945\n","Epoch: 257/500 - Train loss 2.607 - Valid Loss 593.736\n","Epoch: 258/500 - Train loss 2.562 - Valid Loss 591.537\n","Epoch: 259/500 - Train loss 2.519 - Valid Loss 589.349\n","Epoch: 260/500 - Train loss 2.478 - Valid Loss 587.215\n","Epoch: 261/500 - Train loss 2.435 - Valid Loss 585.090\n","Epoch: 262/500 - Train loss 2.394 - Valid Loss 582.971\n","Epoch: 263/500 - Train loss 2.354 - Valid Loss 580.888\n","Epoch: 264/500 - Train loss 2.313 - Valid Loss 578.801\n","Epoch: 265/500 - Train loss 2.274 - Valid Loss 576.710\n","Epoch: 266/500 - Train loss 2.235 - Valid Loss 574.637\n","Epoch: 267/500 - Train loss 2.197 - Valid Loss 572.549\n","Epoch: 268/500 - Train loss 2.159 - Valid Loss 570.457\n","Epoch: 269/500 - Train loss 2.122 - Valid Loss 568.378\n","Epoch: 270/500 - Train loss 2.085 - Valid Loss 566.288\n","Epoch: 271/500 - Train loss 2.048 - Valid Loss 564.206\n","Epoch: 272/500 - Train loss 2.012 - Valid Loss 562.133\n","Epoch: 273/500 - Train loss 1.977 - Valid Loss 560.055\n","Epoch: 274/500 - Train loss 1.942 - Valid Loss 557.999\n","Epoch: 275/500 - Train loss 1.907 - Valid Loss 555.956\n","Epoch: 276/500 - Train loss 1.874 - Valid Loss 553.919\n","Epoch: 277/500 - Train loss 1.840 - Valid Loss 551.912\n","Epoch: 278/500 - Train loss 1.807 - Valid Loss 549.913\n","Epoch: 279/500 - Train loss 1.774 - Valid Loss 547.939\n","Epoch: 280/500 - Train loss 1.742 - Valid Loss 545.994\n","Epoch: 281/500 - Train loss 1.711 - Valid Loss 544.062\n","Epoch: 282/500 - Train loss 1.680 - Valid Loss 542.167\n","Epoch: 283/500 - Train loss 1.649 - Valid Loss 540.289\n","Epoch: 284/500 - Train loss 1.619 - Valid Loss 538.437\n","Epoch: 285/500 - Train loss 1.590 - Valid Loss 536.612\n","Epoch: 286/500 - Train loss 1.561 - Valid Loss 534.798\n","Epoch: 287/500 - Train loss 1.532 - Valid Loss 533.015\n","Epoch: 288/500 - Train loss 1.504 - Valid Loss 531.240\n","Epoch: 289/500 - Train loss 1.476 - Valid Loss 529.490\n","Epoch: 290/500 - Train loss 1.449 - Valid Loss 527.750\n","Epoch: 291/500 - Train loss 1.422 - Valid Loss 526.024\n","Epoch: 292/500 - Train loss 1.395 - Valid Loss 524.313\n","Epoch: 293/500 - Train loss 1.369 - Valid Loss 522.610\n","Epoch: 294/500 - Train loss 1.343 - Valid Loss 520.926\n","Epoch: 295/500 - Train loss 1.318 - Valid Loss 519.243\n","Epoch: 296/500 - Train loss 1.293 - Valid Loss 517.585\n","Epoch: 297/500 - Train loss 1.269 - Valid Loss 515.922\n","Epoch: 298/500 - Train loss 1.245 - Valid Loss 514.294\n","Epoch: 299/500 - Train loss 1.222 - Valid Loss 512.643\n","Epoch: 300/500 - Train loss 1.200 - Valid Loss 511.056\n","Epoch: 301/500 - Train loss 1.181 - Valid Loss 509.403\n","Epoch: 302/500 - Train loss 1.168 - Valid Loss 507.892\n","Epoch: 303/500 - Train loss 1.173 - Valid Loss 506.179\n","Epoch: 304/500 - Train loss 1.240 - Valid Loss 504.850\n","Epoch: 305/500 - Train loss 1.424 - Valid Loss 502.922\n","Epoch: 306/500 - Train loss 2.097 - Valid Loss 501.950\n","Epoch: 307/500 - Train loss 2.416 - Valid Loss 499.729\n","Epoch: 308/500 - Train loss 3.219 - Valid Loss 498.758\n","Epoch: 309/500 - Train loss 1.558 - Valid Loss 497.156\n","Epoch: 310/500 - Train loss 1.076 - Valid Loss 495.465\n","Epoch: 311/500 - Train loss 1.841 - Valid Loss 494.632\n","Epoch: 312/500 - Train loss 1.723 - Valid Loss 492.987\n","Epoch: 313/500 - Train loss 1.201 - Valid Loss 491.755\n","Epoch: 314/500 - Train loss 1.048 - Valid Loss 490.761\n","Epoch: 315/500 - Train loss 1.479 - Valid Loss 489.199\n","Epoch: 316/500 - Train loss 1.473 - Valid Loss 488.192\n","Epoch: 317/500 - Train loss 0.920 - Valid Loss 487.130\n","Epoch: 318/500 - Train loss 1.260 - Valid Loss 485.673\n","Epoch: 319/500 - Train loss 1.497 - Valid Loss 484.629\n","Epoch: 320/500 - Train loss 0.896 - Valid Loss 483.553\n","Epoch: 321/500 - Train loss 1.113 - Valid Loss 482.243\n","Epoch: 322/500 - Train loss 1.425 - Valid Loss 481.171\n","Epoch: 323/500 - Train loss 0.854 - Valid Loss 479.988\n","Epoch: 324/500 - Train loss 1.043 - Valid Loss 478.590\n","Epoch: 325/500 - Train loss 1.353 - Valid Loss 477.512\n","Epoch: 326/500 - Train loss 0.816 - Valid Loss 476.349\n","Epoch: 327/500 - Train loss 0.952 - Valid Loss 474.981\n","Epoch: 328/500 - Train loss 1.210 - Valid Loss 473.874\n","Epoch: 329/500 - Train loss 0.763 - Valid Loss 472.726\n","Epoch: 330/500 - Train loss 0.885 - Valid Loss 471.397\n","Epoch: 331/500 - Train loss 1.087 - Valid Loss 470.310\n","Epoch: 332/500 - Train loss 0.709 - Valid Loss 469.169\n","Epoch: 333/500 - Train loss 0.837 - Valid Loss 467.838\n","Epoch: 334/500 - Train loss 0.990 - Valid Loss 466.815\n","Epoch: 335/500 - Train loss 0.664 - Valid Loss 465.736\n","Epoch: 336/500 - Train loss 0.773 - Valid Loss 464.447\n","Epoch: 337/500 - Train loss 0.896 - Valid Loss 463.411\n","Epoch: 338/500 - Train loss 0.626 - Valid Loss 462.340\n","Epoch: 339/500 - Train loss 0.708 - Valid Loss 461.105\n","Epoch: 340/500 - Train loss 0.814 - Valid Loss 460.119\n","Epoch: 341/500 - Train loss 0.591 - Valid Loss 459.071\n","Epoch: 342/500 - Train loss 0.644 - Valid Loss 457.881\n","Epoch: 343/500 - Train loss 0.740 - Valid Loss 456.940\n","Epoch: 344/500 - Train loss 0.562 - Valid Loss 455.930\n","Epoch: 345/500 - Train loss 0.578 - Valid Loss 454.809\n","Epoch: 346/500 - Train loss 0.668 - Valid Loss 453.884\n","Epoch: 347/500 - Train loss 0.539 - Valid Loss 452.885\n","Epoch: 348/500 - Train loss 0.517 - Valid Loss 451.840\n","Epoch: 349/500 - Train loss 0.596 - Valid Loss 450.952\n","Epoch: 350/500 - Train loss 0.520 - Valid Loss 449.956\n","Epoch: 351/500 - Train loss 0.467 - Valid Loss 448.955\n","Epoch: 352/500 - Train loss 0.519 - Valid Loss 448.081\n","Epoch: 353/500 - Train loss 0.501 - Valid Loss 447.094\n","Epoch: 354/500 - Train loss 0.439 - Valid Loss 446.139\n","Epoch: 355/500 - Train loss 0.446 - Valid Loss 445.249\n","Epoch: 356/500 - Train loss 0.468 - Valid Loss 444.264\n","Epoch: 357/500 - Train loss 0.434 - Valid Loss 443.361\n","Epoch: 358/500 - Train loss 0.399 - Valid Loss 442.461\n","Epoch: 359/500 - Train loss 0.414 - Valid Loss 441.488\n","Epoch: 360/500 - Train loss 0.422 - Valid Loss 440.604\n","Epoch: 361/500 - Train loss 0.389 - Valid Loss 439.673\n","Epoch: 362/500 - Train loss 0.366 - Valid Loss 438.744\n","Epoch: 363/500 - Train loss 0.375 - Valid Loss 437.876\n","Epoch: 364/500 - Train loss 0.377 - Valid Loss 436.932\n","Epoch: 365/500 - Train loss 0.357 - Valid Loss 436.061\n","Epoch: 366/500 - Train loss 0.335 - Valid Loss 435.189\n","Epoch: 367/500 - Train loss 0.333 - Valid Loss 434.278\n","Epoch: 368/500 - Train loss 0.339 - Valid Loss 433.443\n","Epoch: 369/500 - Train loss 0.330 - Valid Loss 432.546\n","Epoch: 370/500 - Train loss 0.312 - Valid Loss 431.710\n","Epoch: 371/500 - Train loss 0.299 - Valid Loss 430.879\n","Epoch: 372/500 - Train loss 0.297 - Valid Loss 430.017\n","Epoch: 373/500 - Train loss 0.298 - Valid Loss 429.229\n","Epoch: 374/500 - Train loss 0.293 - Valid Loss 428.380\n","Epoch: 375/500 - Train loss 0.281 - Valid Loss 427.598\n","Epoch: 376/500 - Train loss 0.269 - Valid Loss 426.794\n","Epoch: 377/500 - Train loss 0.261 - Valid Loss 425.999\n","Epoch: 378/500 - Train loss 0.257 - Valid Loss 425.251\n","Epoch: 379/500 - Train loss 0.256 - Valid Loss 424.456\n","Epoch: 380/500 - Train loss 0.253 - Valid Loss 423.731\n","Epoch: 381/500 - Train loss 0.247 - Valid Loss 422.946\n","Epoch: 382/500 - Train loss 0.240 - Valid Loss 422.232\n","Epoch: 383/500 - Train loss 0.232 - Valid Loss 421.478\n","Epoch: 384/500 - Train loss 0.224 - Valid Loss 420.767\n","Epoch: 385/500 - Train loss 0.217 - Valid Loss 420.045\n","Epoch: 386/500 - Train loss 0.211 - Valid Loss 419.335\n","Epoch: 387/500 - Train loss 0.207 - Valid Loss 418.645\n","Epoch: 388/500 - Train loss 0.203 - Valid Loss 417.935\n","Epoch: 389/500 - Train loss 0.200 - Valid Loss 417.269\n","Epoch: 390/500 - Train loss 0.197 - Valid Loss 416.561\n","Epoch: 391/500 - Train loss 0.196 - Valid Loss 415.927\n","Epoch: 392/500 - Train loss 0.196 - Valid Loss 415.214\n","Epoch: 393/500 - Train loss 0.200 - Valid Loss 414.615\n","Epoch: 394/500 - Train loss 0.209 - Valid Loss 413.880\n","Epoch: 395/500 - Train loss 0.229 - Valid Loss 413.333\n","Epoch: 396/500 - Train loss 0.262 - Valid Loss 412.549\n","Epoch: 397/500 - Train loss 0.322 - Valid Loss 412.073\n","Epoch: 398/500 - Train loss 0.387 - Valid Loss 411.213\n","Epoch: 399/500 - Train loss 0.458 - Valid Loss 410.788\n","Epoch: 400/500 - Train loss 0.436 - Valid Loss 409.935\n","Epoch: 401/500 - Train loss 0.344 - Valid Loss 409.458\n","Epoch: 402/500 - Train loss 0.211 - Valid Loss 408.818\n","Epoch: 403/500 - Train loss 0.152 - Valid Loss 408.214\n","Epoch: 404/500 - Train loss 0.189 - Valid Loss 407.780\n","Epoch: 405/500 - Train loss 0.270 - Valid Loss 407.059\n","Epoch: 406/500 - Train loss 0.338 - Valid Loss 406.683\n","Epoch: 407/500 - Train loss 0.331 - Valid Loss 405.944\n","Epoch: 408/500 - Train loss 0.271 - Valid Loss 405.516\n","Epoch: 409/500 - Train loss 0.183 - Valid Loss 404.922\n","Epoch: 410/500 - Train loss 0.130 - Valid Loss 404.399\n","Epoch: 411/500 - Train loss 0.134 - Valid Loss 403.952\n","Epoch: 412/500 - Train loss 0.173 - Valid Loss 403.326\n","Epoch: 413/500 - Train loss 0.214 - Valid Loss 402.931\n","Epoch: 414/500 - Train loss 0.226 - Valid Loss 402.283\n","Epoch: 415/500 - Train loss 0.201 - Valid Loss 401.879\n","Epoch: 416/500 - Train loss 0.155 - Valid Loss 401.330\n","Epoch: 417/500 - Train loss 0.117 - Valid Loss 400.876\n","Epoch: 418/500 - Train loss 0.104 - Valid Loss 400.422\n","Epoch: 419/500 - Train loss 0.112 - Valid Loss 399.892\n","Epoch: 420/500 - Train loss 0.131 - Valid Loss 399.492\n","Epoch: 421/500 - Train loss 0.152 - Valid Loss 398.934\n","Epoch: 422/500 - Train loss 0.161 - Valid Loss 398.570\n","Epoch: 423/500 - Train loss 0.157 - Valid Loss 398.032\n","Epoch: 424/500 - Train loss 0.141 - Valid Loss 397.651\n","Epoch: 425/500 - Train loss 0.123 - Valid Loss 397.136\n","Epoch: 426/500 - Train loss 0.104 - Valid Loss 396.714\n","Epoch: 427/500 - Train loss 0.090 - Valid Loss 396.237\n","Epoch: 428/500 - Train loss 0.082 - Valid Loss 395.799\n","Epoch: 429/500 - Train loss 0.079 - Valid Loss 395.385\n","Epoch: 430/500 - Train loss 0.080 - Valid Loss 394.938\n","Epoch: 431/500 - Train loss 0.084 - Valid Loss 394.554\n","Epoch: 432/500 - Train loss 0.090 - Valid Loss 394.081\n","Epoch: 433/500 - Train loss 0.095 - Valid Loss 393.712\n","Epoch: 434/500 - Train loss 0.101 - Valid Loss 393.227\n","Epoch: 435/500 - Train loss 0.106 - Valid Loss 392.886\n","Epoch: 436/500 - Train loss 0.110 - Valid Loss 392.408\n","Epoch: 437/500 - Train loss 0.112 - Valid Loss 392.085\n","Epoch: 438/500 - Train loss 0.114 - Valid Loss 391.607\n","Epoch: 439/500 - Train loss 0.114 - Valid Loss 391.285\n","Epoch: 440/500 - Train loss 0.115 - Valid Loss 390.801\n","Epoch: 441/500 - Train loss 0.115 - Valid Loss 390.489\n","Epoch: 442/500 - Train loss 0.114 - Valid Loss 390.017\n","Epoch: 443/500 - Train loss 0.109 - Valid Loss 389.717\n","Epoch: 444/500 - Train loss 0.103 - Valid Loss 389.266\n","Epoch: 445/500 - Train loss 0.094 - Valid Loss 388.964\n","Epoch: 446/500 - Train loss 0.086 - Valid Loss 388.532\n","Epoch: 447/500 - Train loss 0.077 - Valid Loss 388.223\n","Epoch: 448/500 - Train loss 0.070 - Valid Loss 387.813\n","Epoch: 449/500 - Train loss 0.064 - Valid Loss 387.503\n","Epoch: 450/500 - Train loss 0.059 - Valid Loss 387.116\n","Epoch: 451/500 - Train loss 0.056 - Valid Loss 386.807\n","Epoch: 452/500 - Train loss 0.053 - Valid Loss 386.432\n","Epoch: 453/500 - Train loss 0.052 - Valid Loss 386.126\n","Epoch: 454/500 - Train loss 0.051 - Valid Loss 385.755\n","Epoch: 455/500 - Train loss 0.052 - Valid Loss 385.466\n","Epoch: 456/500 - Train loss 0.054 - Valid Loss 385.095\n","Epoch: 457/500 - Train loss 0.059 - Valid Loss 384.833\n","Epoch: 458/500 - Train loss 0.069 - Valid Loss 384.443\n","Epoch: 459/500 - Train loss 0.086 - Valid Loss 384.217\n","Epoch: 460/500 - Train loss 0.118 - Valid Loss 383.778\n","Epoch: 461/500 - Train loss 0.166 - Valid Loss 383.612\n","Epoch: 462/500 - Train loss 0.245 - Valid Loss 383.094\n","Epoch: 463/500 - Train loss 0.312 - Valid Loss 382.986\n","Epoch: 464/500 - Train loss 0.371 - Valid Loss 382.416\n","Epoch: 465/500 - Train loss 0.306 - Valid Loss 382.279\n","Epoch: 466/500 - Train loss 0.184 - Valid Loss 381.849\n","Epoch: 467/500 - Train loss 0.065 - Valid Loss 381.576\n","Epoch: 468/500 - Train loss 0.040 - Valid Loss 381.363\n","Epoch: 469/500 - Train loss 0.101 - Valid Loss 380.948\n","Epoch: 470/500 - Train loss 0.180 - Valid Loss 380.833\n","Epoch: 471/500 - Train loss 0.215 - Valid Loss 380.363\n","Epoch: 472/500 - Train loss 0.163 - Valid Loss 380.203\n","Epoch: 473/500 - Train loss 0.078 - Valid Loss 379.879\n","Epoch: 474/500 - Train loss 0.032 - Valid Loss 379.596\n","Epoch: 475/500 - Train loss 0.053 - Valid Loss 379.431\n","Epoch: 476/500 - Train loss 0.104 - Valid Loss 379.045\n","Epoch: 477/500 - Train loss 0.120 - Valid Loss 378.892\n","Epoch: 478/500 - Train loss 0.090 - Valid Loss 378.542\n","Epoch: 479/500 - Train loss 0.044 - Valid Loss 378.316\n","Epoch: 480/500 - Train loss 0.026 - Valid Loss 378.118\n","Epoch: 481/500 - Train loss 0.044 - Valid Loss 377.822\n","Epoch: 482/500 - Train loss 0.073 - Valid Loss 377.681\n","Epoch: 483/500 - Train loss 0.085 - Valid Loss 377.347\n","Epoch: 484/500 - Train loss 0.065 - Valid Loss 377.171\n","Epoch: 485/500 - Train loss 0.037 - Valid Loss 376.917\n","Epoch: 486/500 - Train loss 0.022 - Valid Loss 376.694\n","Epoch: 487/500 - Train loss 0.027 - Valid Loss 376.535\n","Epoch: 488/500 - Train loss 0.045 - Valid Loss 376.261\n","Epoch: 489/500 - Train loss 0.057 - Valid Loss 376.103\n","Epoch: 490/500 - Train loss 0.055 - Valid Loss 375.810\n","Epoch: 491/500 - Train loss 0.041 - Valid Loss 375.631\n","Epoch: 492/500 - Train loss 0.026 - Valid Loss 375.406\n","Epoch: 493/500 - Train loss 0.017 - Valid Loss 375.206\n","Epoch: 494/500 - Train loss 0.019 - Valid Loss 375.036\n","Epoch: 495/500 - Train loss 0.027 - Valid Loss 374.791\n","Epoch: 496/500 - Train loss 0.034 - Valid Loss 374.630\n","Epoch: 497/500 - Train loss 0.035 - Valid Loss 374.379\n","Epoch: 498/500 - Train loss 0.030 - Valid Loss 374.222\n","Epoch: 499/500 - Train loss 0.023 - Valid Loss 374.014\n","Epoch: 500/500 - Train loss 0.016 - Valid Loss 373.842\n"]}]},{"cell_type":"code","metadata":{"id":"U3Sl3cUJzZV_","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1653859568093,"user_tz":180,"elapsed":329,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"dbd5c2c6-1a4a-4dd5-f70b-4a995b688c07"},"source":["epoch_count = range(1, len(history2['loss']) + 1)\n","sns.lineplot(x=epoch_count,  y=history2['loss'], label='train')\n","sns.lineplot(x=epoch_count,  y=history2['val_loss'], label='valid')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"FveVOv2xzfkC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653859599007,"user_tz":180,"elapsed":10,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"b624626e-1ee4-4a44-8bc4-c4973af3bbc1"},"source":["# Ensayo\n","x_test = [50, 51, 52]\n","y_test = sum(x_test)\n","test_input = np.array([x_test])\n","test_input = test_input.reshape((1, seq_length, input_size))\n","test_input = torch.from_numpy(test_input.astype(np.float32))\n","\n","test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1)\n","\n","y_hat = model2(test_input)\n","\n","print(\"y_test:\", y_test)\n","print(\"y_hat:\", y_hat)\n","\n","loss = model2_criterion(y_hat, test_target).item()\n","print(\"loss:\", loss)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["y_test: 153\n","y_hat: tensor([[104.9728]], grad_fn=<AddmmBackward0>)\n","loss: 2306.611083984375\n"]}]},{"cell_type":"markdown","metadata":{"id":"zd1g5MZfz5qB"},"source":["### 4 - Conclusión\n","Implementar un modelo bidireccional basado en RNN (en este caso LSTM) es muy sensillo. En este ejemplo no se explotó su potencialidad pero queda como nota de como implementar una capa BRNN."]}]}